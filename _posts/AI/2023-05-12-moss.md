# MOSS Vortex

https://github.com/OpenLMLab/MOSS_Vortex

使用 https://huggingface.co/fnlp/moss-moon-003-sft-plugin 这个模型

## 模型路径

把model放在项目根目录，因为根据**docker_run.sh** docker启动会把根目录映射到容器里面。

```sh
├── install_run.sh
├── LICENSE
├── log.sh
├── model
│   ├── moss-moon-003-sft-plugin
│   │   ├── added_tokens.json
│   │   ├── config.json
│   │   ├── configuration_moss.py
│   │   ├── LICENSE
│   │   ├── merges.txt
│   │   ├── modeling_moss.py
│   │   ├── pytorch_model-00001-of-00004.bin
│   │   ├── pytorch_model-00002-of-00004.bin
│   │   ├── pytorch_model-00003-of-00004.bin
│   │   ├── pytorch_model-00004-of-00004.bin
│   │   ├── pytorch_model.bin.index.json
│   │   ├── README.md
│   │   ├── special_tokens_map.json
│   │   ├── tokenization_moss.py
│   │   ├── tokenizer_config.json
│   │   └── vocab.json
```

## 镜像缺py库

```sh
docker run -d --rm --name=${name} --privileged --cap-add=SYS_PTRACE --shm-size=500g \
--gpus=all \
-w /mosec -p${port}:${port} \
-v `pwd`:/mosec piglake/mosec:0.6.1 \  # 修改镜像
python3 mosec_server.py --port ${port} --timeout 60000  --wait 500 --batches ${batch_size} # --debug
```



```sh
# 进到容器里面安装triton
docker exec -it moss_vortex bash

pip install triton
```



```sh
# 保存修改
docker commit moss_vortex piglake/mosec:0.6.1
```



## 运行脚本修改对比

有注释说明的都是修改后的结果

```py
MODEL_DIR = "fnlp/moss-moon-003-sft-plugin-int4"
MODEL_DIR = "model/moss-moon-003-sft-plugin" # 模型在容器里面的路径


class My_WebSocket():
.....

def Local_Init_AutoTokenizer(model_dir) -> PreTrainedTokenizer:
    """
    Initialize and return a custom tokenizer from the local files.

    Returns:
        tokenizer (PreTrainedTokenizer): An instance of the PreTrainedTokenizer class.
    """
    # Uncomment the following lines to load tokenizer from different sources.

    # Load the tokenizer from local files.
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir,trust_remote_code=True) # 要添加这个trust_remote_code=True才能跑

    
    
class Inference(Worker):
    """Pytorch Inference class"""

    def __init__(self, use_onnx=True):
    def __init__(self, use_onnx=False):  # 默认加载onnx模型，但是moss模型都是pytorch, 加载会报错

						.....
            logger.info("[MOSEC] [INIT] PyTorch Loading")
						self.model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True).cuda()
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True,trust_remote_code=True).cuda()  # 要添加trust_remote_code=True才能跑

            
            
            
if __name__ == "__main__":
    
    NUM_DEVICE = 6
    print(torch.cuda.is_available()) # 打印GPU信息
    print(torch.cuda.device_count()) # 可用个数
    NUM_DEVICE = 1  #  相当于load NUM_DEVICE个模型，对应每个模型一张卡 加载一个模型耗70G左右内存,模型完全加载后内存才会释放所以这里降到1

........
    # Append inference worker to the server.
    server.append_worker(Inference,
                         num=NUM_DEVICE,
                         env=[_get_cuda_device(x) for x in range(0, 0+NUM_DEVICE)],  # env=[{"CUDA_VISIBLE_DEVICES":"7"}],
                         env=[_get_cuda_device(1)],  # 指定1号卡
                         max_batch_size=INFERENCE_BATCH_SIZE,
                        )
    # Run the server.
    server.run()
```

启动后

```sh
nvidia-smi 

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    1   N/A  N/A     38972      C   /usr/bin/python3                68103MiB |
+-----------------------------------------------------------------------------+
```

测试

```sh
curl 'http://dev4.com:21333/inference' \
-H 'Content-Type: application/json' \
-d '{
    "x":"<|Human|>: hello<eoh>\n<|Inner thoughts|>: NoNone<eoc>\n<|Results|>: None<eor>\n<|MOSS|>:",
    "max_iterations":"128",
    "temperature":"0.7",
    "repetition_penalty":"1.1"
}'



返回

{
    "pred": "<|Human|>: hello <eoh> \n<|Inner thoughts|>: NoNone <eoc> \n<|Results|>: None <eor> \n<|MOSS|>: Hello! How may I assist you today? <eom>",
    "input_token_num": 322,
    "new_generations_token_num": 10,
    "new_generations": " Hello! How may I assist you today? <eom>"
}
```

