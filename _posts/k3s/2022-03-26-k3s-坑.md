---
layout:     post
rewards: false
title:      k3s，autok3s的一些坑
categories:
    - k3s
---

# 设置coredns后还原

中心集群和边缘集群通过好几个前置机，边缘云要通过域名连通到中心云

![image-20211210100932751](https://tva1.sinaimg.cn/large/e6c9d24egy1h0nmrkhchpj21g00mkq5h.jpg)

这里主要涉及到边缘云主节点的coredns的configmap的hosts插件域名解析。

但是后来发现，**重启k3s或者添加新节点时候，coredns的hosts部分会还原回去**。[More extensibility to CoreDNS configmap](https://github.com/k3s-io/k3s/issues/462) 这里有比较详细的讨论。

主要就是我们用的k3s version 1.19.5，试了最新的stable1.22.7确实**支持最新的自定义coredns**（主要是使用import插件）

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: xxxx
  namespace: kube-system
data:
    xxx.server: | # 必须.server结尾
          example1.org example3.org  {
              hosts {
                   127.0.0.1 example1.org example3.org
                   fallthrough
              }
          }
```

但是**升级k3s成本有点难以预料**

- 升级k3s本身就有潜在的问题（软件本身，数据迁移之类）
- 使用rancher接管k3s，对k3s适用版本范围有限制

**后面应急方法**

- 修改纳管时候部署的agent，使用添加**hostAliases**，只能解决云间连通的问题，**存在域名解析必须配coredns（例如镜像仓库地址）**。

```sh
kubectl patch  deploy cattle-cluster-agent -n cattle-system --type='merge' -p '{"spec":{"template":{"spec":{"hostAliases":[{"ip":"10.19.64.205","hostnames":["ums.uniin.cn"]}]}}}}
```

- 通过crd+controller解决



# AutoK3S存在问题

- [0.4.6版本 .ssh/id_rsa: no such file or directory just use password to create cluster](https://github.com/cnrancher/autok3s/issues/391)
- [0.4.7 `native provider` does not support adding nodes to a cluster that is not managed by AutoK3s](https://github.com/cnrancher/autok3s/issues/407)这个问题严重，**之前版本不保留集群信息到.autok3s/.db/autok3s.db**，0.4.7后面突然保留，而且完全没有考虑版本间如何兼容，这么大的改变也没有体现在changelog。这会导致**旧版本/k3s创建的集群，用新版本添加不了节点**，而且代码里面集群信息管理也混乱，感觉半成品。
- [不支持离线安装docker](https://github.com/cnrancher/autok3s/pull/423)（向社区贡献自己的代码）

最后决定从0.4.6开始维护自己的分支，对autok3s的维护者水平有点失望