---
layout:     post
rewards: false
title:      NN
categories:
    - ml
---
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvkdfiryu9j31hm0uu7bu.jpg)
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvkdo1d99cj31jc12811p.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvke8z09loj31hy12u48w.jpg)

![](https://i.loli.net/2018/09/24/5ba853af28a7e.png)

# 求W
## 损失函数
- 分类交叉熵
- 回归MSE

## 反向传播 backward propagation
$cost=\frac12\left(\mathrm{真实值}-\mathrm{预测值}\right)^2$ 方便求导
求 cost的min 

梯度下降SGD 不断修正W
$W\;=\;W\;-\;\frac{\partial\;cost}{\partial W}$

Example :
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvkhiehkemj31hk0c2aay.jpg)
目标求$\frac{\partial\;P}{\partial W_2},\frac{\partial\;P}{\partial W_1}$

链式偏导
$\frac{\partial\;P}{\partial W_2} = \frac{\partial\;P}{\partial Z}*\frac{\partial\;Z}{\partial W_2}$ 恒等变换
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvkhiu718sj31540le75y.jpg)
整理一下
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvkhm5vzh5j319i0iwjt3.jpg)

从后往前推导微分
当网络复杂时 偏导路径 很多
反向传播: 重复偏导的地方 重复使用不再算 省时间（链式偏导+动态规划）
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvkhydyjz1j30zu0nu445.jpg)


# 优化NN
NN 结果对W初始值敏感 一般选随机

复杂度 神经网络中神经元的个数,权值的数量D O(VD) 
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvkibqyo4hj30z80om0xo.jpg)
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvkif9r2xrj316s0qadk8.jpg)

# dNN

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvkkhlvgmwj31ik138do7.jpg)
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvkkumuflnj31io132153.jpg)


# 梯度消失
激活函数sigmoid将负无穷到正无穷的数映射到0和1之间
神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，
最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，
从而导致层数比较浅的权重没有更新，这就是梯度消失。
此深层网络的学习就等价于只有后几层的浅层网络的学习

# 梯度爆炸
初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了

#sumary
网络太深 -> 因为梯度反向传播中的连乘效应 用ReLU取代sigmoid

