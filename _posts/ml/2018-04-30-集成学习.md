---
layout:     post
rewards: false
title:      集成学习 Ensemble
categories:
    - ml
---
sklearn xgboot

Ensemble
好处
feature transform和regularization
单一模型通常只能倾向于feature transform和regularization之一，但是Ensemble却能将feature transform和regularization各自的优势结合起来



常见的 Ensemble 方法有这么几种：
# Blending
用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。
vote 分类
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvi4c7r0hfj31kw0zzk20.jpg)
avg 回归
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvi4eem5csj31kw12t7eb.jpg)

## 加权Linear blending
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvi5fdfjxwj31kw14n14b.jpg)

## 训练过程
用[validation](/ml/2018/05/10/trick/#validation) 训练不同的base model, 得到的值代入 Linear blending 获取对应的权重a。
使用完整的data set 训练不同的base model得到更好的model + 之前的权重 => 目标函数

## 获取base model
- select different base model
- different params
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvi7n94hdmj31kw0ngk3w.jpg)

# Stacking
在[blending](#blending)的基础上 => 多层blending
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvi6otk5wfj31kw126qei.jpg)
Blending只有一层，而Stacking有多层 使用前一层的输出(预测结果)作为本层的训练集(有点像多层神经网络)
模型复杂度过高，容易造成过拟合

# Bagging
使用训练数据的不同随机子集来，有放回随机抽取m次每次n个样本作为子集训练相同Base Model，由于不同data-set子集得到不同的model
最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvi9m9ev8dj31kw0vk44k.jpg)

>Blending & Bagging 都是可并行训练base model 再aggregation

以上算法不能解决下图 阴影是三个model都出错部分
<img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fvibwphh09j30so0pkmy3.jpg" style="zoom:30%"/>
# Boosting
迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。

前一个base model 预测错误的的样本会增权，正确的会减少权重，权值更新过后的样本训练下一个新的base model 直到目标错误率或最大迭代次数
aggregation时 表现好的model 权重就大

## 错误因子
犯错放大，正确缩小
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvidwukd1nj30wm036mxf.jpg)
通常学习比乱猜要好所以犯错率ϵt <= 0.5
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvidxcqv8hj30vo0d0mzk.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvieb665v4j30we08c3yn.jpg)
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fviedpni57j30w608i0t4.jpg)
u 表示重要性  AdaBoost
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fviee4rxgqj30wm0ha0uc.jpg)
aggregation时 表现好的 权重就大 $\alpha\;=\;\frac12\ln\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)$
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvieek5oo0j30vy05i74u.jpg)
