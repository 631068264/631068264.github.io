---
layout:     post
rewards: false
title:      pca
categories:
    - ml
---
高维数据集处于非常稀疏的风险，大多数训练实例可能彼此远离。当然，这也意味着一个新实例可能远离任何训练实例，这使得预测的可靠性远低于较低维度，因为它们将基于更大的外推。
train-set 维数越多 overfit风险越大
降低维数，肯定加快训练速度，但并不总是会导致更好或更简单的解决方案;这一切都取决于数据集。

# 投影
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvkpm1rcsaj31e00reabo.jpg)

# PCA
Principal Component Analysis (PCA)
首先识别与数据最接近的超平面，然后将数据投影到它上面

主成分分析
数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。
第一个新坐标轴选 择的是原始数据中**方差最大**的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有**最大方差**的方向。
该过程一直重复，**重复次数为原始数据中特征的数目**。
我们会发现，大部分方差都包含 在最前面的几个新坐标轴中。我们可以忽略余下的坐标轴，即对数据进行了降维处理。

## How to:
如何选择投影坐标轴方向（或者说[基](/数学/2018/09/20/向量/#基)）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。=> 方差

正交基 协方差=0

如何得到这些包含最大差异性的主成分方向
计算数据矩阵的[协方差矩阵](/数学/2018/05/21/概率常用/#协方差矩阵)，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维
得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵

# 特征值分解协方差矩阵
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvkq9l9r97j31kw0oawhr.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvkq9xji11j31ba0eitad.jpg)

# 奇异值分解协方差矩阵
SVD奇异值分解
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvkqacw8pwj31e60datas.jpg)
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvkqazu7o1j31kw0uggr9.jpg)
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvkqbjklguj31f60tygqi.jpg)

[PCA数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)