---
layout:     post
rewards: false
title:      SVM
categories:
    - ml
tags:
    - ml algorithm
---

Support Vector Machine

若要保证对未知的测量数据也能进行正确分类，最好让分类直线距离数据集的点都有一定的距离。距离越大，分类直线对**测量数据误差的容忍度**越高。

**容忍更多的noise,有更好的泛化能力** 距离用**margin**表示
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvg2gzl8f4j31hi0zywlf.jpg)
Why large margin ?
因为分隔的线**粗** 难以将数据集任意完全分隔，有点像regularization，降低model复杂度。


**支持向量**(support vector)就是离分隔超平面最近的那些点。

接下来要试着最大化支持向量到分隔面的距离
W 是[法向量](/数学/2018/09/20/向量/#法线-法向量)
分隔超平面的形式可以写成$w^Tx+b$
要计算点A到分隔超平面的距离,就必须给出点到分隔面的法线 或垂线的长度,该值为 $\left|w^TA+b\right|/\left|\left|w\right|\right|$
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvg2kg076tj30yk0re0tu.jpg)
>margin最大 => 即让离分类线最近的点到分类线距离最大
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvg2kl5mrrj31i80acq4c.jpg)

推导过程
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvg4uz86l5j30zq0najsy.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvg568gkcnj31c610on49.jpg)
简化计算
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvg5dctsqoj31h4114k0k.jpg)

再简化
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvg6bik8jlj31kw146aol.jpg)
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvg6cr30w1j30yq02sjrd.jpg)

# Dual SVM
对于非线性SVM，我们通常可以使用非线性变换将变量从x域转换到z域，设z域维度=d+1 ,d越高，svm通过二次规划求解就越复杂。
dual不依赖d  
当数据量N很大时，也同样会增大计算难度。如果N不是很大，一般使用Dual SVM来解决问题,只是简化求解不能真正脱离对d的依赖

# Kernel

基于dual SVM推导 Kernel Function **合并特征转换和计算内积**   **非线性不受维度限制**简化计算速度 
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvgz7hfs65j31kw15wwq7.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvgz8rtymaj31kw0c778f.jpg)


## polynomial Kernel
简化好看的形式  不同的Kernel=>不同的margin
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvgzsvsmlxj31kw10rn90.jpg)
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvgzw15mxpj31kw0i3jwp.jpg)

## linear Kernel
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvh00l768fj31kw15gjyq.jpg)

## Gaussian Kernel
无限维转换  在低维空间解决非线性
利用**泰勒展开**反推 => Φ(x) ，Φ(x)是无限多维的
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvh0k8xpqij31kw14r7h2.jpg)
高斯核函数称为径向基函数(Radial Basis Function, RBF)
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvh6hhfuxoj31kw0wsqcx.jpg)

## 对比

[Linear Kernel](#linear-kernel)
优点：
- 计算简单、快速，
- 直观，便于理解
缺点：如果数据不是线性可分的情况，不能使用了

[Polynomial Kernel](#polynomial-kernel)
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvh70wgi76j31kw14c48q.jpg)
比线性灵活
缺点：
- 当Q很大时，K的数值范围波动很大
- 参数个数较多难以选择合适的值

[Gaussian Kernel](#gaussian-kernel)
最常用 要小心用
- 比其他kernel强大 
- 容易计算 比poly
- 参数少 比poly
缺点
- 不直观
- slower than linear
- easy overfit

## other kernel
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvh7cv0f0pj31kw0yt14b.jpg)

# Soft-Margin
允许有分类错误
C 越小 容忍错误度越大
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvha7qkd8aj31i012in5r.jpg)
上面的不能用二次规划计算，不能区分error的程度

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvhape6sgdj317m0ckwem.jpg)
点距离边界<img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fvhavneffvj309q0280sl.jpg" style="zoom:40%"/>

其中，ξn表示每个点犯错误的程度，ξn=0，表示没有错误，ξn越大，表示错误越大，
即点距离边界越小。

参数C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡，因为边界宽了，往往犯错误的点会增加
large C表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类
small C表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界

# svm regression
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvhijpnm2kj31kw15otgj.jpg)
![](https://i.loli.net/2018/09/21/5ba5037ff1474.png)
λ 越大，L2Regularization的程度就越大。C越小，相应的margin就越大。Large-Margin等同于Regularization效果一致，防止过拟合的作用。

L2-regularized linear model都可以使用kernel来解决。


m 数据集数量 n 维度
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvi1yjng4aj31fa0daab9.jpg)
- SVM Classifier
在margin外
trying to fit the largest possible street between two classes while limiting margin violations

- SVM Regression
在margin内
tries to fit as many instances as possible on the street while limiting margin violations