---
layout:     post
rewards: false
title:      线性模型
categories:
    - ml
tags:
    - 梯度
    - 过拟合
---

# 普通线性回归模型
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvf7vcnx8sj317m0e80tq.jpg)
目标函数
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvf7wn49uxj316207igm0.jpg)
最小二乘法求权重
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvf7wqr13mj31280f2t9f.jpg)

# logistic 分类
<span class='gp-2'>
    <img src='https://ws2.sinaimg.cn/large/006tNbRwgy1fve1x9atycj31i8116ah0.jpg' />
    <img src='https://ws2.sinaimg.cn/large/006tNbRwgy1fve1z8vkf4j31hw11oter.jpg' />
<span>

预测值表示为1的概率，取值范围在[0,1]之间

$$h=P(y=1 | x)$$

$ŷ=w^Tx+b$普通回归模型套一个 **sigmoid** function 控制输出 (0,1)

$$ŷ=Sigmoid(w^Tx+b)$$


$$Sigmoid(z)=\frac1{1+e^{-z}}$$

$$L(ŷ ,y)=−(ylog ŷ +(1−y)log (1−ŷ ))$$

Cost function

$$J(w,b)=\frac1m\sum_{i=1}^m\;L(\;\widehat y^{(i)}\;,y^{(i)})\;=\;-\frac1m\sum_{i=1}^m\lbrack y^{(i)}\log\;\;\widehat y^{(i)}+(1-\;y^{(i)})\log\;(1-\;\widehat y^{(i)}\;)\rbrack$$

- softmax
```python
def softmax(X):
    exps = np.exp(X)
    return exps / np.sum(exps)
```
<span class='gp-2'>
    <img src='https://ws4.sinaimg.cn/large/006tNbRwgy1fvfv2wsvs1j306u02ymwy.jpg' />
    <img src='https://ws1.sinaimg.cn/large/006tNbRwgy1fvfuwk0a4sj30qa06at8r.jpg' />
    <img src='https://ws4.sinaimg.cn/large/006tNbRwgy1fvfuwo7ap6j31iy0l0abq.jpg' />
    <img src='https://ws4.sinaimg.cn/large/006tNbRwgy1fvfuwy25f7j317e08amxm.jpg' />
<span>

- cross entry
```python
def cross_entropy(X,y):
    """
    X is the output from fully connected layer (num_examples x num_classes)
    y is labels (num_examples x 1)
    	Note that y is not one-hot encoded vector. 
    	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.
    """
    m = y.shape[0]
    p = softmax(X)
    # We use multidimensional array indexing to extract 
    # softmax probability of the correct label for each sample.
    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.
    log_likelihood = -np.log(p[range(m),y])
    loss = np.sum(log_likelihood) / m
    return loss
```
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvfv5g1qcgj30ci01y3yc.jpg)
交叉熵，应该导致这个目标，因为当它估计目标类别的低概率时，它会对模型进行惩罚。交叉熵通常用于衡量一组估计的类别概率与目标类别的匹配程度
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fvfuxggbl6j31j40f4q3o.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvfuxk18nnj30zu08gdg5.jpg)

目标函数
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fve2psh65yj31iu0h2ab2.jpg)

# Gradient 梯度
[梯度下降](https://ctmakro.github.io/site/on_learning/gd.html)

<img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fudlsx49zlj30jo0eqdfz.jpg" style="zoom:40%"/>
error函数的图像，看上去像个碗一样，中间是凹的，两边翘起。这个碗的最低点，也就是 error(x) 的最小值，就是我们要寻找的点。

## 发现:
当x在最小值左边的时候，error函数的导数（斜率）是负的；
当x在最小值右边的时候，导数是正的；
当x在最小值附近的时候，导数接近0.

因此，如果我们在：
导数为负的时候增加x；
导数为正的时候减小x；


$$x = x - derivative * alpha$$

## 解释：
_求权重的偏导_

- **derivative** 是 error 在 x 处的导数，这里是用导数的定义求的。
x = x - derivative 这就是“导数下降”名称的由来。通过不断地减去导数，x最终会到达函数的最低点。
- **alpha** 参数控制的是点 x 逆着导数方向前进的距离（下降速度），alpha 越大，x 就会前进得越多，误差下降得越快。alpha 太小会导致下降缓慢，alpha 太大会导致 x 冲得太远，令函数无法收敛。

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fve36mmm0cj31560iktah.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fve37q4ynaj30sy0acjrr.jpg)

## Batch Gradient Descent

它使用整个训练集来计算每一步的梯度，这使得训练集很大时非常缓慢

## Stochastic Gradient Descent
随机梯度下降只是在每个步骤中在训练集中选取一个随机实例，并仅基于该单个实例计算梯度
由于其随机性，该算法比批处理梯度成本函数会上下反弹 只会平均减少 随着时间的推移，
它将最终接近最小值，但一旦它到达那里，它将继续反弹，永远不会稳定下来。
所以一旦算法停止，最终的参数值是好的，但不是最优的


当成本函数非常不规则时，这实际上可以帮助算法跳出局部最小值，
因此，随机性很好地摆脱局部最优。但不好，因为这意味着该算法永远无法最小化

方法是
逐渐降低学习率。这些步骤开始较大（这有助于快速进展并避免局部最小值），
然后变得越来越小，从而使算法在全局最小值处达到最小。（simulated annealing模拟退火）

## Mini-batch Gradient Descent
小批量的小随机实例集上的梯度

进展比SGD更不稳定，特别是在相当大的小批量时。
因此，小批量GD最终会走得比SGD更接近最小值。但另一方面，它可能难以摆脱局部最小值

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvfugwafynj31kw0q7mz5.jpg)

m is the number of training instances and n is the number of features

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvfuh11l4rj31jm0a80tv.jpg)
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvfuh59sm4j31k8058aac.jpg)



# 非线性变换
**用不太复杂的model 解决非线性问题**
通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行线性分类
- 特征转换
- 训练线性模型

例如
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvep9slkq7j31bi0esdi0.jpg)

模型太复杂容易带来过拟合
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvep83x6wpj30ik0cymxk.jpg)
非线性变换可能会带来的一些问题：**时间复杂度和空间复杂度的增加**，尽可能使用简单的模型，而不是模型越复杂越好

# 过拟合
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvexcrvkpbj31k20ui102.jpg)
## 造成overfit 主要是
- noise
- 模型复杂度太高
-  train-set too small
## 解决
- 数据清洗（错误数据的更正，或者删除）
- 简化模型
- 更多数据 （如果没有办法获得更多的训练集，对已知的样本进行简单的处理、变换，从而获得更多的样本）
- [regularization](#regularization)
- [validation](/ml/2018/05/10/trick/#validation)

## 维度灾难 
当模型很复杂的时候 ，复杂度本身就会引入一种**noise**，
所以即使高阶无noise，模型也不能很好泛化。

# Regularization
正则化 给予model惩罚 **减低model复杂度**
## 岭回归 L2
普通回归+惩罚项（ $\alpha$ 越大惩罚越大）

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvfuj3314hj30um08k3yp.jpg)
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvfuj8y527j30yi07eaaa.jpg)

## lasso L1 
部分权重归0
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvfukpdz8uj31jy0e8diq.jpg)
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvfukpdz8uj31jy0e8diq.jpg)


# 更好的训练结果
- 选择简单的model 参数很少，或者用regularization，数据集的特征减少中参数个数减少，都能降低模型复杂度
- 训练数据和验证数据要服从同一个分布，最好都是独立分布的，这样训练得到的模型才能更好地具有代表性。
- 在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据。第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。

