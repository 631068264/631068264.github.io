---
layout: post
rewards: false
title: start
categories:
- ml
tags:
- tensorflow
---

- 创建一个graph，
- TensorFlow 会话(Session)负责处理在诸如 CPU 和 GPU 之类的设备上的操作并运行它们，并且它保留所有变量值。

# 常量
```python
a = tf.constant(5)
b = 5 + a
c = a + b
with tf.Session() as sess:
    result = sess.run(c)
    print(result)
10
```
# 变量
训练过程中会不断优化
```python
x = tf.Variable(1, name="x")
y = tf.Variable(2, name="y")
f = (x + y) * a
```

`global_variables_initializer`必须运行在所有`Variable`声明之后 不然报错 ~~fuck session 和 变量初始化不能搞在一起吗?~~
```python
# 初始化变量
init_var = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_var)
    """
    想要高效地评估f和c，在一次图形运行中评估f和c,所有节点值都在图运行之间删除，除了变量值, b会重复求值
    单进程TensorFlow中,每个会话都有自己的每个变量副本 多个会话不共享任何状态，即使它们重复使用同一个图。
    变量在其初始化程序运行时启动其生命周期，并且在会话关闭时结束。
    """
    con_, var_ = sess.run([f, c])
    print(con_, var_)

15 10
```
# scope

## get_variable and Variable
- `tf.Variable` 必须有初始值

- `tf.Variable` 始终创建一个新变量，已存在具有此类名称的变量，则可能会为变量名称添加后缀
- `tf.get_variable` 从graph中获取具有这些参数的现有变量，如果它不存在，它将创建一个新变量,存在的话会报错

```python
from pprint import pprint

tf.Variable(0, name="a")  # name == "a:0"
tf.get_variable(initializer=0, name="a")  # name == "a_1:0"
tf.Variable(0, name="a")  # name == "a_2:0"

b = tf.get_variable(initializer=0, name="b")  # name == "b:0"
c = tf.get_variable(name="c", shape=1)  # name == "c:0"
pprint(tf.trainable_variables())
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(b.eval())
    print(c.eval())
```

## name_scope and variable_scope
- 两个scope对使用`tf.Variable`创建的变量具有相同的效果，即范围将作为操作或变量名称的前缀添加。
- `tf.variable_scope`就不一样了

```python
with tf.variable_scope('var'):
    with tf.name_scope("name"):
        v1 = tf.get_variable("var1", [1], dtype=tf.float32)
        v2 = tf.Variable(1, name="var2", dtype=tf.float32)
        a = tf.add(v1, v2)

print(v1.name)  # var/var1:0
print(v2.name)  # var/name/var2:0
print(a.name)  # var/name/Add:0

```

scope reuse 共享变量 `tf.variable_scope` 和 `tf.get_variable` 搭配

```python
with tf.variable_scope('bar'):
    foo1 = tf.get_variable(name="a", shape=1)  # name == "bar/a:0"
    foo2 = tf.get_variable(name="a", shape=1)  # ValueError: Variable bar/a already exists

with tf.variable_scope('foo', reuse=tf.AUTO_REUSE):
    foo1 = tf.get_variable(name="a", shape=1)  # name == "foo/a:0"

with tf.name_scope("bar"):
    with tf.variable_scope("foo", reuse=True):
        v1 = tf.get_variable("a")
        assert foo1 == v1

pprint(tf.trainable_variables())
```

# 占位符
它们通常用于在训练期间将训练数据传递给TensorFlow 实际上不执行任何计算
```python
A = tf.placeholder(tf.float32, shape=(None, 3))
B = A + 5
with tf.Session() as sess:
    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})
    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})

print(B_val_1)
print(B_val_2)

[[6. 7. 8.]]
[[ 9. 10. 11.]
 [12. 13. 14.]]
```

# save load
默认情况下，保存器将以自己的名称保存并还原所有变量，但如果需要更多控制，则可以指定要保存或还原的变量以及要使用的名称
```python
saver = tf.train.Saver()
"""
Saver saves and restores all variables under their own name
saver = tf.train.Saver({"weights": theta})
"""
with tf.Session() as sess:
     save_path = saver.save(sess, "/tmp/my_model.ckpt")

with tf.Session() as sess:
     saver.restore(sess, "/tmp/my_model_final.ckpt")


```

# tensor board
正在定期保存检查点 可视化训练进度 checkpoint
```python
tf.summary.histogram('D_X/true', self.D_X(x))
tf.summary.scalar('loss/cycle', cycle_loss)
tf.summary.image('X/generated', utils.batch_convert2int(self.G(x)))

# Merge all the summaries and write them out to the summaries_dir
# merged = tf.summary.merge_all()

# graph = sess.graph or tf.get_default_graph()

mse_summary = tf.summary.scalar('MSE', mse)
file_writer = tf.summary.FileWriter(checkpoints_dir, graph)
with tf.Session() as sess:
    for i in step:
        s = sess.run(mse_summary)
        file_writer.add_summary(s, i)
        # file_writer.flush()

file_writer.close()
```

必须是完整路径 ~~fuck 不能相对路径~~   Pycharm 有Copy Path
```shell
tensorboard --logdir path/to/log-directory
```