---
layout:     post
rewards: false
title:      NLP2
categories:
    - ml
tags:
    - rnn
    - nlp
---
# 词模型
词袋模型：（词,出现次数）统计词频 CountVectorize 词频统计
## TF-IDF模型 
评估某一字词对于一个文件集或一个语料库的重要程度
如果一个词条在一个类的文档中频繁出现，则说明该词条能够很好代表这个类的文本的特征

$$W_{tf-idf}=W_{tf}\;\ast\;\lg({\textstyle\frac1{W_{df}}})$$

$W_{tf}$ text frequency (TF) 某文档词频
$W_{df}$ 在所有的文档总词频
```python
TfidfVectorizer.fit = TfidfTransformer.fit(CountVectorizer.fit_transform)
```
## 词汇表模型
词袋模型:文本由哪些单词组成 无法表达出单词之间的前后关系
生成的词汇表对原有句子按照单词逐个进行编码
  
## Word2Vec
**生成词向量 显示词间关系** 采用的模型有
**CBOW**(Continuous Bag-Of-Words，即连续的词袋模型)
**Skip-Gram** 两种
>  CBOW模型能够根据输入周围n-1个词来预测出这个词本身，而Skip-gram模型能够根据词本身来预测周围有哪些词。也就是说，
CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量，而Skip-gram模型的输入是词A本身，输出是词A周围的n个单词的词向量。

<span class='gp-2'>
    <img src='https://ws4.sinaimg.cn/large/006tNbRwgy1fwhe09lsy8j31kw17m41r.jpg' />
    <img src='https://ws4.sinaimg.cn/large/006tNbRwgy1fwhe0qcca2j31i816677i.jpg' />
</span>

## Doc2Vec
分为Distributed Memory (DM) 和Distributed Bag of Words (DBOW)。

## fasttext
有效且快速的方式生成词向量以及进行文档分类 高效

## LDA
一种文档主题模型，包含词、主题和文档三层结构
LDA认为一篇文档由一些主题按照一定概率组成，一个主题又由一些词语按照一定概率组成。
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwhe6wdvx7j30s208q74b.jpg)

**Why 主题**：
主题的个数通常为几百，这就把文档使用了维数为几百的向量进行了表示，大大加快了训练速度，并且相对不容易造成过拟合。从某种程度上来说，主题是对若干词语的抽象表示。
  
## TextRank TF-IDF
使用TextRank提取关键字

## 计算文档相似度
simhash
- n个(关键词，权重)对
- 计算关键词的hash，生成(hash,weight），并将hash和weight相乘，这一过程是对hash值加权
- 将hash和weight相乘的值相加，比如图中的[13, 108, -22, -5, -32, 55]，并最终转换成simhash值110001，转换的规则为正数为1负数为0

# 评论预处理
文本去重
- 原因
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwhe5dmzd5j31kw0o4wix.jpg)
- 做法
  尽量保留有用的
压缩去词
![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwhe5fgvqkj31kw0jqdih.jpg)
短句删除
![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwhe5imzzmj31kw097gmh.jpg)
