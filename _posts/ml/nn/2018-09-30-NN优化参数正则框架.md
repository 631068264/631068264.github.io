---
layout:     post
rewards: false
title:      NN优化参数正则框架
categories:
    - ml
tags:
    - nn优化
---

# Hyperparameters Tuning
常见找参数
- $\alpha$：学习因子
- $\beta$：动量梯度下降因子
- $\beta_1,\beta_2,\varepsilon$：Adam算法参数
- layers：神经网络层数
- hidden units：各隐藏层神经元个数
- learning rate decay：学习因子下降参数
- mini-batch size：批量训练样本包含的样本个数

参数采样
- 随机采样
- 放大表现较好的区域，再对此区域做更密集的随机采样，由粗到细的采样（coarse to fine sampling scheme）

# 为超参数选择合适的范围
对于某些超参数，可能需要**非均匀随机采样**（即非均匀刻度尺）。
例如超参数αα\alpha，待调范围是[0.0001, 1]。如果使用均匀随机采样，那么有90%的采样点分布在[0.1, 1]之间，
只有10%分布在[0.0001, 0.1]之间。这在实际应用中是不太好的，因为最佳的$\alpha$值可能主要分布在[0.0001, 0.1]之间，
而[0.1, 1]范围内$\alpha$值效果并不好。因此我们更关注的是区间[0.0001, 0.1]，应该在这个区间内细分更多刻度。

将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。
![](https://ws4.sinaimg.cn/large/006tNc79gy1fvsg9eb0u5j30nq0763yg.jpg)
一般解法是，如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，
然后得到的采样值r，最后反推到线性区间，即$10^r$。$10^r$就是最终采样的超参数。
```python
m = np.log10(a)
n = np.log10(b)
r = np.random.rand()
r = m + (n-m)*r
r = np.power(10,r)
```
经过调试选择完最佳的超参数并不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型。

一般来说，对于非常复杂或者数据量很大的模型,一个模型进行训练，调试不同的超参数。也可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。

# Batch Normalization
Batch Normalization不仅可以让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。
也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。
[Normalizing input](/ml/2018/09/29/NN优化/#normalizing-input)只是对输入进行了处理，**Batch Normalization**各隐藏层的输入进行标准化处理。

第l层隐藏层的输入就是第l-1层隐藏层的输出$A^{[l-1]}$。对$A^{[l-1]}$进行标准化处理，从原理上来说可以提高$W^{[l]}$和$b^{[l]}$的训练速度和准确度。
这种对各隐藏层的标准化处理就是Batch Normalization。值得注意的是，实际应用中，一般是对$Z^{[l-1]}$进行标准化处理而不是$A^{[l-1]}$，其实差别不是很大。
![](https://ws2.sinaimg.cn/large/006tNc79gy1fvshpr5ehvj31kw0v1n05.jpg)
> Normalizing inputs和Batch Normalization有区别的,Normalizing inputs使所有输入的均值为0，方差为1。
而Batch Normalization可使各隐藏层输入的均值和方差为任意值。实际上，从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，
这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。这也解释了为什么需要用**γ和β来对z[l](i)**作进一步处理。
![](https://ws1.sinaimg.cn/large/006tNc79gy1fvshzkjj5cj31kw0mwjuv.jpg)

如果实际应用的样本与训练样本分布不同，即发生了**covariate shift**，则一般是要对模型重新进行训练的。深度神经网络中，covariate shift会导致模型预测效果变差。
而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强。Batch Norm减少了各层$W^{[l]}、B^{[l]}$之间的耦合性，让各层更加独立，
实现自我训练学习的效果。也就是说，如果输入发生covariate shift，那么因为Batch Norm的作用，
对个隐藏层输出$Z^{[l]}$进行均值和方差的归一化处理，$W^{[l]}和B^{[l]}$更加稳定，使得原来的模型也有不错的表现。

从另一个方面来说，Batch Norm也起到轻微的正则化（regularization）效果。具体表现在：
- 每个mini-batch都进行均值为0，方差为1的归一化操作
- 每个mini-batch中，对各个隐藏层的$Z^{[l]}$添加了随机噪声，效果类似于Dropout
- mini-batch越小，正则化效果越明显
但是，Batch Norm的正则化效果比较微弱，正则化也不是Batch Norm的主要功能。

# Softmax 多分类
目前我们介绍的都是二分类问题，神经网络输出层只有一个神经元，
表示预测输出$\hat y$是正类的概率$P(y=1|x)，$\hat y>0.5$则判断为正类，$\hat y<0.5$则判断为负类。

对于**多分类问题**，用C表示种类个数，神经网络中输出层就有C个神经元，即$Cn^{[L]}=C$。
其中，**每个神经元的输出依次对应属于该类的概率**，即$P(y=c|x)$。为了处理多分类问题，我们一般使用**Softmax**回归模型。

Softmax回归模型输出层的激活函数

Softmax回归模型输出层的激活函数如下所示：

$$z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]}$$

$$a^{[L]}_i=\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}}$$

输出层每个神经元的输出$a^{[L]}_i$对应属于该类的概率，满足：

$$\sum_{i=1}^Ca^{[L]}_i=1$$

所有的$a^{[L]}_i，即\hat y$，维度为(C, 1)。

