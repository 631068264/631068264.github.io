---
layout:     post
rewards: false
title:      k8s对象 pod
categories:
    - k8s
---

`Pod` 是 `Kubernetes` 应用程序的基本执行单元，即它是 `Kubernetes` 对象模型中创建或部署的最小和最简单的单元。

容器设计模式

云原生应用运行的环境都是复杂的分布式环境，目前 K8s 社区推出的容器设计模式主要分为三大类：

- 单容器管理模式

- 单节点多容器模式

- 多节点多容器模式



- 运行单个容器的 `Pod` —— **支持多容器的微服务实例**

  - 每个 `Pod` 一个容器的模型是最常见的使用情况，在这种情况下，可以将 `Pod` 看作单个容器的包装器，并且 `Kubernetes` 直接管理 `Pod`，而不是容器。

- 运行多个协同工作的容器的 `Pod` —— **基于多容器微服务模型的分布式应用模型**

  基于多容器微服务模型的分布式应用模型

  - `Pod` 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序，`Pod` 将这些容器和存储资源打包为一个可管理的实体。

- 分布式系统工具包：容器组合的模式 ——有状态服务水平扩展

  - 即将多个服务同时封装到同一个 `Pod` 中，对外提供服务，而不再需要用户手动安装其他服务或者工具，真正做到即时即用的便捷效果。

```yaml
# 通过定义清单文件创建Pod
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox
      command: ["sh", "-c", "echo Hello Kubernetes! && sleep 3600"]
```



```yaml
# 通过定义清单文件创建Pod
apiVersion: v1
kind: Pod
  metadata:
  name: pod-demo
  namespace: default
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-1
      image: hub.escapelife.site/library/nginx-test:v1
    - name: busybox-1
      image: busybox:latest
      command:
        - "/bin/sh"
        - "-c"
        - "sleep 3600"
```

# pod 控制器

我们很少会直接在 kubernetes 中创建单个 Pod。因为 Pod 的**生命周期是短暂的，用后即焚的实体**。当 Pod 被创建后，都会被 Kubernetes 调度到集群的 Node 上。直到 Pod 的进程终止、被删掉、因为缺少资源而被驱逐、或者 Node 故障之前这个 Pod 都会一直保持在那个 Node 上。

我们需要知道 Pod 本身是不会自愈修复的。如果 Pod 运行的 Node 故障或者是调度器本身故障，这个 Pod 就会被删除。同样的，如果 Pod 所在 Node 因为缺少资源或者 Pod 处于维护状态，那么 Pod 也就会被自动驱逐掉。

**Kubernetes 使用更高级的称为 Controller 的抽象层，来管理 Pod 实例。虽然可以直接使用 Pod，但是在 Kubernetes 中通常是使用 Controller 来管理 Pod 的。Controller 可以创建和管理多个 Pod，提供副本管理、滚动升级和集群级别的自愈能力。**

需要注意的是，重启 Pod 中的容器跟重启 Pod 不是一回事。Pod 只提供容器的运行环境并保持容器的运行状态，重启容器不会造成 Pod 重启。

Kubernetes 使用了一个更高级的称为 控制器 的抽象，由它处理相对可丢弃的 Pod 实例的管理工作。因此，虽然可以直接使用 Pod，但在 Kubernetes 中，更为常见的是使用控制器管理 Pod。

- Deployment
- StatefulSet
- DaemonSet

# pod 结构

**Pod 代表着集群中运行的进程：共享网络、共享存储**



在 `Pod` 中其实可以同时运行一个或者多个容器，这些容器能够**共享网络**、**存储**以及 **CPU/内存**等资源。

首先，我们需要知道的是，每个 `Pod` 都有一个特殊的被称为 **“根容器”** 的 `Pause` 容器。`Pause` 容器对应的镜像属于 `Kubernetes` 平台的一部分，通过 `Pause` 容器使工作在对应 `Pod` 的容器之间可以**共享网络**、**共享存储**。

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gvhegubh9hj60sc0n4q4402.jpg" alt="image-20211016195805541" style="zoom:50%;" />

# pod 共享

**why use pause**

使用 `Pause` 容器作为 `Pod` 根容器，以它的状态代表整个容器组的状态；其次，`Pod` 里的多个业务容器共享 `Pause` 容器的 `IP` 地址，共享 `Pause` 容器挂接的 `Volume` 资源。



- **共享存储资源**

可以为一个 `Pod` 指定多个共享的 `Volume` 资源。`Pod` 中的所有容器都可以访问共享的 `volume` 资源。`Volume` 也可以用来持久化 `Pod` 中的存储资源，以防容器重启后文件丢失。

- **共享网络资源**

  每个 `Pod` 都会被分配一个唯一的 `IP` 地址。**`Pod` 中的所有容器共享网络空间，包括 `IP` 地址和端口。`Pod` 内部的容器可以使用 `localhost` 互相通信。**`Pod` 中的容器与外界通信时，必须分配共享网络资源，例如使用宿主机的端口映射。

     - 同一个 `Pod` 内多个容器之前通过回环网络(`lo` - `127.0.0.1`)进行通信
     - 各 `Pod` 之间的通讯，则是通过 `Overlay Network` 网络进行通信
     - 而 `Pod` 与 `Service` 之间的通讯，则是各节点的 `iptables` 或 `lvs` 规则

- 不同情况下**网络通信**方式

  - 同一个 Pod 内部通讯：
    - 同一个 Pod 共享同一个网络命名空间，共享同一个 Linux 协议栈。
  - 不同 Pod 之间通讯：
    - Pod1 和 Pod2 在同一台 Node 主机，由 docker0 网桥直接转发请求到 Pod2 上面，不经过 Flannel 的转发。
    - Pod1 和 Pod2 不在同一台 Node 主机，Pod 的地址是与 docker0 在同一个网段的，但 docker0 网络与宿主机网卡是两个完全不同的 IP 网段，并且不同的 Node 之间的通讯只能通过宿主机的物理网卡进行。将 Pod 的 IP 地址和所在 Node 的 IP 地址关联起来，通过这个关联让 Pod 可以互相访问。
  - Pod 至 Service 的网络
    - 目前基于性能考虑，全部为 iptables 或 lvs 维护和转发。
  - Pod 到外网
    - Pod 想外网发送请求，查找路由表，转发数据包到宿主机的网卡，宿主机网卡完成路由选择之后，iptables 或 lvs 执行 Masquerade，把源 IP 地址更改为宿主机的网卡的 IP 地址，然后向外网服务器发送请求。
  - 外网访问 Pod
    - 通过 Service 服务来向外部提供 Pod 服务。

# Fannel

`Flannel` 是 `CoreOS` 团队针对 `Kubernetes` 设计的一个网络规划服务，简单来说，它的功能就是让集群中的不同节点主机创建的 `Docker` 容器都具有全集群唯一的虚拟 `IP` 地址。而且它还能在这些 `IP` 地址之间建立一个覆盖的网络(`Overlay Network`)，通过这个覆盖网络，将数据包原封不动地传递给目标容器内。

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gvhf661x7oj60s20len0a02.jpg" alt="Overlay Network 网络进行通信" style="zoom:70%;" />

**Overlay Network 网络进行通信**





<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gvhf66dv5bj60nk0dfta802.jpg" alt="数据包原封格式" style="zoom:70%;" />

**Flannel利用ETCD：**

- 存储管理 Flannel 可分配的 IP 地址段资源
- 监控 ETCD 中每一个 Pod 的实际 IP 地址，并在内存中建立维护 Pod 节点的路由表



![ETCD之于Flannel提供说明](https://tva1.sinaimg.cn/large/008i3skNgy1gvhfdjkma7j60nn0boq3u02.jpg)



# veth 设备的特点

一个设备收到协议栈的数据发送请求后，会将数据发送到另一个设备上去

- `veth` 和其它的网络设备都一样，一端连接的是内核协议栈
- `veth` 设备是成对出现的，另一端两个设备彼此相连

```bash
# 物理网卡eth0配置的IP为192.168.1.11
# 而veth0和veth1的IP分别是192.168.2.11和192.168.2.10

+----------------------------------------------------------------+
|                                                                |
|       +------------------------------------------------+       |
|       |             Network Protocol Stack             |       |
|       +------------------------------------------------+       |
|              ↑               ↑               ↑                 |
|..............|...............|...............|.................|
|              ↓               ↓               ↓                 |
|        +----------+    +-----------+   +-----------+           |
|        |   eth0   |    |   veth0   |   |   veth1   |           |
|        +----------+    +-----------+   +-----------+           |
|192.168.1.11  ↑               ↑               ↑                 |
|              |               +---------------+                 |
|              |         192.168.2.11     192.168.2.10           |
+--------------|-------------------------------------------------+
               ↓
         Physical Network
```

# 不同类型的pod

- **ReplicationController**

`ReplicationController` 用来确保容器应用的副本数量始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 `Pod` 来代替，而如果异常多出现的容器会自动回收。

- **ReplicaSet**

在新版本(相对而言的较优方式)的 `Kubernetes` 中建议使用 `ReplicaSet` 来取代 `ReplicationController` 来管理 `Pod`。虽然 `ReplicaSet` 和 `ReplicationController` 并没有本质上的不同，只是名字不一样而已，唯一的区别就是 `ReplicaSet` 支持集合式的 `selector`，可供标签筛选。

虽然 `ReplicaSet` 可以独立使用，但一般还是建议使用 `Deployment` 来自动管理 `ReplicaSet` 创建的 `Pod`，这样就无需担心跟其他机制的不兼容问题。比如 `ReplicaSet` 自身并不支持滚动更新(`rolling-update`)，但是使用 `Deployment` 来部署就原生支持。

- **Deployment**

`Deployment` 为 `Pod` 和 `ReplicaSet` 提供了一个声明式定义方法，用来替代以前使用 `ReplicationController` 来方便且便捷的管理应用。主要的应用场景，包括：**滚动升级和回滚应用**、**扩容和缩容**、**暂停和继续**。

- **HPA**

**`HPA`** 仅仅适用于 `Deployment` 和 `ReplicaSet`，在 `V1` 版本中仅支持根据 `Pod` 的 `CPU` 利用率扩缩容，在新版本中，支持根据内存和用户自定义的 `metric` 动态扩缩容。

- **StatefulSet**

`StatefulSet` 是为了解决有状态服务的问题，相对于 `Deployment` 和 `ReplicaSet` 而已。其主要的使用场景，包括：稳定的持久化存储、稳定的网络标识、有序部署、有序收缩。

- **DaemonSet**

`DaemonSet` 确保全部或者一些 `Node` 上面运行一个 `Pod` 副本。当有 `Node` 加入集群的时候，也会为它们新加一个 `Pod`。当有 `Node` 从集群中移除的时候，这些 `Pod` 也会被回收。删除 `DaemonSet` 将会删除它所创建的所有 `Pod`。

使用 `DaemonSet` 的典型场景就是，在每个节点运行日志收集、运行监控系统、运行集群存储等服务，只要新加进来的节点都需要运行该服务。

- **Job**

`Job` 负责批处理任务，仅执行一次的任务，它保证批处理任务的一个或者多个 `Pod` 成功结束，才会返回成功。

- **Cron Job**

`Cron Job` 管理是基于时间的 `Job`，即在给定时间点只运行一次，且周期行的在给定时间点运行特定任务。





# pod 生命周期

Kubernetes 中的基本组件 **kube-controller-manager 就是用来控制 Pod 的状态和生命周期的**

![Kubernetes的资源清单](https://tva1.sinaimg.cn/large/008i3skNgy1gvhg31oj7bj60pz0ga0uy02.jpg)

## 了解 Init 容器

我们知道 Pod 可以包含多个容器，应用运行在这些容器里面，同时 **Pod 也可以有一个或多个先于应用容器启动的 Init 容器**。

**对比应用容器**

- Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置，
- 不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成。

- Init 容器总是运行到完成为止。
- 多个 `Init` 容器，这些容器会按顺序逐个运行。

`Pod` 内设置的 `Init` 容器运行失败了，那么 `Kubernetes` 就会不断地重启该 `Pod`，直到 `Init` 容器成功为止。然而，如果 `Pod` 对应的重启策略 `restartPolicy` 的值为 `Never`，它不会重新启动。如果为一个 `Pod` 指定了多个 `Init` 容器，这些容器会按顺序逐个运行。每个 `Init` 容器必须运行成功，下一个才能够运行。当所有的 `Init` 容器运行完成时，`Kubernetes` 才会为 `Pod` 初始化应用容器并像平常一样运行。



**因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：**

- Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。
- Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。
- Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可具有访问 Secrets 的权限，而应用容器不能够访问。
- 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod 内的所有的应用容器会并行启动。

```shell
# 启动Pod
$ kubectl apply -f myapp.yaml
pod/myapp-pod created

# 检查Pod状态
$ kubectl get -f myapp.yaml
NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m

# 如需更详细的信息
$ kubectl describe -f myapp.yaml

# 如需查看Pod内Init容器的日志
$ kubectl logs myapp-pod -c init-myservice
$ kubectl logs myapp-pod -c init-mydb

# 创建mydb和myservice
$ kubectl create -f services.yaml
service "myservice" created
service "mydb" created

# 查看Pod状态
$ kubectl get -f myapp.yaml
NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
```

## 状态和策略

**状态值**

Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。Pod 的运行阶段(phase)中其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。下面是 phase 可能的值：

| 状态值      | 描述                                                         |
| :---------- | :----------------------------------------------------------- |
| `Pending`   | API Server 已经创建该 Pod，但还有一个或多个容器的镜像没有创建成功，如下载镜像等状态 |
| `Running`   | Pod 内所有容器均已创建，且至少有一个容器处于运行状态、正在启动或正在重启状态 |
| `Succeeded` | Pod 内所有容器均已成功执行退出，且不会再重启                 |
| `Failed`    | Pod 内所有容器均已退出，但至少有一个容器退出为失败状态       |
| `Unknown`   | 由于某种原因无法获取该 Pod 的状态，可能由于网络通信不畅导致  |

**重启策略**

Pod 重启策略 RestartPolicy 应用于 Pod 内的所有容器，并且仅在 Pod 所处的 Node 上由 kubelet 进行判断和重启操作。当某个容器异常退出或者健康检查失败时，**kubelet 将根据 RestartPolicy 的设置来进行相应的操作。Pod 的重启策略包括 Always、OnFailure 和 Never 三种，默认值为 Always。**

**kubelet 重启失效容器的时间间隔以 sync-frequency 乘以 2n 来计算**，例如 1、2、4 等，最长延时 5min，并且在成功重启后的 10min 后重置该时间。

| 重启策略    | 描述                                                         |
| :---------- | :----------------------------------------------------------- |
| `Always`    | 当容器失效时，由 kubelet 自动重启该容器                      |
| `OnFailure` | 当容器终止运行切退出代码不为 0 时，由 kubelet 自动重启该容器 |
| `Never`     | 不论容器运行状态如何，kubelet 都不会重启该容器               |



# 容器探针

探针是由 kubelet 对容器执行的定期诊断，主要是为了保证我们在使用容器探针来帮助我们检测和保证 Pod 中的服务正常运行。要执行诊断，kubelet 调用由容器实现的 Handler，有三种类型的处理程序：

- ExecAction

  在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。

- TCPSocketAction

  对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。

- HTTPGetAction

  对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。

Kubelet 可以选择是否执行在容器上运行的三种探针执行和做出反应：

- livenessProbe

  **指示容器是否正在运行**。

  如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其**重启策略**的影响。如果容器不提供存活探针，则默认状态为 Success。

- readinessProbe

  **指示容器是否准备好服务请求**。

  如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。

- startupProbe

  **指示容器中的应用是否已经启动**。

  如果提供了启动探测(startup probe)，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。

## 检测探针 -就绪检测

- readinessProbe-httpget

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: readiness-httpget-pod
  namespace: default
spec:
  containers:
    - name: readiness-httpget-container
      image: escape/nginx-test:v1
      imagePullPolicy: IfNotPresent
      readinessProbe:
        httpGet:
          port: 80
          path: /index1.html
        initialDelaySeconds: 1
        periodSeconds: 3
```

## 检测探针 - 存活检测

- livenessProbe-exec

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-exec-pod
  namespace: default
spec:
  containers:
    - name: liveness-exec-container
      image: busybox
      imagePullPolicy: IfNotPresent
      command:
        - "/bin/sh"
        - "-c"
        - "touch /tmp/live ; sleep 60; rm -rf /tmp/live; sleep 3600 "
      livenessProbe:
        exec:
          command: ["test", "-e", "/tmp/live"]
        initialDelaySeconds: 1
        periodSeconds: 3
```

- livenessProbe-tcp

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-tcp
  namespace: default
spec:
  containers:
    - name: nginx
      image: escape/nginx-test:v1
      livenessProbe:
        initialDelaySeconds: 5
        timeoutSeconds: 1
        tcpSocket:
          port: 80
```

- livenessProbe-httpget

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-httpget-pod
  namespace: default
spec:
  containers:
    - name: liveness-httpget-container
      image: escape/nginx-test:v1
      imagePullPolicy: IfNotPresent
      ports:
        - name: http
          containerPort: 80
      livenessProbe:
        httpGet:
          port: http
          path: /index.html
        initialDelaySeconds: 1
        periodSeconds: 3
        timeoutSeconds: 10
```

## 启动退出动作

我们可以设置，在 Pod 启动和停止的时候执行某些操作。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
    - name: lifecycle-demo-container
      image: nginx
      lifecycle:
        postStart:
          exec:
            command:
              - "/bin/sh"
              - "-c"
              - "echo Hello from the postStart handler > /usr/share/message"
        preStop:
          exec:
            command:
              - "/bin/sh"
              - "-c"
              - "echo Hello from the poststop handler > /usr/share/message"
```

## Pod 的状态示例

Pod 中只有一个容器并且正在运行，容器成功退出

- 记录事件完成

- 如果 restartPolicy 为：

- - Always：重启容器；Pod phase 仍为 Running
  - OnFailure：Pod phase 变成 Succeeded
  - Never：Pod phase 变成 Succeeded

Pod 中只有一个容器并且正在运行，容器退出失败

- 记录失败事件

- 如果 restartPolicy 为：

- - Always：重启容器；Pod phase 仍为 Running
  - OnFailure：重启容器；Pod phase 仍为 Running
  - Never：Pod phase 变成 Failed

Pod 中有两个容器并且正在运行，容器 1 退出失败

- 记录失败事件

- 如果 restartPolicy 为：

- - Always：重启容器；Pod phase 仍为 Running
  - OnFailure：重启容器；Pod phase 仍为 Running
  - Never：不重启容器；Pod phase 仍为 Running

- 如果有容器 1 没有处于运行状态，并且容器 2 退出：

- - Always：重启容器；Pod phase 仍为 Running
  - OnFailure：重启容器；Pod phase 仍为 Running
  - Never：Pod phase 变成 Failed
  - 记录失败事件
  - 如果 restartPolicy 为：

Pod 中只有一个容器并处于运行状态，容器运行时内存超出限制

- 容器以失败状态终止

- 记录 OOM 事件

- 如果 restartPolicy 为 ：

- - Always：重启容器；Pod phase 仍为 Running
  - OnFailure：重启容器；Pod phase 仍为 Running
  - Never: 记录失败事件；Pod phase 仍为 Failed

Pod 正在运行，磁盘故障

- 杀掉所有容器。记录适当事件
- Pod phase 变成 Failed
- 如果使用控制器来运行，Pod 将在别处重建

Pod 正在运行，其节点被分段

- 节点控制器等待直到超时
- 节点控制器将 Pod phase 设置为 Failed
- 如果是用控制器来运行，Pod 将在别处重建

# pod 配置

##  pod控制器

- ReplicationController
- ReplicaSet
- Deployment
- DaemonSet
- StateFulSet
- Job
- CronJob
- Horizontal Pod Autoscaling

## Pod 模板

### .spec.template

书写配置文件的时候，.spec.template 是一个 pod 模板，它的模式与 pod 完全相同，只是它是嵌套的，没有 apiVersion 或 kind 属性。

### 设置标签

  - 优先级高 => .metadata.labels
  - 优先级低 => .spec.template.metadata.labels
  - 没有指定 .metadata.labels 则默认为 .spec.template.metadata.labels

控制器本身是可以设置标签的，用于做分类或者控制选择时使用。

### Pod 选择器

  - .spec.selector
  - .spec.template.metadata.labels

没有指定 .spec.selector 则默认为 .spec.template.metadata.labels 其中 .spec.selector 字段是一个标签选择器，所创建出来的标签用于选择则出匹配的 Pod。如果指定了 .spec.template.metadata.labels 字段，则必须和 .spec.selector 字段相同，否则在创建的时候将会被 API 拒绝。

### 多个副本

- .spec.replicas

可以通过设置 .spec.replicas 来指定应该同时运行多少个 Pod。在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如副本个数刚刚被增加或减少时，或者一个 pod 处于优雅终止过程中而其替代副本已经提前开始创建时。如果没有指定 .spec.replicas，那么它默认是 1 个。



# 补充

## 概念

![image-20211205113232391](https://tva1.sinaimg.cn/large/008i3skNgy1gx2su6xyifj317y0heaax.jpg)

![image-20211205113111520](https://tva1.sinaimg.cn/large/008i3skNgy1gx2sstu278j31dy0qm433.jpg)

![image-20211205113319583](https://tva1.sinaimg.cn/large/008i3skNgy1gx2sv0gcboj31bu0q0q77.jpg)

## 为什么pod是必须的

container之间需要相互依赖，必须同一机器共享某些信息，解决超亲密关系统一调度

各自为政的话

- 启动先后顺序，效率慢，互相等造成死锁

![image-20211205113129445](https://tva1.sinaimg.cn/large/008i3skNgy1gx2st3o5y3j31bg0qe43o.jpg)

如何决定pod有多少container

![image-20211205113449553](https://tva1.sinaimg.cn/large/008i3skNgy1gx2swkpb6wj31bc0p6tc9.jpg)

## 容器设计模式

![image-20211205120009967](https://tva1.sinaimg.cn/large/008i3skNgy1gx2tmxoyx8j31ac0osq6h.jpg)

可以使用init cointainer引入需要的资源

![image-20211205121137066](https://tva1.sinaimg.cn/large/008i3skNgy1gx2tyuodoij31bu0r479g.jpg)



这是一个通过组合两个不同角色的容器，并且按照这样一些像 Init Container 这样一种编排方式，统一的去打包这样一个应用，把它用 Pod 来去做的非常典型的一个例子。像这样的一个概念，在 Kubernetes 里面就是一个非常经典的容器设计模式，叫做**Sidecar**。

 

![image-20211205121254865](https://tva1.sinaimg.cn/large/008i3skNgy1gx2u07aurtj31aa0paq5q.jpg)

其实在 Pod 里面，可以**定义一些专门的容器，来执行主业务容器所需要的一些辅助工作**，比如我们前面举的例子，其实就干了一个事儿，这个 Init Container，它就是一个 Sidecar，它只负责把镜像里的 WAR 包拷贝到共享目录里面，以便被 Tomcat 能够用起来。

其它有哪些操作呢？比如说：

- 原本需要在容器里面执行 SSH 需要干的一些事情，可以写脚本、一些前置的条件，其实都可以通过像 Init Container 或者另外像 Sidecar 的方式去解决； 

- 当然还有一个典型例子就是我的日志收集，日志收集本身是一个进程，是一个小容器，那么就可以把它打包进 Pod 里面去做这个收集工作；

- 还有一个非常重要的东西就是 Debug 应用，实际上现在 Debug 整个应用都可以在应用 Pod 里面再次定义一个额外的小的 Container，它可以去 exec 应用 pod 的 namespace； 

- 查看其他容器的工作状态，这也是它可以做的事情。不再需要去 SSH 登陆到容器里去看，只要把监控组件装到额外的小容器里面就可以了，然后把它作为一个 Sidecar 启动起来，跟主业务容器进行协作，所以同样业务监控也都可以通过 Sidecar 方式来去做。

这种做法一个非常明显的优势就是在于**其实将辅助功能从我的业务容器解耦了，所以我就能够独立发布 Sidecar 容器，并且更重要的是这个能力是可以重用的**，即同样的一个监控 Sidecar 或者日志 Sidecar，可以被全公司的人共用的。这就是设计模式的一个威力。

## sidecar 应用场景

- Sidecar：应用与日志收集

  业务容器将日志写在一个 Volume 里面，而由于 Volume 在 Pod 里面是被共享的，所以日志容器 —— 即 Sidecar 容器一定可以通过共享该 Volume，直接把日志文件读出来，然后存到远程存储里面，或者转发到另外一个例子。现在业界常用的 Fluentd 日志进程或日志组件，基本上都是这样的工作方式。

  ![image-20211205122152039](https://tva1.sinaimg.cn/large/008i3skNgy1gx2u9iilnfj31ao0okju5.jpg)

- 代理容器 Proxy

  假如现在有个 Pod 需要访问一个外部系统，或者一些外部服务，但是这些外部系统是一个集群，那么这个时候如何通过一个统一的、简单的方式，用一个 IP 地址，就把这些集群都访问到？有一种方法就是：修改代码。因为代码里记录了这些集群的地址；另外还有一种解耦的方法，即通过 Sidecar 代理容器。

   

  简单说，单独写一个这么小的 Proxy，用来处理对接外部的服务集群，它对外暴露出来只有一个 IP 地址就可以了。所以接下来，业务容器主要访问 Proxy，然后由 Proxy 去连接这些服务集群，这里的关键在于 Pod 里面多个容器是通过 localhost 直接通信的，因为它们同属于一个 network Namespace，网络视图都一样，所以它们俩通信 localhost，并没有性能损耗。

  ![image-20211205122329732](https://tva1.sinaimg.cn/large/008i3skNgy1gx2ub7gqwtj31be0oaad3.jpg)

- 适配器容器 Adapter

  现在业务暴露出来的 API，比如说有个 API 的一个格式是 A，但是现在有一个外部系统要去访问我的业务容器，它只知道的一种格式是 API B ,所以要做一个工作，就是把业务容器怎么想办法改掉，要去改业务代码。但实际上，你可以通过一个 Adapter 帮你来做这层转换。

  ![image-20211205122856115](https://tva1.sinaimg.cn/large/008i3skNgy1gx2ugvjchyj31ek0q8juj.jpg)

# 元数据

## label

标签主要用来筛选资源和组合资源，可以使用类似于 SQL 查询 select，来根据 Label 查询相关的资源。

![image-20211212170830168](https://tva1.sinaimg.cn/large/008i3skNgy1gxb5vwrsmvj315w0iwwgd.jpg)

## Annotations

一般是系统或者工具用来存储资源的非标示性信息，可以用来扩展资源的 spec/status 的描述

![image-20211212171548248](https://tva1.sinaimg.cn/large/008i3skNgy1gxb63iobr6j316i0ladj2.jpg)

## Ownereference

所谓所有者，一般就是指集合类的资源，比如说 Pod 集合，就有 replicaset、statefulset，这个将在后序的课程中讲到。

**集合类资源的控制器会创建对应的归属资源**。比如：replicaset 控制器在操作中会创建 Pod，被创建 Pod 的 Ownereference 就指向了创建 Pod 的 replicaset，Ownereference 使得用户可以方便地查找一个创建资源的对象，另外，还可以用来实现级联删除的效果。
