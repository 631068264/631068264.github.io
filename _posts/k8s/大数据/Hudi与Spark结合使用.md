# Hudiä¸Sparkç»“åˆ

<aside>
ğŸ’¡ æœ¬æŒ‡å—æ€»ç»“HudiåŸºäºspark-shellã€spark-submit(javaæ¡ˆä¾‹)ï¼Œä½¿ç”¨spark datasourceæ¥æ¼”ç¤ºHudiçš„æ“ä½œHudiè¡¨çš„æ’å…¥ã€æ›´æ–°ã€è¯»å–å¿«ç…§å’Œå¢é‡æ•°æ®ç­‰åŠŸèƒ½ã€‚

</aside>

# è®¾ç½®ç‰ˆæœ¬

Hudiä¸Spark-2.4.3å’ŒSpark 3.Xç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥æŒ‰ç…§æ­¤å¤„çš„è¯´æ˜è®¾ç½®Sparkã€‚

**Spark 3 Support Matrix**

| Hudi | Supported Spark 3 version |
| --- | --- |
| 0.12.x | 3.3.x (default build), 3.2.x, 3.1.x |
| 0.11.x | 3.2.x (default build, Spark bundle only), 3.1.x |
| 0.10.x | 3.1.x (default build), 3.0.x |
| 0.7.0 - 0.9.0 | 3.0.x |
| 0.6.0 and prior | not supported |

é»˜è®¤çš„Build Sparkç‰ˆæœ¬è¡¨æ˜å®ƒè¢«ç”¨æ¥æ„å»ºHudi-Spark3-Bundleã€‚

å½“å‰æœ¬æœºæµ‹è¯•ç‰ˆæœ¬ï¼š

- spark:3.2.2
- hudi:0.13.1

---

# Spark-Shellæµ‹è¯•

æµ‹è¯•hudiçš„è¯»å†™åŠŸèƒ½ï¼Œä½¿ç”¨å¦‚ï¼šorg.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.1ä¾èµ–åŒ…, å¯åŠ¨å‘½ä»¤ï¼š

```bash
# Spark 3.2
spark-shell \
  --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.1 \
  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'
```

ä¸Šé¢å‘½ä»¤å› ä¸ºpackageså‚æ•°æ˜¯è¯»å–è¿œç¨‹æœåŠ¡å™¨å¯èƒ½ä¸‹è½½æ¯”è¾ƒæ…¢ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½jaråŒ…ï¼š

[https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.2-bundle_2.12/0.13.1/hudi-spark3.2-bundle_2.12-0.13.1.jar](https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.2-bundle_2.12/0.13.1/hudi-spark3.2-bundle_2.12-0.13.1.jar) ï¼Œ å­˜æ”¾åˆ°$SPARK_HOME/jarsç›®å½•å†… æˆ–è€… MinIOä¸­

ä¹‹åï¼Œå¯ä»¥ç›´æ¥å»é™¤packageså‚æ•°è¿›å…¥spark-shellæ§åˆ¶å°

```bash
[root@test001 spark-3.2.2-bin-hadoop3.2]# spark-shell \
--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'
23/07/10 16:16:46 WARN Utils: Your hostname, test001 resolves to a loopback address: 127.0.0.1; using 192.168.137.100 instead (on interface eth0)
23/07/10 16:16:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/07/10 16:16:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/10 16:16:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
Spark context Web UI available at http://minio.local:4040
Spark context available as 'sc' (master = local[*], app id = local-1688977010380).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.2
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

å…¶ä¸­ï¼Œæç¤ºhttp://minio.local:4040,  è¯´æ˜è®¿é—®å½“å‰IP:4040å¯ä»¥è¿›å…¥spark-historyç•Œé¢æŸ¥çœ‹

![Untitled](./img/hudi/01.png)

## è¡¨åè®¾ç½®

è®¾ç½®è¡¨åã€åŸºæœ¬è·¯å¾„å’Œæ•°æ®ç”Ÿæˆå™¨ä»¥ç”Ÿæˆæœ¬æŒ‡å—çš„è®°å½•ã€‚

```scala
// spark-shell
import org.apache.hudi.QuickstartUtils._
import scala.collection.JavaConversions._
import org.apache.spark.sql.SaveMode._
import org.apache.hudi.DataSourceReadOptions._
import org.apache.hudi.DataSourceWriteOptions._
import org.apache.hudi.config.HoodieWriteConfig._
import org.apache.hudi.common.model.HoodieRecord

val tableName = "hudi_trips_cow"
val basePath = "s3a://bigdatas/hudi-test/001"
val dataGen = new DataGenerator
```

## æ’å…¥æ•°æ®

scalaæ’å…¥ä»£ç ï¼š

```scala
// spark-shell
val inserts = convertToStringList(dataGen.generateInserts(10))
val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))
df.write.format("hudi").
  options(getQuickstartWriteConfigs).
  option(PRECOMBINE_FIELD_OPT_KEY, "ts").
  option(RECORDKEY_FIELD_OPT_KEY, "uuid").
  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").
  option(TABLE_NAME, tableName).
  mode(Overwrite).
  save(basePath)
```

æ‰§è¡Œæƒ…å†µï¼š

```bash
scala> val inserts = convertToStringList(dataGen.generateInserts(10))
inserts: java.util.List[String] = [{"ts": 1688468894617, "uuid": "a7df5838-1139-4560-a024-4a1027dc596d", "rider": "rider-213", "driver": "driver-213", "begin_lat": 0.4726905879569653, "begin_lon": 0.46157858450465483, "end_lat": 0.754803407008858, "end_lon": 0.9671159942018241, "fare": 34.158284716382845, "partitionpath": "americas/brazil/sao_paulo"}, "partitionpath": "americas/brazil/sao_paulo"}, {"ts": 1688517421513, "uuid": "11282742-a2bb-4072-997b-462a329c5bc2", "rider": "rider-213", "driver": "driver-213", "begin_lat": 0.6100070562136587, "begin_lon": 0.8779402295427752, "end_lat": 0.3407870505929602, "end_lon": 0.5030798142293655, "fare": 43.4923811219014, "partitionpath": "americas/brazil/sao_paulo"}, "partitionpath": "americas/brazil/sao_paulo"}, {"ts":...

scala> val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))
warning: one deprecation (since 2.12.0)
warning: one deprecation (since 2.2.0)
warning: two deprecations in total; for details, enable `:setting -deprecation' or `:replay -deprecation'
df: org.apache.spark.sql.DataFrame = [begin_lat: double, begin_lon: double ... 8 more fields]

scala> df.write.format("hudi").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD_OPT_KEY, "ts").
     |   option(RECORDKEY_FIELD_OPT_KEY, "uuid").
     |   option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").
     |   option(TABLE_NAME, tableName).
     |   mode(Overwrite).
     |   save(basePath)
warning: one deprecation; for details, enable `:setting -deprecation' or `:replay -deprecation'
23/07/10 16:27:14 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
23/07/10 16:27:14 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
23/07/10 16:27:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
23/07/10 16:27:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
23/07/10 16:27:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
23/07/10 16:27:17 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore root@127.0.0.1
23/07/10 16:27:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
23/07/10 16:27:18 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://bigdatas/hudi-test/001/.hoodie/metadata
23/07/10 16:27:23 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
```

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥åˆ°MinIOæŸ¥çœ‹æ˜¯å¦ç”Ÿæˆhudiç›¸å…³æ•°æ®è¡¨æ–‡ä»¶ï¼š

![Untitled](./img/hudi/02.png)

è¯´æ˜æ­¤æ—¶ï¼Œæ’å…¥æ•°æ®æˆåŠŸï¼Œå…¶ä¸­

1. **`.hoodie`** ç›®å½•ï¼š
    - **`.hoodie`** ç›®å½•æ˜¯ Hudi è¡¨çš„å…ƒæ•°æ®ç›®å½•ï¼Œç”¨äºå­˜å‚¨ Hudi è¡¨çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¡¨çš„é…ç½®ã€è¡¨çš„ç‰ˆæœ¬ä¿¡æ¯ã€å…¨å±€æ–‡ä»¶ç´¢å¼•ã€å†™å…¥è®°å½•ç­‰ã€‚å®ƒæ˜¯ Hudi è¡¨çš„æ ¸å¿ƒå…ƒæ•°æ®ç›®å½•ï¼Œç”¨äºç»´æŠ¤è¡¨çš„çŠ¶æ€å’Œä¸€è‡´æ€§ã€‚
2. **`americas`** ç›®å½•ï¼š
    - **`americas`** ç›®å½•æ˜¯ Hudi è¡¨æ•°æ®çš„å­˜å‚¨ç›®å½•ä¹‹ä¸€ã€‚åœ¨ Hudi ä¸­ï¼Œè¡¨çš„æ•°æ®é€šå¸¸è¢«åˆ†åŒºå­˜å‚¨ï¼Œæ¯ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªç›®å½•ã€‚**`americas`** ç›®å½•å¯ä»¥è¡¨ç¤ºä¸€ä¸ªåˆ†åŒºï¼Œç”¨äºå­˜å‚¨è¯¥åˆ†åŒºçš„æ•°æ®æ–‡ä»¶ï¼ˆä¾‹å¦‚ Parquet æ–‡ä»¶ï¼‰å’Œç´¢å¼•æ–‡ä»¶ã€‚
3. **`asia`** ç›®å½•ï¼š
    - **`asia`** ç›®å½•æ˜¯ Hudi è¡¨æ•°æ®çš„å¦ä¸€ä¸ªå­˜å‚¨ç›®å½•ï¼ŒåŒæ ·ç”¨äºå­˜å‚¨ç‰¹å®šåˆ†åŒºçš„æ•°æ®æ–‡ä»¶å’Œç´¢å¼•æ–‡ä»¶ã€‚ä¸ **`americas`** ç›®å½•ç±»ä¼¼ï¼Œ**`asia`** ç›®å½•è¡¨ç¤ºå¦ä¸€ä¸ªåˆ†åŒº

è¿™äº›ç›®å½•ç»“æ„æ˜¯ Hudi åœ¨å­˜å‚¨å’Œç®¡ç†è¡¨æ•°æ®æ—¶çš„ä¸€ç§çº¦å®šå’Œç»„ç»‡æ–¹å¼ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ**`.hoodie`** ç›®å½•å’Œåˆ†åŒºç›®å½•ä¸­çš„å…·ä½“æ–‡ä»¶ç»„ç»‡å½¢å¼æ˜¯ç”± Hudi å†…éƒ¨é€»è¾‘å’Œæ–‡ä»¶å‘½åè§„åˆ™å†³å®šçš„ï¼Œå¯¹äºç”¨æˆ·è€Œè¨€ï¼Œæ›´é‡è¦çš„æ˜¯ä½¿ç”¨ Hudi æä¾›çš„ API å’Œå·¥å…·è¿›è¡Œè¡¨çš„æ“ä½œå’Œæ•°æ®è¯»å†™ã€‚

---

## æŸ¥è¯¢æ•°æ®

scalaä»£ç ï¼š

```scala
// spark-shell
val tripsSnapshotDF = spark.
  read.
  format("hudi").
  load(basePath)
tripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")

spark.sql("select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0").show()
spark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot").show()
```

æ‰§è¡Œæ•ˆæœå›¾ï¼š

![Untitled](./img/hudi/03.png)

## æ—¶é—´æŸ¥è¯¢

```scala
spark.read.
  format("hudi").
  option("as.of.instant", "20210728141108100").
  load(basePath)

spark.read.
  format("hudi").
  option("as.of.instant", "2021-07-28 14:11:08.200").
  load(basePath)

// It is equal to "as.of.instant = 2021-07-28 00:00:00"
spark.read.
  format("hudi").
  option("as.of.instant", "2021-07-28").
  load(basePath)
```

## æ›´æ–°æ•°æ®

```scala
// spark-shell
val updates = convertToStringList(dataGen.generateUpdates(10))
val df = spark.read.json(spark.sparkContext.parallelize(updates, 2))
df.write.format("hudi").
  options(getQuickstartWriteConfigs).
  option(PRECOMBINE_FIELD_OPT_KEY, "ts").
  option(RECORDKEY_FIELD_OPT_KEY, "uuid").
  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").
  option(TABLE_NAME, tableName).
  mode(Append).
  save(basePath)
```

æ›´æ–°ä¹‹åï¼Œé‡æ–°æŸ¥è¯¢ï¼Œå‘ç°æ•°æ®å·²æ”¹å˜ï¼ˆå¯¹æ¯”ä¸Šé¢æ¡ˆä¾‹çš„ç¬¬1æ¬¡æŸ¥è¯¢ï¼‰

![Untitled](./img/hudi/04.png)

# ç¼–è¯‘hudiçš„jaråŒ…ä½¿ç”¨

# hudiæºç 

[GitHub - apache/hudi: Upserts, Deletes And Incremental Processing on Big Data.](https://github.com/apache/hudi#building-apache-hudi-from-source)

ä¸‹è½½0.13.1ç‰ˆæœ¬ï¼š[https://codeload.github.com/apache/hudi/zip/refs/tags/release-0.13.1](https://codeload.github.com/apache/hudi/zip/refs/tags/release-0.13.1)

```bash
# Checkout code and build
# git clone https://github.com/apache/hudi.git && cd hudi
wget [https://codeload.github.com/apache/hudi/zip/refs/tags/release-0.13.1](https://codeload.github.com/apache/hudi/zip/refs/tags/release-0.13.1)
cd hudi

mvn clean package -DskipTests -Dspark 3.2

cd /home/soft/
# æ‹·è´ç¼–è¯‘ä¹‹åçš„packagingç›®å½•åˆ°/home/softä¸‹
# Start command
spark-3.2.2-bin-hadoop3.2/bin/spark-shell \
  --jars `ls packaging/hudi-spark-bundle/target/hudi-spark3.2-bundle_2.12-*.jar` \
  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'
```

ä¹‹åï¼Œé‡å¤ä¸Šé¢çš„scalaä»£ç ï¼š**è¡¨åè®¾ç½® å’Œ æŸ¥è¯¢æ•°æ® éƒ¨åˆ†ä»£ç æ‰§è¡Œï¼Œèƒ½æŸ¥è¯¢åˆ°æ•°æ®ï¼Œè¯´æ˜ç¼–è¯‘çš„jaråŒ…æ­£å¸¸ã€‚**

# æµ‹è¯•java

## hudi-examples-sparkæµ‹è¯•

```bash
# åŸºç¡€ä¾èµ–åŒ…
/home/soft/spark-3.2.2-bin-hadoop3.2/jars/hudi-spark3.2-bundle_2.12-0.13.1.jar
/home/soft/spark-3.2.2-bin-hadoop3.2/jars/hudi-examples-common-0.13.1.jar
# æ¼”ç¤ºæ¡ˆä¾‹
/home/soft/spark-3.2.2-bin-hadoop3.2/examples/jars/hudi-examples-spark-0.13.1.jar
```

```bash
spark-submit  \
--class org.apache.hudi.examples.spark.HoodieWriteClientExample \
--num-executors  2 \
/home/soft/spark-3.2.2-bin-hadoop3.2/examples/jars/hudi-examples-spark-0.13.1.jar s3a://tmp/hoodie/sample-table hoodie_rt
```

æµ‹è¯•ä¹‹åï¼Œå¯ä»¥çœ‹åˆ°åœ¨MinIOå¯¹åº”/tmp/hoodie/sample-tableç”Ÿæˆäº†hudiæ•°æ®æ–‡ä»¶ï¼Œæ­¤æ—¶å¯ä»¥ä½¿ç”¨spark-shellæŸ¥çœ‹æ•°æ®ï¼š

```bash
spark-shell \
--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'
```

```scala
import org.apache.hudi.QuickstartUtils._
import scala.collection.JavaConversions._
import org.apache.spark.sql.SaveMode._
import org.apache.hudi.DataSourceReadOptions._
import org.apache.hudi.common.model.HoodieRecord

val tableName = "hoodie_rt"
val basePath = "s3a://tmp/hoodie/sample-table"
val dataGen = new DataGenerator

// spark-shell
val tripsSnapshotDF = spark.
  read.
  format("hudi").
  load(basePath)
tripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")

spark.sql("select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0").show()
spark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot").show(
```

```bash
# è¾“å‡ºç»“æœ
scala> spark.sql("select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0").show()
+-----------------+------------------+------------------+---+
|             fare|         begin_lon|         begin_lat| ts|
+-----------------+------------------+------------------+---+
|38.63372961020515|0.9065078444936647|0.6662084366450246|  0|
|81.37564420028626| 0.964603455586492|0.4106290929046368|  0|
+-----------------+------------------+------------------+---+

scala> spark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot").show()
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+
|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|               rider|              driver|              fare|
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+
|  20230711104948033|891c8a94-0d4f-4fb...|            2020/01/02|rider-20230711104...|driver-2023071110...| 38.63372961020515|
|  20230711104948033|6bb11672-28a3-42a...|            2020/01/02|rider-20230711104...|driver-2023071110...| 81.37564420028626|
|  20230711104948033|95805aaf-9024-452...|            2020/01/01|rider-20230711104...|driver-2023071110...|12.153670568058683|
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+
```

## hudi-examples-sparkä¿®æ”¹

åœ¨å®˜æ–¹æä¾›çš„HoodieWriteClientExample è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæ²¡æœ‰æ•°æ®çš„è¯»å–ï¼Œè¿™é‡Œå¯ä»¥å°è¯•ä¿®æ”¹æ·»åŠ ï¼š

```bash
// è¡¥å……æ‰“å°è¡¨ç»“æ„
SparkSession sparkSession = HoodieExampleSparkUtils.defaultSparkSession("hoodie-client-example");
Dataset<Row> tripsSnapshotDF  = sparkSession.read().format("hudi").load(tablePath);
tripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot");

// Query trips with fare greater than 20.0
sparkSession.sql("select fare, begin_lon, begin_lat, ts from hudi_trips_snapshot where fare > 20.0").show();

// Query all trips data
sparkSession.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from hudi_trips_snapshot").show();
client.close();
```

é‡æ–°ç¼–è¯‘æ‰§è¡Œï¼š

```bash
# åˆ‡æ¢åˆ°ä¸‹é¢æ–‡ä»¶å¤¹
hudi-release-0.13.1\hudi-examples\hudi-examples-spark
# æ‰§è¡Œ
mvn clean package -DskipTests
```

ä¹‹åé‡æ–°æ‰§è¡Œspark-submitï¼Œå¯ä»¥çœ‹åˆ°æœ€åä¼šåœ¨æ§åˆ¶å°è¾“å‡ºï¼š

```bash
23/07/11 11:26:26 INFO DAGScheduler: Job 79 finished: show at HoodieWriteClientExampleTest.java:157, took 0.040572 s
23/07/11 11:26:26 INFO CodeGenerator: Code generated in 16.207392 ms
+-----------------+------------------+------------------+---+
|             fare|         begin_lon|         begin_lat| ts|
+-----------------+------------------+------------------+---+
|38.63372961020515|0.9065078444936647|0.6662084366450246|  0|
|81.37564420028626| 0.964603455586492|0.4106290929046368|  0|
|81.37564420028626| 0.964603455586492|0.4106290929046368|  0|
|38.63372961020515|0.9065078444936647|0.6662084366450246|  0|
+-----------------+------------------+------------------+---+

23/07/11 11:26:26 INFO FileSourceStrategy: Pushed Filters: 
23/07/11 11:26:26 INFO FileSourceStrategy: Post-Scan Filters:
...
23/07/11 11:26:26 INFO CodeGenerator: Code generated in 16.549209 ms
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+
|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|               rider|              driver|              fare|
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+
|  20230711104948033|891c8a94-0d4f-4fb...|            2020/01/02|rider-20230711104...|driver-2023071110...| 38.63372961020515|
|  20230711104948033|6bb11672-28a3-42a...|            2020/01/02|rider-20230711104...|driver-2023071110...| 81.37564420028626|
|  20230711112614582|a83cc302-7512-486...|            2020/01/02|rider-20230711112...|driver-2023071111...| 81.37564420028626|
|  20230711112614582|50474432-358f-4ee...|            2020/01/02|rider-20230711112...|driver-2023071111...| 38.63372961020515|
|  20230711104948033|95805aaf-9024-452...|            2020/01/01|rider-20230711104...|driver-2023071110...|12.153670568058683|
|  20230711112614582|64343243-6765-427...|            2020/01/01|rider-20230711112...|driver-2023071111...|12.153670568058683|
+-------------------+--------------------+----------------------+--------------------+--------------------+------------------+

23/07/11 11:26:26 INFO BaseHoodieClient: Stopping Timeline service !!
23/07/11 11:26:26 INFO EmbeddedTimelineService: Closing Timeline server
23/07/11 11:26:26 INFO TimelineService: Closing Timeline Service
....
```

# Spark-Operatorç»“åˆä½¿ç”¨
```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: hudi-spark-071204
  namespace: spark
spec:
  type: Scala
  mode: cluster
  image: "umr/spark:3.2.2_v2"
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.hudi.examples.spark.HoodieWriteClientExampleTest
  arguments:
    - "s3a://bigdatas/datas/sample-table"
    - "hoodie_rt"
  mainApplicationFile: "s3a://bigdatas/jars/hudi-examples-spark-0.13.1-0704.jar"
  sparkVersion: "3.2.2"
  timeToLiveSeconds: 259200
  restartPolicy:
    type: Never
  volumes:
    - name: "test-volume"
      hostPath:
        path: "/tmp"
        type: Directory
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.2.2
    serviceAccount: spark
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.2.2
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"
  sparkConf:
    spark.ui.port: "4045"
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3a://sparklogs/all"
    spark.hadoop.fs.s3a.access.key: "minio"
    spark.hadoop.fs.s3a.secret.key: "minio123"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.endpoint: "http://10.19.64.205:32000"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    spark.jars: "s3a://bigdatas/jars/hudi-spark3.2-bundle_2.12-0.13.1.jar, s3a://bigdatas/jars/hudi-examples-common-0.13.1.jar"
```

# hudiä¸äº‘åŸç”Ÿ

Hudiï¼ˆHadoop Upserts Deletes and Incrementalsï¼‰åœ¨äº‘åŸç”Ÿç¯å¢ƒä¸‹ä½¿ç”¨å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. å¼¹æ€§å’Œå¯æ‰©å±•æ€§ï¼šäº‘åŸç”Ÿç¯å¢ƒæä¾›äº†å¼¹æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä½¿å¾—åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®å’Œé«˜å¹¶å‘å·¥ä½œè´Ÿè½½æ—¶æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚Hudi å¯ä»¥æ ¹æ®éœ€è¦è‡ªåŠ¨è°ƒæ•´èµ„æºï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„å·¥ä½œè´Ÿè½½éœ€æ±‚ã€‚
2. å¼¹æ€§å­˜å‚¨ï¼šåœ¨äº‘åŸç”Ÿç¯å¢ƒä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨äº‘å­˜å‚¨æœåŠ¡ï¼ˆå¦‚ Amazon S3ã€Azure Blob Storageã€Google Cloud Storageï¼‰ä½œä¸º Hudi è¡¨çš„åº•å±‚å­˜å‚¨ã€‚è¿™äº›äº‘å­˜å‚¨æœåŠ¡å…·æœ‰é«˜å¯é æ€§ã€å¯æ‰©å±•æ€§å’ŒæŒä¹…æ€§ï¼Œå¹¶ä¸”èƒ½å¤ŸæŒ‰éœ€å­˜å‚¨å’Œè®¿é—®æ•°æ®ã€‚
3. å¼¹æ€§è®¡ç®—ï¼šäº‘åŸç”Ÿç¯å¢ƒæä¾›äº†å¼¹æ€§è®¡ç®—èµ„æºï¼Œå¦‚äº‘æœåŠ¡å™¨å’Œå®¹å™¨æœåŠ¡ï¼ˆå¦‚ Amazon EC2ã€Azure VMã€Kubernetesï¼‰ã€‚ä½¿ç”¨ Hudi åœ¨äº‘åŸç”Ÿç¯å¢ƒä¸­ï¼Œæ‚¨å¯ä»¥æ ¹æ®å·¥ä½œè´Ÿè½½çš„éœ€æ±‚å¿«é€Ÿå¯åŠ¨å’Œåœæ­¢è®¡ç®—èµ„æºï¼Œä»¥å®ç°é«˜æ•ˆçš„æ•°æ®å¤„ç†å’Œåˆ†æã€‚
4. å®¹å™¨åŒ–æ”¯æŒï¼šäº‘åŸç”Ÿç¯å¢ƒé€šå¸¸ä½¿ç”¨å®¹å™¨æŠ€æœ¯ï¼Œå¦‚ Docker å’Œ Kubernetesï¼Œæ¥ç®¡ç†åº”ç”¨ç¨‹åºå’Œèµ„æºã€‚Hudi æä¾›äº†ä¸å®¹å™¨æŠ€æœ¯çš„è‰¯å¥½é›†æˆï¼Œå¯ä»¥è½»æ¾éƒ¨ç½²å’Œç®¡ç† Hudi åº”ç”¨ç¨‹åºï¼Œå¹¶å®ç°å¼¹æ€§ä¼¸ç¼©å’Œæ•…éšœæ¢å¤ã€‚
5. ä¸äº‘æœåŠ¡é›†æˆï¼šäº‘åŸç”Ÿç¯å¢ƒæä¾›äº†ä¸°å¯Œçš„äº‘æœåŠ¡å’Œå·¥å…·ï¼Œå¦‚ç›‘æ§ã€æ—¥å¿—ã€èº«ä»½è®¤è¯ã€å¯†é’¥ç®¡ç†ç­‰ã€‚Hudi å¯ä»¥ä¸è¿™äº›äº‘æœåŠ¡è¿›è¡Œé›†æˆï¼Œä»¥å®ç°å…¨é¢çš„æ•°æ®ç®¡æ§å’Œæ“ä½œã€‚
6. ä¸å…¶ä»–äº‘åŸç”Ÿå·¥å…·çš„é…åˆï¼šHudi å¯ä»¥ä¸å…¶ä»–äº‘åŸç”Ÿå·¥å…·å’ŒæŠ€æœ¯æ ˆï¼ˆå¦‚ Apache Kafkaã€Apache Flinkã€Apache Airflowï¼‰è¿›è¡Œé›†æˆï¼Œæ„å»ºç«¯åˆ°ç«¯çš„æ•°æ®å¤„ç†å’Œåˆ†ææµæ°´çº¿ï¼Œä»æ•°æ®æ‘„å–åˆ°æ•°æ®æ¹–å­˜å‚¨ã€æ‰¹å¤„ç†å’Œå®æ—¶å¤„ç†ç­‰ã€‚

æ€»ä½“è€Œè¨€ï¼Œå°† Hudi éƒ¨ç½²åœ¨äº‘åŸç”Ÿç¯å¢ƒä¸­ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨äº‘è®¡ç®—å’Œäº‘å­˜å‚¨çš„ä¼˜åŠ¿ï¼Œæä¾›å¼¹æ€§ã€å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®ç®¡ç†å’Œå¤„ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®å·¥ä½œè´Ÿè½½æä¾›æ›´å¥½çš„æ€§èƒ½å’Œå¯é æ€§ã€‚

# å‚è€ƒï¼š

[Spark Guide | Apache Hudi](https://hudi.apache.org/docs/quick-start-guide)