# å¤§æ•°æ®ä¸äº‘åŸç”Ÿç«å“åŠŸèƒ½åˆ†æ

<aside>
ğŸ’¡ å¦‚ä½•å®ç°å¤§æ•°æ®Hadoopã€Sparkã€Flinkã€Hbaseç­‰äº§å“ï¼Œåœ¨èäº‘ä¸Šä¸€é”®éƒ¨ç½²ï¼Œå¼€ç®±å³ç”¨çš„æ€è·¯å’Œå®è·µ

</aside>

# 1. ç«å“å‚è€ƒï¼ˆEMRï¼‰

é˜¿é‡Œäº‘ä¸Šæä¾›ä¸€äº›å¤§æ•°æ®ç›¸å…³çš„äº§å“ï¼š

[**E-MapReduce**](https://www.alibabacloud.com/help/zh/e-mapreduce/latest/getting-started) ï¼ˆç®€ç§°EMRï¼‰ï¼Œ é˜¿é‡Œäº‘ [Elastic MapReduce](https://www.alibabacloud.com/zh/product/emapreduce)ï¼ˆE-MapReduceï¼‰æ˜¯è¿è¡Œåœ¨é˜¿é‡Œäº‘å¹³å°ä¸Šçš„ä¸€å¥—å¤§æ•°æ®å¤„ç†çš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚E-MapReduce æ„å»ºäºé˜¿é‡Œäº‘äº‘æœåŠ¡å™¨ ECS å¼¹æ€§è™šæ‹Ÿæœºä¹‹ä¸Šï¼ŒåŸºäºå¼€æºçš„ Apache Hadoop å’Œ Apache Sparkï¼ˆk8sçš„Operatoræ¨¡å¼ï¼‰ï¼Œæ‚¨å¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨Hadoopå’ŒSparkç”Ÿæ€ç³»ç»Ÿä¸­çš„å…¶ä»–å‘¨è¾¹ç³»ç»Ÿï¼ˆå¦‚ Apache Hiveã€Apache Kafkaã€Flinkã€Druidã€TensorFlow ç­‰ï¼‰æ¥åˆ†æå’Œå¤„ç†è‡ªå·±çš„æ•°æ®ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡E-MapReduceå°†æ•°æ®éå¸¸æ–¹ä¾¿åœ°å¤„ç†é˜¿é‡Œäº‘å…¶ä»–çš„äº‘æ•°æ®å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®ï¼Œå¦‚OSSã€SLSã€RDS ç­‰ã€‚ä¸»è¦æ­¥éª¤ï¼š

- åˆ›å»ºé›†ç¾¤ï¼Œé›†ç¾¤åç§°ã€èº«ä»½å‡­è¯ã€äº§å“ç‰ˆæœ¬ã€æœåŠ¡é«˜å¯ç”¨ã€å¯é€‰æœåŠ¡(HDFS\YARN\Hive\Sparkå’ŒTEZï¼Œè¢«é€‰ä¸­çš„ç»„ä»¶ä¼šé»˜è®¤å¯åŠ¨ç›¸å…³çš„æœåŠ¡è¿›ç¨‹)
- åˆ›å»ºå¹¶æ‰§è¡Œä½œä¸šï¼Œé€šè¿‡SSHæ–¹å¼é“¾æ¥é›†ç¾¤ï¼Œæäº¤å¹¶è¿è¡Œä½œä¸šï¼š

```bash
spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --driver-memory 512m --num-executors 1 --executor-memory 1g --executor-cores 2 /usr/lib/spark-current/examples/jars/spark-examples_2.12-3.1.1.jar 10
```

- æŸ¥çœ‹ä½œä¸šè¿è¡Œè®°å½•ï¼Œé€šè¿‡YARN UIã€Hadoopæ§åˆ¶å°æŸ¥çœ‹ä½œä¸šè¿è¡Œçš„è¯¦æƒ…
- (å¯é€‰)é‡Šæ”¾é›†ç¾¤ï¼Œç¡®è®¤é‡Šæ”¾é›†ç¾¤ï¼Œä¼šå¼ºåˆ¶ç»ˆæ­¢é›†ç¾¤ä¸Šçš„æ‰€æœ‰ä½œä¸šã€ç»ˆæ­¢å¹¶é‡Šæ”¾æ‰€æœ‰ECSå®ä¾‹

[**DataWorks on EMR**](https://www.alibabacloud.com/help/zh/e-mapreduce/latest/dataworks)ï¼Œ DataWorksæ”¯æŒåŸºäºEMRï¼ˆE-MapReduceï¼‰è®¡ç®—å¼•æ“åˆ›å»ºHiveã€MRã€Prestoå’ŒSpark SQLç­‰èŠ‚ç‚¹ï¼Œå®ç°EMRä»»åŠ¡å·¥ä½œæµçš„é…ç½®ã€å®šæ—¶è°ƒåº¦å’Œå…ƒæ•°æ®ç®¡ç†ç­‰åŠŸèƒ½ï¼Œå¸®åŠ©EMRç”¨æˆ·æ›´å¥½åœ°äº§å‡ºæ•°æ®ã€‚

# 2. è¯•ç”¨ä½“éªŒEMR on ACK

[å¿«é€Ÿå…¥é—¨ - E-MapReduce - é˜¿é‡Œäº‘](https://www.alibabacloud.com/help/zh/e-mapreduce/latest/start-ack)

é€šè¿‡é˜¿é‡Œäº‘è´¦å·ç™»å½•E-MapReduceæ§åˆ¶å°ï¼ŒåŸºäºKubernetesåˆ›å»ºE-MapReduceï¼ˆç®€ç§°EMRï¼‰é›†ç¾¤å¹¶æ‰§è¡Œä½œä¸šã€‚ä½¿ç”¨EMRä¹‹å‰ï¼Œéœ€è¦å…ˆæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

- k8sé›†ç¾¤ã€ossç®¡ç†ç­‰æƒé™ï¼Œè§’è‰²æˆæƒ
- åˆ›å»ºKubernetesé›†ç¾¤ï¼Œä¸»è¦æ˜¯åˆ›å»º[Kubernetesä¸“æœ‰ç‰ˆé›†ç¾¤](https://www.alibabacloud.com/help/zh/container-service-for-kubernetes/latest/create-an-ack-dedicated-cluster?spm=a2c63.p38356.0.0.46e07148YmiR5M#task-skz-qwk-qfb)æˆ–è€…åˆ›å»º[Kubernetesæ‰˜ç®¡ç‰ˆé›†ç¾¤](https://www.alibabacloud.com/help/zh/container-service-for-kubernetes/latest/create-an-ack-managed-cluster?spm=a2c63.p38356.0.0.46e07148YmiR5M#task-skz-qwk-qfb)
- åˆ›å»º[èŠ‚ç‚¹æ± ](https://www.alibabacloud.com/help/zh/container-service-for-kubernetes/latest/manage-node-pools?spm=a2c63.p38356.0.0.46e07148YmiR5M#section-eq0-lmv-4a7)ï¼ˆç®¡ç†é›†ç¾¤èŠ‚ç‚¹æ•°ã€å•èŠ‚ç‚¹æœ€å¤§Podæ•°ç­‰ç­‰ï¼‰
- å¼€é€šå¯¹è±¡å­˜å‚¨OSS

é€šè¿‡å®¹å™¨acké¢æ¿åˆ›å»ºk8sé›†ç¾¤ä¹‹åï¼Œå†ç™»å½•[EMR on ACKæ§åˆ¶å°](https://emr-next.console.aliyun.com/#/region/cn-hangzhou/resource/all/ack/list)ã€‚

é›†ç¾¤è®¿é—®æ–¹å¼ï¼Œ**é€šè¿‡ kubectl è¿æ¥ Kubernetes é›†ç¾¤ï¼ˆå†…ç½‘æˆ–è€…SSHï¼‰**

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/01.png)

## driverPotTemplate.yaml

```bash
apiversion: v1
kind: Pod
metadata:
  labels:
    product: emr
    spark/pod.monitor: "true"
spec:
  serviceAccountName: spark
  containers:
    - name: spark-kubernetes-driver
      env:
        - name: SPARKLOGENV
          value: spark-driver
        - name: SPARK_CONF_DIR
          value: /etc/spark/conf
        - name: SPARK_SUBMIT_OPTS
          value: -javaagent:/opt/spark/jars/jmx_prometheus_javaagent-0.16.1.jar=8090:/etc/metrics/conf/prometheus.yaml
      ports:
        - containerPort: 8090
          name: jmx-exporter
          protocol: TCP
  nodeSelector:
    emr-spark: emr-spark
  tolerations:
    - key: emr-node-taint
      effect: NoSchedule
      operator: Equal
      value: emr
```

## executorPodTemplate.yaml

```bash
apiversion: v1
kind: Pod
metadata:
  labels:
    product: emr
    spark/pod.monitor: "true"
spec:
  serviceAccountName: spark
  enableServiceLinks: false
  automountServiceAccountToken: false
  containers:
    - name: spark-kubernetes-executor
      env:
        - name: SPARKLOGENV
          value: spark-executor
      ports:
        - containerPort: 8090
          name: jmx-exporter
          protocol: TCP
  nodeSelector:
    emr-spark: emr-spark
  tolerations:
    - key: emr-node-taint
      effect: NoSchedule
      operator: Equal
      value: emr
```

## log4j.properties

```bash
log4j.rootCategory=INFO, console, file
log4j.appender.console=com.alibaba.emr.ack.AsyncConsoleAppender
log4j.appender.console.layoutPattern=%d{yy/MM/dd HH:mm:ss} %p [%t] %c{1}: %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR

log4j.appender.file=org.apache.log4j.FileAppender
log4j.appender.file.append=true
log4j.appender.file.file=/opt/spark/log/spark.log
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n
```

## spark-default.conf

spark.kubernetes.driver.podTemplateFile

```bash
/etc/spark/conf/driverPodTemplate.yaml
```

spark.kubernetes.executor.podTemplateFile

```bash
/etc/spark/conf/executorPodTemplate.yaml
```

spark.kubernetes.authenticate.driver.serviceAccountName

```bash
spark
```

ack.clusterid.for.rss.linked

```bash

```

spark.kubernetes.container.image.pullSecrets

```bash
emr-image-regsecret
```

spark.shuffle.manager

```bash
org.apache.spark.shuffle.sort.SortShuffleManager
```

spark.serializer

```bash
org.apache.spark.serializer.KryoSerializer
```

## oauth-config.conf

allowed-accounts

```bash
 1791744788253780
```

# 3. é…ç½®kubectlè®¿é—®é›†ç¾¤ã€æäº¤ä½œä¸š

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/02.png)

```bash
[root@node-6 ~]# cd ~/.kube/
[root@node-6 .kube]# ls
[root@node-6 .kube]# vi config
[root@node-6 .kube]# kubectl get nodes
NAME                        STATUS   ROLES    AGE   VERSION
cn-hangzhou.192.168.0.143   Ready    <none>   32m   v1.22.15-aliyun.1
cn-hangzhou.192.168.0.144   Ready    <none>   32m   v1.22.15-aliyun.1

[root@node-6 .kube]# kubectl get pod  -A | grep operator
c-16504ffdf9e3e5d9   sparkoperator-fb6b678d6-jc9vm                              1/1     Running     0             16m
c-16504ffdf9e3e5d9   sparkoperator-webhook-init-8fw8x                           0/1     Completed   0             16m
kube-system          storage-operator-545bdc9dc7-bpf2b                          1/1     Running     0             37m
```

ç¼–è¾‘spark-pi.yaml

```bash
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-pi-simple
spec:
  type: Scala
  sparkVersion: 3.2.1
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: "local:///opt/spark/examples/spark-examples.jar"
  arguments:
    - "1000"
  driver:
    cores: 1
    coreLimit: 1000m
    memory: 1g
  executor:
    cores: 1
    coreLimit: 1000m
    memory: 1g
    memoryOverhead: 1g
    instances: 1
```

æäº¤ä½œä¸š

```bash
kubectl apply -f spark-pi.yaml --namespace c-16504ffdf9e3e5d9
```

æŸ¥çœ‹ç›¸å…³èµ„æº

```bash
[root@node-6 test-emr]# kubectl get SparkApplication -n c-16504ffdf9e3e5d9
NAME              STATUS      ATTEMPTS   START                  FINISH       AGE
spark-pi-simple   SUBMITTED   1          2023-02-02T09:20:33Z   <no value>   79s
```

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/03.png)

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/04.png)

# 4.æäº¤Sparkä½œä¸šçš„æ–¹å¼æ±‡æ€»

[](https://help.aliyun.com/document_detail/212167.html?spm=a2cug.25178945.help.dexternal.736f12513A39VF)

# 5.EMR-ACKå…¶ä»–ç•Œé¢å’ŒåŠŸèƒ½

åŸºäºå·²æœ‰acké›†ç¾¤éƒ¨ç½²EMR-ACK

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/05.png)

EMR-ACKé›†ç¾¤åˆ›å»ºè¿›åº¦

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/06.png)

EMR-ACKé›†ç¾¤è¯¦æƒ…

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/07.png)

å¤§æ•°æ®sparkç»„ä»¶é…ç½®

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/08.png)

å¤§æ•°æ®sparkç›¸å…³è®¿é—®UIé“¾æ¥

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/09.png)

å¤§æ•°æ®ç»„ä»¶æœåŠ¡è¯¦æƒ…

![Untitled](./img/%E9%98%BF%E9%87%8C%E4%BA%91EMR%E7%AB%9E%E5%93%81/10.png)