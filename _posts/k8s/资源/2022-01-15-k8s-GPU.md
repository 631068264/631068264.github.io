---
layout:     post
rewards: false
title:   k8s GPU
categories:
    - k8s

---

![img](https://cdn.jsdelivr.net/gh/631068264/img/008i3skNgy1gyfaxw8f4xj31360katav-20221230094903918.jpg)

# GPU 容器镜像原理

如下图左边所示，

- 最底层是先安装 Nvidia 硬件驱动；
- 再到上面是通用的 Cuda 工具库；
- 最上层是 PyTorch、TensorFlow 这类的机器学习框架。

上两层的 CUDA 工具库和应用的耦合度较高，应用版本变动后，对应的 CUDA 版本大概率也要更新

而最下层的 Nvidia 驱动，通常情况下是比较稳定的，它不会像 CUDA 和应用一样，经常更新。

![img](https://cdn.jsdelivr.net/gh/631068264/img/008i3skNgy1gyfanmgy26j31640lyn00-20221230095022992.jpg)

同时 **Nvidia 驱动需要内核源码编译**，如上图右侧所示，英伟达的 GPU 容器方案是：**在宿主机上安装 Nvidia 驱动**，而在 CUDA 以上的软件交给容器镜像来做。同时把 Nvidia 驱动里面的链接以 Mount Bind 的方式映射到容器中。这样的一个好处是：当你安装了一个新的 Nvidia 驱动之后，你就可以在同一个机器节点上运行不同版本的 CUDA 镜像了。

# 利用容器运行 GPU 程序

![image-20220116102745893](https://cdn.jsdelivr.net/gh/631068264/img/202212301012077.jpg)

![image-20220116102910623](https://cdn.jsdelivr.net/gh/631068264/img/202212301012079.jpg)

在运行时刻一个 GPU 容器和普通容器之间的差别，仅仅在于**需要将宿主机的设备和 Nvidia 驱动库映射到容器中**。

上图右侧反映了 GPU 容器启动后，容器中的 GPU 配置。右上方展示的是设备映射的结果，右下方显示的是驱动库以 Bind 方式映射到容器后，可以看到的变化。

通常大家会使用 Nvidia-docker 来运行 GPU 容器，而 Nvidia-docker 的实际工作就是来自动化做这两个工作。

其中挂载设备比较简单，而**真正比较复杂的是 GPU 应用依赖的驱动库**。对于深度学习，视频处理等**不同场景，所使用的一些驱动库并不相同。这又需要依赖 Nvidia 的领域知识，而这些领域知识就被贯穿到了 Nvidia 的容器之中。**



当用户的容器被创建之后，这个容器里必须出现如下两部分设备和目录：

1. GPU 设备，比如 /dev/nvidia0；
2. GPU 驱动目录，比如 /usr/local/nvidia/*。



# 部署 GPU Kubernetes

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012080.jpg)

- 首先安装 Nvidia 驱动

  由于 Nvidia 驱动需要内核编译，所以在安装 Nvidia 驱动之前需要**安装 gcc 和内核源码**。

- 第二步通过 yum 源，安装 Nvidia Docker2

  安装完 Nvidia Docker2 需要重新加载 docker，可以检查 docker 的 daemon.json 里面默认启动引擎已经被替换成了 nvidia，也可以通过 docker info 命令查看运行时刻使用的 runC 是不是 Nvidia 的 runC。 

- 第三步是部署 Nvidia Device Plugin。

  从 Nvidia 的 git repo 下去下载 Device Plugin 的部署声明文件，并且通过 kubectl create 命令进行部署。这里 Device Plugin 是以 deamonset 的方式进行部署的。这样我们就知道，如果需要排查一个 Kubernetes 节点无法调度 GPU 应用的问题，需要从这些模块开始入手，比如我要查看一下 Device Plugin 的日志，Nvidia 的 runC 是否配置为 docker 默认 runC 以及 Nvidia 驱动是否安装成功。



在k8s 上支持gpu 设备调度，需要做

1. 节点上安装 nvidia 驱动 （对物理机） Install the NVIDIA driver on the nodes
2. 节点上安装 nvidia-docekr （对接docker）Install the NVIDIA Docker runtime on the nodes
3. 集群部署 gpu device plugin （对接kubelet）Deploy the GPU device plugin on the cluster to expose the GPU resources to the Kubernetes scheduler and allow for GPU device scheduling.

每种方案都有自己独立的一套 Kubernetes 集成实现方式，通常是由**调度器 + device plugin** 组成。这些方案相互独立，没有统一标准，无法共通。这导致用户在**单个集群中很难同时使用多种 GPU 后端技术，同时也没有一个全局的视角获取集群层面 GPU 信息。**



# 验证部署 GPU Kubernetes 结果

当 GPU 节点部署成功后，我们可以从节点的状态信息中发现相关的 GPU 信息。

- 一个是 GPU 的名称，这里是 nvidia.com/gpu；
- 另一个是它对应的数量，如下图所示是 2，表示在该节点中含有两个 GPU。

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012081.jpg)

站在用户的角度，在 Kubernetes 中使用 GPU 容器还是非常简单的。只需要在 Pod 资源配置的 limit 字段中指定 **nvidia.com/gpu** 使用 GPU 的数量，如下图样例中我们设置的数量为 1；然后再通过 kubectl create 命令将 GPU 的 Pod 部署完成。

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012082.jpg)

# 工作原理

Kubernetes 本身是通过插件扩展的机制来管理 GPU 资源的，具体来说这里有两个独立的内部机制。

![image-20220116120240065](https://cdn.jsdelivr.net/gh/631068264/img/202212301012083.jpg)

- 第一个是 Extend Resources，允许用户自定义资源名称。而该**资源的度量是整数级别**，这样做的目的在于通过一个通用的模式支持不同的异构设备，包括 RDMA、FPGA、AMD GPU 等等，而不仅仅是为 Nvidia GPU 设计的；

- Device Plugin Framework 允许第三方设备提供商以外置的方式对设备进行全生命周期的管理，而 Device Plugin Framework 建立 Kubernetes 和 Device Plugin 模块之间的桥梁。它一方面负责设备信息的上报到 Kubernetes，另一方面负责设备的调度选择。







## Extended Resource 的上报

Extend Resources 属于 Node-level 的 api，完全可以独立于 Device Plugin 使用。The Extend Resources feature is a Node-level API and can be used independently of the Device Plugin

而上报 Extend Resources，只需要通过一个 PACTH API 对 Node 对象进行 status 部分更新即可，而**这个 PACTH 操作可以通过一个简单的 curl 命令来完成。这样，在 Kubernetes 调度器中就能够记录这个节点的 GPU 类型**，它所对应的资源数量是 1。Reporting Extend Resources, you can update the status section of the Node object using a PATCH API.

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012084.jpg)

当然如果使用的是 Device Plugin，就不需要做这个 PACTH 操作，只需要遵从 Device Plugin 的编程模型，在设备上报的工作中 Device Plugin 就会完成这个操作。



## Device Plugin 工作机制

介绍一下 Device Plugin 的工作机制，整个 Device Plugin 的工作流程可以分成两个部分：

- 一个是启动时刻的资源上报；
- 另一个是用户使用时刻的调度和运行。

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012085.jpg)

Device Plugin 的开发非常简单。主要包括最关注与最核心的两个事件方法：

- 其中 ListAndWatch 对应**资源的上报，同时还提供健康检查的机制**。当设备不健康的时候，可以上报给 Kubernetes 不健康设备的 ID，让 Device Plugin Framework 将这个设备从可调度设备中移除 The ListAndWatch method corresponds to the reporting of resources and also provides a mechanism for health checks. When a device is unhealthy, you can report the ID of the unhealthy device to Kubernetes, allowing the Device Plugin Framework to remove this device from the pool of schedulable devices.
- 而 **Allocate 会被 Device Plugin 在部署容器时调用**，传入的参数核心就是容器会使用的设备 ID，返回的参数是**容器启动时**，需要的设备、数据卷以及环境变量。 the Allocate method is called by the Device Plugin during container deployment. The core parameter passed to this method is the device ID that the container will use. The returned parameters are the devices, volumes, and environment variables required for the container to start.

大概流程：

- 对于每一种硬件设备，都需要有它所对应的 Device Plugin 进行管理，这些 Device Plugin，都通过 gRPC 的方式同 kubelet 连接起来。以 NVIDIA GPU 为例，它对应的插件叫作NVIDIA GPU device plugin。DevicePlugin 注册一个socket 文件到 `/var/lib/kubelet/device-plugins/` 目录下，Kubelet 通过这个目录下的socket 文件向对应的 DevicePlugin 发送gRPC 请求。PS： **通过目录做服务发现。**For each type of hardware device, there needs to be a corresponding Device Plugin to manage it. These Device Plugins are connected to kubelet through gRPC. Taking NVIDIA GPU as an example, the corresponding plugin is called the NVIDIA GPU device plugin. The DevicePlugin registers a socket file in the `/var/lib/kubelet/device-plugins/` directory, and kubelet sends gRPC requests to the corresponding DevicePlugin through this directory. PS: **Service discovery is done through the directory**.
- Device Plugin 会通过一个叫作 ListAndWatch 的 API，定期向 kubelet 汇报该 Node 上 GPU 的列表。比如，在上图的例子里，一共有三个 GPU（GPU0、GPU1 和 GPU2）。这样，kubelet 在拿到这个列表之后，就可以直接在它向 APIServer 发送的心跳里，以 Extended Resource 的方式，加上这些 GPU 的数量，比如nvidia.com/gpu=3。The Device Plugin periodically reports the list of GPUs on the Node to kubelet through an API called ListAndWatch。After kubelet receives this list, it can directly add the number of these GPUs as an Extended Resource in the heartbeat it sends to the APIServer,
- 当 kubelet 发现这个 Pod 的容器请求一个 GPU 的时候，kubelet 就会从自己持有的 GPU 列表里，为这个容器分配一个 GPU。此时，kubelet 就会向本机的 Device Plugin 发起一个 `Allocate()` 请求。这个请求携带的参数，正是即将分配给该容器的设备 ID 列表。When kubelet detects that a container in this Pod requests a GPU, it will allocate a GPU from its own list of GPUs for this container. At this point, kubelet initiates an `Allocate()` request to the local Device Plugin. The parameters carried by this request are the device ID list that will be allocated to the container.
- 当 Device Plugin 收到 Allocate 请求之后，它就会根据 kubelet 传递过来的设备 ID，从 Device Plugin 里**找到这些设备对应的设备路径和驱动目录**。比如，在 NVIDIA Device Plugin 的实现里，它会定期访问 nvidia-docker 插件，从而获取到本机的 GPU 信息。而被分配 GPU 对应的设备路径和驱动目录信息被返回给 kubelet 之后，kubelet 就完成了为一个容器分配 GPU 的操作。When the Device Plugin receives the Allocate request, it will use the device IDs passed by kubelet to **find the device paths and driver directories** corresponding to these devices in the Device Plugin。After the device paths and driver directory information corresponding to the allocated GPU are returned to kubelet, kubelet completes the operation of allocating a GPU to a container.
- 接下来，**kubelet 会把这些信息追加在创建该容器所对应的 CRI 请求当中**。这样，当这个 CRI 请求发给 Docker 之后，Docker 为你创建出来的容器里，就会出现这个 GPU 设备，并把它所需要的驱动目录(/dev/nvidia*)挂载进去。**kubelet will append this information to the CRI request corresponding to the creation of the container**. In this way, when this CRI request is sent to Docker, the GPU device will appear in the container created by Docker, and the required driver directory will be mounted into it.



### 资源上报和监控

对于每一个硬件设备，都需要它所对应的 Device Plugin 进行管理，这些 Device Plugin 以客户端的身份**通过 GRPC 的方式对 kubelet 中的 Device Plugin Manager 进行连接**，并且将自己监听的 Unis socket api 的版本号和设备名称比如 GPU，上报给 kubelet。

 

我们来看一下 Device Plugin 资源上报的整个流程。总的来说，整个过程分为四步，其中前三步都是发生在节点上，第四步是 kubelet 和 api-server 的交互。

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012086.jpg)

- 第一步是 Device Plugin 的注册，需要 Kubernetes 知道要跟哪个 Device Plugin 进行交互。这是因为一个节点上可能有多个设备，需要 Device Plugin 以客户端的身份向 Kubelet 汇报三件事情。

  - 我是谁？就是 Device Plugin 所管理的设备名称，是 GPU 还是 RDMA；

  - 我在哪？就是插件自身监听的 unis socket 所在的文件位置，让 kubelet 能够调用自己；

  - 交互协议，即 API 的版本号。

- 第二步是服务启动，Device Plugin 会启动一个 GRPC 的 server。在此之后 Device Plugin 一直以这个服务器的身份提供服务让 kubelet 来访问，而监听地址和提供 API 的版本就已经在第一步完成了

- 第三步，当该 GRPC server 启动之后，kubelet 会建立一个到 Device Plugin 的 ListAndWatch 的长连接， 用来发现设备 ID 以及设备的健康状态。当 Device Plugin 检测到某个设备不健康的时候，就会主动通知 kubelet。而此时如果这个设备处于空闲状态，kubelet 会将其移除可分配的列表。但是当这个设备已经被某个 Pod 所使用的时候，kubelet 就不会做任何事情，如果此时杀掉这个 Pod 是一个很危险的操作

- 第四步，kubelet 会将这些设备暴露到 Node 节点的状态中，把设备数量发送到 Kubernetes 的 api-server 中。后续调度器可以根据这些信息进行调度。

需要注意的是 kubelet 在**向 api-server** 进行汇报的时候，**只会汇报该 GPU 对应的数量**。而 **kubelet** 自身的 Device Plugin Manager 会对这个 **GPU 的 ID 列表进行保存**，并用来具体的设备分配。而这个对于 Kubernetes 全局调度器来说，它不掌握这个 GPU 的 ID 列表，它只知道 GPU 的数量。这就意味着在**现有的 Device Plugin 工作机制下，Kubernetes 的全局调度器无法进行更复杂的调度**。比如说想做两个 **GPU 的亲和性调度**，同一个节点两个 GPU 可能需要进行通过 NVLINK 通讯而不是 PCIe 通讯，才能达到更好的数据传输效果。在这种需求下，目前的 Device Plugin 调度机制中是无法实现的。

### Pod 的调度和运行的过程

![img](https://cdn.jsdelivr.net/gh/631068264/img/202212301012087.jpg)

Pod 想使用一个 GPU 的时候，它只需要像之前的例子一样，在 Pod 的 Resource 下 limits 字段中声明 GPU 资源和对应的数量 (比如nvidia.com/gpu: 1)。Kubernetes 会找到满足数量条件的节点，然后将该节点的 GPU 数量减 1，并且完成 Pod 与 Node 的绑定。绑定成功后，自然就会被对应节点的 kubelet 拿来创建容器。

而当 kubelet 发现这个 Pod 的容器请求的资源是一个 GPU 的时候，**kubelet 就会委托自己内部的 Device Plugin Manager 模块，从自己持有的 GPU 的 ID 列表中选择一个可用的 GPU 分配给该容器**。此时 kubelet 就会向本机的 DeAvice Plugin 发起一个 Allocate 请求，这个请求所携带的参数，正是即将分配给该容器的设备 ID 列表。

**Device Plugin 收到 AllocateRequest 请求之后，它就会根据 kubelet 传过来的设备 ID，去寻找这个设备 ID 对应的设备路径、驱动目录以及环境变量，并且以 AllocateResponse 的形式返还给 kubelet。**

AllocateResponse 中所携带的设备路径和驱动目录信息，一旦返回给 kubelet 之后，kubelet 就会根据这些信息执行为容器分配 GPU 的操作，这样 Docker 会根据 kubelet 的指令去创建容器，而这个容器中就会出现 GPU 设备。并且把它所需要的驱动目录给挂载进来，至此 Kubernetes 为 Pod 分配一个 GPU 的流程就结束了。

 