---
layout:     post
rewards: false
title:   GPU节点 docker k8s 适配
categories:
    - k8s

---

# 准备

在每个工作节点上都安装 NVIDIA GPU 或 AMDGPU 驱动程序，如下所述 。 使用 NVIDIA GPU 的系统要求包括:

- NVIDIA 驱动程序的版本为 384.81 及以上;
- nvidia-docker 的版本为 2.0 及以上;
- kubelet 配置的容器运行时 (Container Runtime) 必须为 Docker;
- Docker 配置的默认运行时 (Default Runtime) 必须为 nvidia-container-runtime, 而不能用runc;
- Kubernetes 版本为 1.11 及以上 。

- [检查系统要求](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements)

[参考预安装操作](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions)

```sh
# Verify You Have a CUDA-Capable GPU
lspci | grep -i nvidia

# Verify You Have a Supported Version of Linux
uname -m && cat /etc/*release

# Verify the System Has gcc Installed
gcc --version

# Verify the System has the Correct Kernel Headers and Development Packages Installed
uname -r
```





# NVIDIA Drivers

[docker环境部署参考](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)



## 检查驱动

检查驱动是否存在

使用`nvcc -V`检查驱动和cuda。

```sh
# 存在
vcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Wed_Jul_22_19:09:09_PDT_2020
Cuda compilation tools, release 11.0, V11.0.221
Build cuda_11.0_bu.TC445_37.28845127_0
```

然后`nvidia-smi` 有报错参考[重新安装驱动](#重新安装驱动)



## 查看型号

```
lspci | grep -i nvidia


root@d-ecs-38357230:~# lspci | grep -i nvidia
00:08.0 3D controller: NVIDIA Corporation Device 1eb8 (rev a1)
```

返回一个**1eb8**，可以通过[PCI devices查询](http://pci-ids.ucw.cz/mods/PC/10de?action=help?help=pci)

![image-20220412105920629](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h1atm5dr4wj21bu0u0wk8.jpg)

![image-20220412105957003](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h1atm5si9tj21be0l40un.jpg)



也可以直接安装[NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)

> The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.

根据 https://github.com/NVIDIA/nvidia-docker **CUDA Toolkit**不是必须的，驱动是必须的



## 安装驱动

[查看支持显卡的驱动最新版本及下载，下载之后是.run后缀](https://www.nvidia.com/Download/index.aspx?lang=en-us) （可以直接下载驱动）

![image-20220413095333085](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h1atm4et97j20yy0fwdhm.jpg)



```sh
sudo sh NVIDIA-Linux-x86_64-384.90.run -no-x-check -no-nouveau-check -no-opengl-files
```

代码注释：

-no-x-check  #安装驱动时关闭X服务

-no-nouveau-check  #安装驱动时禁用nouveau

-no-opengl-files  #只安装驱动文件，不安装OpenGL文件

或者

```
更新软件源，运行
apt-get upgrade
apt-cache search nvidia-* |grep 455 查询455版本的驱动是否存在
nvidia-driver-418-server - NVIDIA Server Driver metapackage
nvidia-driver-440-server - NVIDIA Server Driver metapackage
nvidia-driver-450-server - NVIDIA Server Driver metapackage
nvidia-driver-455 - NVIDIA driver metapackage
安装 apt-get install nvidia-driver-455 -y
安装完reboot系统
```



查看cuda版本

```sh
root@d-ecs-38357230:~# cat /usr/local/cuda/version.txt
CUDA Version 11.0.228
```

查看显卡驱动版本

```bash
root@d-ecs-38357230:~# cat /proc/driver/nvidia/version
NVRM version: NVIDIA UNIX x86_64 Kernel Module  450.89  Thu Oct 22 20:49:26 UTC 2020
GCC version:  gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
```


验证驱动

```bash
nvidia-smi -L （必须安装好nvidia驱动）

root@d-ecs-38357230:~# nvidia-smi -L
GPU 0: Tesla T4 (UUID: GPU-6b1d3e62-f94c-9236-1398-813bb48aab5a)
```



## 重新安装驱动

> NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. 
> Make sure that the latest NVIDIA driver is installed and running.

运行`nvidia-smi`时，报错。

使用`nvcc -V`检查驱动和cuda。

```sh
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
```

发现驱动是存在的，于是进行下一步

查看已安装驱动的版本信息

```sh
ls /usr/src | grep nvidia
nvidia-450.57
```

重新安装

```sh
apt-get install dkms
dkms install -m nvidia -v 450.57
```

等待安装完成后，再次输入`nvidia-smi`，查看GPU使用状态



# nvidia-docker2

`/etc/docker/daemon.json`，修改docker Runtime

```yaml
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
```

根据[Setting up Docker 安装nvidia-docker2](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#setting-up-docker)

Setup the package repository and the GPG key:

```sh
$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

Install the `nvidia-docker2` package (and dependencies) after updating the package listing:

```sh
$ sudo apt-get update
```



```sh
$ sudo apt-get install -y nvidia-docker2
```



Restart the Docker daemon to complete the installation after setting the default runtime:

```sh
systemctl daemon-reload && systemctl restart docker
```



At this point, a working setup can be tested by running a base CUDA container:

```sh
$ sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```



This should result in a console output shown below:

```sh
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

**注意事项**

如果存在这种现象：
"没有运行程序，nvidia-smi查看GPU-Util 达到100% GPU利用率很高"
需要把驱动模式设置为常驻内存才可以，设置命令：

>  nvidia-smi -pm 1



# k8s

[K8s](https://kubernetes.io/zh/docs/tasks/manage-gpus/scheduling-gpus/) + [Nvidia-Device-Plugin](https://github.com/NVIDIA/k8s-device-plugin)的方式两个限制

- 每一块GPU同时最多只能被一个容器使用(会造成巨大的资源浪费)
- 没有考虑GPU卡之间的通道亲和性(因为不同的连接介质对卡与卡之间的数据传输速度影响非常大)

所以使用[AliyunContainerService 提供Pod 之间共享 GPU解决方案](https://github.com/AliyunContainerService/gpushare-scheduler-extender)

- https://blog.csdn.net/u012751272/article/details/120566202?spm=1001.2014.3001.5502
- https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/install.md



## 配置调度器配置文件

从 **Kubernetes v1.23** 开始[不再支持调度策略，](https://kubernetes.io/docs/reference/scheduling/policies/)而是应该使用[调度程序配置。](https://kubernetes.io/docs/reference/scheduling/config/)这意味着`scheduler-policy-config.yaml`需要包含在调度程序配置 ( `/etc/kubernetes/manifests/kube-scheduler.yaml`) 中。这是最终修改的[kube-scheduler.yaml的示例](https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/config/kube-scheduler-v1.23+.yaml)

> 注意：如果您的 Kubernetes 默认调度程序部署为静态 pod，请不要编辑 /etc/kubernetes/manifest 中的 yaml 文件。您需要在`/etc/kubernetes/manifest`目录外编辑 yaml 文件。并将您编辑的yaml文件复制到'/etc/kubernetes/manifest/'目录，然后kubernetes会自动使用yaml文件更新默认的静态pod。



```sh
# 下载 调度器配置文件
cd /etc/kubernetes
curl -O https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/scheduler-policy-config.yaml
```

因为结合rke，需要修改**kubeconfig**路径

```yaml
---
apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /etc/kubernetes/ssl/kubecfg-kube-scheduler.yaml # 根据rke scheduler kubeconfig路径
extenders:
- urlPrefix: "http://127.0.0.1:32766/gpushare-scheduler"
  filterVerb: filter
  bindVerb: bind
  enableHTTPS: false
  nodeCacheCapable: true
  managedResources:
  - name: aliyun.com/gpu-mem
    ignoredByScheduler: false
  ignorable: false
```



使用rke配置cluster.yml  [scheduler config](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-scheduler/)

```yaml
services:
    scheduler:
      extra_args:
        config: /etc/kubernetes/scheduler-policy-config.yaml
```

kube-scheduler容器启动，通过`docker inspect kube-scheduler`已经挂载了**/etc/kubernetes**，所以不用修改scheduler本身

```json
"Mounts": [
            {
                "Type": "bind",
                "Source": "/etc/kubernetes",
                "Destination": "/etc/kubernetes",
                "Mode": "",
                "RW": true,
                "Propagation": "rprivate"
            },
```



```sh
# 更新配置
./rke up -config cluster.yml --ignore-docker-version
```



## 平面部署GPU共享调度扩展器

```sh
kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/gpushare-schd-extender.yaml
```

**注意事项**

- 单节点需要修改gpushare-schd-extender.yaml，不然会**0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector.**  [参考](https://github.com/AliyunContainerService/gpushare-scheduler-extender/issues/170)

  ```yaml
      spec:
        hostNetwork: true
        tolerations:
  #      - effect: NoSchedule
  #        operator: Exists
  #        key: node-role.kubernetes.io/master
        - effect: NoSchedule
          operator: Exists
          key: node.cloudprovider.kubernetes.io/uninitialized
  #      nodeSelector:
  #         node-role.kubernetes.io/master: ""
  ```



## 部署设备插件

```sh
kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-rbac.yaml

kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-ds.yaml
```

**注意事项**

- 默认情况下，GPU显存以GiB为单位，若需要使用MiB为单位，修改**device-plugin-ds.yaml**，将--memory-unit=GiB修改为--memory-unit=MiB



## 配置节点

[下载kubectl](https://www.downloadkubernetes.com/)

```sh
chmod +x /usr/bin/kubectl

# gpushare 插件
https://github.com/AliyunContainerService/gpushare-device-plugin/releases/download/v0.3.0/kubectl-inspect-gpushare

chmod +x /usr/bin/kubectl-inspect-gpushare
```



为要安装设备插件的所有节点添加标签“gpushare=true”，因为设备插件是 deamonset

```sh
kubectl get nodes
NAME           STATUS   ROLES                      AGE   VERSION
xxx   Ready    controlplane,etcd,worker   47m   v1.18.6
# 加标签
kubectl label node xxx gpushare=true

# 如果node状态不对 SchedulingDisabled
kubectl patch node kube-node1 -p '{"spec":{"unschedulable":false}}'
```

测试

```sh
kubectl get pods -n kube-system |grep gpushare

gpushare-device-plugin-ds-z27jc           1/1     Running     0          23m
gpushare-schd-extender-56799db8c7-8gshd   1/1     Running     0          25m

# 通过插件看GPU分配情况
kubectl-inspect-gpushare

NAME          IPADDRESS     GPU0(Allocated/Total)  GPU Memory(GiB)
10.81.25.224  10.81.25.224  0/14                   0/14
--------------------------------------------------------
Allocated/Total GPU Memory In Cluster:
0/14 (0%)  
```



## 测试

部署官方测试[demo](https://github.com/AliyunContainerService/gpushare-scheduler-extender/tree/master/samples)



```yaml
apiVersion: apps/v1
kind: Deployment

metadata:
  name: binpack-1
  labels:
    app: binpack-1

spec:
  replicas: 3

  selector: # define how the deployment finds the pods it mangages
    matchLabels:
      app: binpack-1

  template: # define the pods specifications
    metadata:
      labels:
        app: binpack-1

    spec:
      containers:
      - name: binpack-1
        image: cheyang/gpu-player:v2
        resources:
          limits:
            # GiB
            aliyun.com/gpu-mem: 1  

```



```shell
# 查看情况
kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
binpack-1-79dd548c78-9c49p   1/1     Running   0          4m23s
binpack-1-79dd548c78-f9bg2   1/1     Running   0          4m23s
binpack-1-79dd548c78-z2v6p   1/1     Running   0          4m23s


kubectl-inspect-gpushare
NAME          IPADDRESS     GPU0(Allocated/Total)  GPU Memory(GiB)
10.81.25.224  10.81.25.224  3/14                   3/14
--------------------------------------------------------
Allocated/Total GPU Memory In Cluster:
3/14 (21%)  
```



