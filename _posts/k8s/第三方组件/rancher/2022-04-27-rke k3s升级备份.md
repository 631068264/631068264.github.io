---
layout:     post
rewards: false
title:   rancher k3s 升级 备份恢复
categories:
    - k8s
tags:
    - rancher
---

# rke

## 安装命令

cluster.yml 是配置文件

```
rke up -config cluster.yml
```

https://rancher.com/docs/rke/latest/en/example-yamls/ example



## 升级条件

RKE v0.2.0 及以上的版本使用`cluster.rkestate`文件管理集群状态。`cluster.rkestate`文件中含有集群的当前状态，包括 RKE 配置和证书等信息。

这个文件和`cluster.yml`位于同一目录下。

`cluster.rkestate`文件非常重要，控制集群和升级集群的时候都需要用到这个文件，请妥善保管该文件。



### 升级流程

RKE v1.1.0 及以上的版本提供了以下新功能：

- 支持不宕机升级，编辑或升级集群时，不会影响集群内的应用。
- 支持手动指定单个节点，并单独升级这个节点。
- 支持使用包含老版本的 Kubernetes 集群快照，将集群使用的 Kubernetes 恢复到先前的版本。这个功能使升级集群的过程变得更加安全。如果某次升级的过程中，集群中有些节点无法完成更新，导致该节点内的 pods 和应用无法访问，您可以将已经完成升级的集群所使用的 Kubernetes 降级为升级前使用的版本。

使用默认配置选项，输入`rke up`命令更新集群时，会依次触发以下事项：

- 逐个更新每个节点的 etcd plane。RKE 集群内的任何一个 etcd 节点在升级的过程中失效，会导致集群升级失败。
- 逐个更新 controlplane 节点，包括 controlplane 组件和 controlplane 节点的 worker plane 组件。
- 逐个更新每个 etcd 节点的 worker plane 组件。
- 批量更新 worker 节点，您可以配置每批更新的节点数量。在批量更新 worker 节点的过程中，可能会有部分节点不可用，为了降低这部分节点对于升级的影响，您可以配置最大不可用节点数量。默认的最大不可用节点数量是总节点数量的 10%，最小值为 1，如果是小数则向下取整至最近的一个整数，详情请参考下文。
- 更新[每个插件](https://docs.rancher.cn/docs/rke/config-options/add-ons/_index)。



## 升级k8s

### k8s

输入以下命令，快速获取支持K8s版本号

```sh
rke config --list-version --all
v1.15.3-rancher2-1
v1.13.10-rancher1-2
v1.14.6-rancher2-1
v1.16.0-beta.1-rancher1-1
```

指定k8s版本安装，修改配置文件

```yaml
kubernetes_version: v1.10.3-rancher2
```

[不同的版本需要到的镜像版本](https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go)

**升级 Kubernetes 版本**，打开`cluster.yml`文件，找到 `kubernetes_version`字符串，将原有的版本号修改为新的版本号即可。重新执行安装命令。

如果在`kubernetes_version`和`system_images`中都定义了 Kubernetes 版本，`system_images`中定义的版本会生效，而`kubernetes_version`中定义的版本不会生效。如果两者都没有定义 Kubernetes 版本，RKE 会使用默认的 Kubernetes 版本。

### docker

k8s支持的**docker版本**

https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG

```yaml
# If set to true, RKE will not fail when unsupported Docker version
# are found，默认是false
ignore_docker_version: false
```



## 升级服务

是指`cluster.yml`文件里面的service

可以修改服务的对象，或添加`extra_args`，然后运行`rke up`命令，升级服务。

> **说明：** `service_cluster_ip_range` 和 `cluster_cidr`不可修改。

## 升级system_images

system_images更新替换对应的镜像组件，但要注意组件兼容。**一般不随便替换**

## 添加/删除节点

https://rancher.com/docs/rke/latest/en/managing-clusters/

```sh
# Adding/Removing Nodes
rke up -config cluster.yml

# Adding/Removing Worker Nodes
rke up --update-only -config cluster.yml

# Removing Kubernetes Components from Nodes
rke remove
```

The cluster’s etcd snapshots are removed, including both local snapshots and snapshots that are stored on S3.

**Pods are not removed from the nodes. If the node is re-used, the pods will automatically be removed when the new Kubernetes cluster is created.**

- Clean each host from the directories left by the services:
  - /etc/kubernetes/ssl
  - /var/lib/etcd
  - /etc/cni
  - /opt/cni
  - /var/run/calico

## 创建快照

打开命令行工具，输入`rke etcd snapshot-save`命令，运行后即可保存 cluster config 文件内每个 etcd 节点的快照。RKE 会将节点快照保存在`/opt/rke/etcd-snapshots`路径下。运行上述命令时，RKE 会创建一个用于备份快照的容器。完成备份后，RKE 会删除该容器。

https://docs.rancher.cn/docs/rke/etcd-snapshots/one-time-snapshots/_index

```sh
# 运行以下命令，在本地创建一个一次性快照
rke etcd snapshot-save --config cluster.yml --name snapshot-name

WARN[0000] This is not an officially supported version (v1.2.16-rc1) of RKE. Please download the latest official release at https://github.com/rancher/rke/releases

INFO[0018] Finished saving/uploading snapshot [snapshot-name] on all etcd hosts
```

保存到**/opt/rke/etcd-snapshots**

```sh
root@d-ecs-38357230:~/rancher# ll /opt/rke/etcd-snapshots
total 3040
drwxr-xr-x 2 root root    4096 Jun  9 16:23 ./
drwxr-xr-x 3 root root    4096 Jun  9 11:33 ../
-rw------- 1 root root 3103107 Jun  9 16:23 snapshot-name.zip
```

上传到minio

```sh
rke etcd snapshot-save \
--config cluster.yml \
--name snapshot-name \
--s3 \
--access-key admin \
--secret-key 12345678 \
--bucket-name rancher-backup \
--s3-endpoint back.xxxx.cn \ 
--s3-endpoint-ca minio.crt   # https公钥文件路径
```



## 恢复快照

https://docs.rancher.cn/docs/rke/etcd-snapshots/restoring-from-backup/_index

您的 Kubernetes 集群发生了灾难，您可以使用`rke etcd snapshot-restore`来恢复您的 etcd。这个命令可以将 etcd 恢复到特定的快照，应该**在遭受灾难的特定集群的 etcd 节点上运行**。

当您运行该命令时，将执行以下操作。

- 同步快照或从 S3 下载快照(如有必要)。
- 跨 etcd 节点检查快照校验和，确保它们是相同的。
- 通过运行`rke remove`删除您当前的集群并清理旧数据。这将删除整个 Kubernetes 集群，而不仅仅是 etcd 集群。
- 从选择的快照重建 etcd 集群。
- 通过运行`rke up`创建一个新的集群。
- 重新启动集群系统 pod。

> **警告：**在运行`rke etcd snapshot-restore`之前，您应该备份集群中的任何重要数据，因为该命令会删除您当前的 Kubernetes 集群，并用新的集群替换。



```sh
# 本地
rke etcd snapshot-restore --config cluster.yml --name snapshot-name

# minio
rke etcd snapshot-restore \
--config cluster.yml \
--name snapshot-name \
--s3 \
--access-key admin \
--secret-key 12345678 \
--bucket-name rancher-backup \
--s3-endpoint back.xxxx.cn \ 
--s3-endpoint-ca minio.crt
```

> 有自定义配置例如

```yaml
nodes:
  - address: xxxxx
    user: root
    role: ["controlplane", "etcd", "worker"]
    ssh_key_path: ~/.ssh/id_rsa
    port: 22
services:
  kube-api:
    # 为NodePort服务提供不同的端口范围
    service_node_port_range: 20000-32767
  scheduler:
      extra_args:
        policy-config-file: /etc/kubernetes/scheduler-policy-config.json # 例如这种配置

kubernetes_version: v1.19.16-rancher1-1
cluster_name: cluster01
```

恢复过程中会重建**/etc/kubernetes**目录，然后**scheduler-policy-config.json**会丢失导致恢复失败，所以要在恢复过程中配好配置



## 轮换证书

- etcd
- kubelet (node certificate)
- kubelet（服务证书，如果[启用](https://docs.rancher.cn/docs/rke/config-options/services/_index#kubelet-选项))
- kube-apiserver
- kube-proxy
- kube-scheduler
- kube-controller-manager

**/etc/kubernetes/ssl**，证书目录

```sh
rke cert rotate --rotate-ca
```

因为证书改变，相应的`token`也会变化，所以在完成集群证书更新后，需要对连接`API SERVER`的 Pod 进行重建，以获取新的`token`。

例如：

搭建集群，添加网络后，coredns异常

```
kubectl get pods -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-5dc887ffbb-2s8jd             0/1     Running   0          16h
```

coredns日志如下

```sh
085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Namespace: Unauthorized
pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Endpoints: Unauthorized
pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Service: Unauthorized
```

查看api server日志

```sh
Unable to authenticate the request due to an error: [invalid bearer token, square/go-jose: error in cryptographic primitive

```

处理方式：

删除coredns-token和pods后恢复

```
kubectl delete secret  -n kube-system coredns-token-6n887
kubectl delete pods coredns-5dc887ffbb-zgff5
```

原理：

一般发生在你自定义ca，或者升级集群的时候。secret保留的 仍然是依据旧的ca 生成的token，这时需要手动删除token 和pods，让集群根据新的ca重新生成新的token,新的pod 也能使用新的token去访问。



# k3s

## 升级

为了在升级期间实现高可用性，K3s 容器在 K3s 服务停止时继续运行。

要停止所有 K3s 容器并重置 containerd 状态，`k3s-killall.sh`可以使用该脚本。

**killall 脚本清理容器、K3s 目录和网络组件，同时还删除 iptables 链以及所有相关规则。集群数据不会被删除。**

要从服务器节点运行 killall 脚本，请运行

```sh
/usr/local/bin/k3s-killall.sh
```

- 从[发布](https://github.com/rancher/k3s/releases)中下载所需版本的 K3s 二进制文件

-  将下载的二进制文件复制到`/usr/local/bin/k3s`（或您想要的位置）
-  重启k3s服务和代理

```sh
systemctl restart k3s
systemctl restart k3s-agent


service k3s restart
service k3s-agent restart
```

## 创建快照

默认情况下启用快照。

存放数据的目录由`k3s server --data-dir ${data-dir}` 确定

`${data-dir}` 默认 **/var/lib/rancher/k3s** or **${HOME}/.rancher/k3s**(如果不是root用户)

快照目录默认为 `${data-dir}/server/db/snapshots`

完整快照目录默认

```
/var/lib/rancher/k3s/server/db/snapshots
```

k3s 默认每12hour备份一次，保留5个备份分别用`--etcd-snapshot-schedule-cron`和 `--etcd-snapshot-retention`定义，[都是k3s server 的子命令](https://docs.rancher.cn/docs/k3s/backup-restore/_index#%E5%8F%82%E6%95%B0)

```sh
ll /var/lib/rancher/k3s/server/db/snapshots
total 104460
-rw------- 1 root root 21389344 Jun  8 12:00 etcd-snapshot-1654660800
-rw------- 1 root root 21389344 Jun  9 00:00 etcd-snapshot-1654704000
-rw------- 1 root root 21389344 Jun  9 12:00 etcd-snapshot-1654747200
-rw------- 1 root root 21389344 Jun 10 00:00 etcd-snapshot-1654790400
-rw------- 1 root root 21389344 Jun 10 12:00 etcd-snapshot-1654833600

```

这些选项可以通过命令行传入，也可以在[配置文件中传入](https://rancher.com/docs/k3s/latest/en/installation/install-options/#configuration-file),这样可能更容易使用。

| 选项                                | 描述                                                         |
| :---------------------------------- | :----------------------------------------------------------- |
| `--etcd-disable-snapshots`          | 禁用自动 etcd 快照                                           |
| `--etcd-snapshot-schedule-cron`价值 | cron 规范中的快照间隔时间。例如。每 5 小时`0 */5 * * *`(默认: `0 */12 * * *`) |
| `--etcd-snapshot-retention`价值     | 要保留的快照数量（默认值：5）                                |
| `--etcd-snapshot-dir`价值           | 保存数据库快照的目录。（默认位置：`${data-dir}/db/snapshots`） |
| `--cluster-reset`                   | 忘记所有对等点，成为新集群的唯一成员。这也可以使用环境变量来设置`[$K3S_CLUSTER_RESET]`。 |
| `--cluster-reset-restore-path`价值  | 要恢复的快照文件的路径                                       |

## 从快照恢复集群

当 K3s 从备份中恢复时，旧的数据目录将被移动到`${data-dir}/server/db/etcd-old/`. 然后 K3s 将尝试通过创建一个新的数据目录来恢复快照，然后使用具有一个 etcd 成员的新 K3s 集群启动 etcd。

要从备份中恢复集群，请使用以下`--cluster-reset`选项运行 K3s，`--cluster-reset-restore-path`同时给出：当 K3s 从备份中恢复时，旧的数据目录将被移动到 `${data-dir}/server/db/etcd-old/`。然后 K3s 会尝试通过创建一个新的数据目录来恢复快照，然后从一个带有一个 etcd 成员的新 K3s 集群启动 etcd。

```sh
systemctl stop k3s

k3s server --cluster-reset --cluster-reset-restore-path /var/lib/rancher/k3s/server/db/snapshots/etcd-snapshot-1654833600
```

成功结果

```
INFO[2022-06-10T15:24:05.311564146+08:00] Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log
INFO[2022-06-10T15:24:05.311691243+08:00] Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd
{"level":"warn","ts":"2022-06-10T15:24:05.311+0800","caller":"grpclog/grpclog.go:60","msg":"grpc: addrConn.createTransport failed to connect to {/run/k3s/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = \"transport: Error while dialing dial unix /run/k3s/containerd/containerd.sock: connect: no such file or directory\". Reconnecting..."}
INFO[2022-06-10T15:24:05.886266149+08:00] Etcd is running, restart without --cluster-reset flag now. Backup and delete ${datadir}/server/db on each peer etcd server and rejoin the nodes

```

**结果：** 日志中的一条消息说 K3s 可以在没有标志的情况下重新启动。再次启动 k3s，应该会成功运行并从指定的快照恢复。

启动k3s

```
systemctl start k3s
```

旧数据

```sh
[root@1xxx db]# ll
total 12
drwx------ 3 root root 4096 Jun 10 15:29 etcd
drwx------ 3 root root 4096 Jun  1 15:34 etcd-old-1654845840
drwx------ 2 root root 4096 Jun 10 12:00 snapshots
[root@1xxx db]# pwd
/var/lib/rancher/k3s/server/db
```

# rancher使用rke添加集群

## 设置备份etcd

高级选项可以设置备份，默认会设置备份

![image-20220609155115716](https://tva1.sinaimg.cn/large/e6c9d24egy1h33kwu99nwj21hm0u041h.jpg)



## 还原

选择集群

![image-20220609155740195](https://tva1.sinaimg.cn/large/e6c9d24egy1h33kwumwjoj212k0s4dih.jpg)

选择工具-> 备份，可以手动备份新的，或者根据时间选择还原
![image-20220609155842826](https://tva1.sinaimg.cn/large/e6c9d24egy1h33kwv8f4ij21hj0u0q6k.jpg)

# kubeadm创建的k8s

## 备份

安装etcd-client

```sh
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>
```

获得 `trusted-ca-file`、`cert-file` 和 `key-file`。通过**etcd pod describe**获得

```sh
kubectl describe pod/etcd-master -n kube-system

Containers:
  etcd:
    Container ID:  docker://027f21079b676178393c73ff4d8adddb788dc78fc361a7d9482281bc8e3f3dae
    Image:         k8s.gcr.io/etcd:3.5.1-0
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:64b9ea357325d5db9f8a723dcf503b5a449177b17ac87d69481e126bb724c263
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://10.211.55.32:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --initial-advertise-peer-urls=https://10.211.55.32:2380
      --initial-cluster=master=https://10.211.55.32:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://10.211.55.32:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://10.211.55.32:2380
      --name=master
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Fri, 10 Jun 2022 08:11:49 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
```



```sh
# 备份参数
ETCDCTL_API=3 etcdctl   snapshot save  -h


NAME:
	snapshot save - Stores an etcd node backend snapshot to a given file

USAGE:
	etcdctl snapshot save <filename> [flags]

OPTIONS:
  -h, --help[=false]	help for save

GLOBAL OPTIONS:
      --cacert=""				verify certificates of TLS-enabled secure servers using this CA bundle
      --cert=""					identify secure client using this TLS certificate file
      --command-timeout=5s			timeout for short running command (excluding dial timeout)
      --debug[=false]				enable client-side debug logging
      --dial-timeout=2s				dial timeout for client connections
      --endpoints=[127.0.0.1:2379]		gRPC endpoints
      --hex[=false]				print byte strings as hex encoded strings
      --insecure-skip-tls-verify[=false]	skip server certificate verification
      --insecure-transport[=true]		disable transport security for client connections
      --key=""					identify secure client using this TLS key file
      --user=""					username[:password] for authentication (prompt if password is not supplied)
  -w, --write-out="simple"			set the output format (fields, json, protobuf, simple, table)
```



最终

```sh
# 创建快照
mkdir -p backup_etcd

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save backup_etcd/snap-$(date +%Y%m%d%H%M).db
  
# 验证 
ETCDCTL_API=3 etcdctl --write-out=table snapshot status backup_etcd/snap-202206100917.db

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| a669ed44 |   156435 |       1457 |      11 MB |
+----------+----------+------------+------------+



```

- 最好不好直接复制**${data-dir}/member/snap/db**文件，还原时验证快照完整性，用命令会计算hash供恢复时候校验用。

- 不要轻易使用status check，会破坏数据库完整性

  不然恢复时候报错

  ```sh
  Error:  expected sha256 [192 34 116 2 72 172 108 91 102 14 204 205 193 70 163 254 156 27 205 148 190 171 99 63 234 149 201 220 37 154 105 68], got [178 153 122 181 91 182 33110 230 141 239 175 40 71 74 48 165 184 179 124 41 121 133 130 18 145 113 190 86 163 186 184]
  ```

以上可以在恢复时候添加**--skip-hash-check=true** 参数



## 恢复

- 优先停掉所有api-server
- 恢复etcd
- 重启api-server

其实迁移重启所有的 组件 (e.g. `kube-scheduler`, `kube-controller-manager`, `kubelet`) ,以确保它们不依赖于一些陈旧的数据

**停止前**

![image-20220610180406356](https://tva1.sinaimg.cn/large/e6c9d24egy1h33kywkkndj227s0ke157.jpg)



```sh
# 防止kubelet重启静态pod, 相当于停掉etcd kube-apiserver kube-controller-manager kube-scheduler
mv /etc/kubernetes/manifests  /etc/kubernetes/manifests.bak


mv /var/lib/etcd /var/lib/etcd.bak

# 使用查看是否停止
docker ps|grep k8s_
```

**停止后**

![image-20220610221709512](https://tva1.sinaimg.cn/large/e6c9d24egy1h33kyx48aaj227q0bgwl1.jpg)



```sh
# 恢复参数
ETCDCTL_API=3 etcdctl snapshot restore  -h


NAME:
	snapshot restore - Restores an etcd member snapshot to an etcd directory

USAGE:
	etcdctl snapshot restore <filename> [options] [flags]

OPTIONS:
      --data-dir=""						Path to the data directory
  -h, --help[=false]						help for restore
      --initial-advertise-peer-urls="http://localhost:2380"	List of this member's peer URLs to advertise to the rest of the cluster
      --initial-cluster="default=http://localhost:2380"		Initial cluster configuration for restore bootstrap
      --initial-cluster-token="etcd-cluster"			Initial cluster token for the etcd cluster during restore bootstrap
      --name="default"						Human-readable name for this member
      --skip-hash-check[=false]					Ignore snapshot integrity hash value (required if copied from data directory)

GLOBAL OPTIONS:
      --cacert=""				verify certificates of TLS-enabled secure servers using this CA bundle
      --cert=""					identify secure client using this TLS certificate file
      --command-timeout=5s			timeout for short running command (excluding dial timeout)
      --debug[=false]				enable client-side debug logging
      --dial-timeout=2s				dial timeout for client connections
      --endpoints=[127.0.0.1:2379]		gRPC endpoints
      --hex[=false]				print byte strings as hex encoded strings
      --insecure-skip-tls-verify[=false]	skip server certificate verification
      --insecure-transport[=true]		disable transport security for client connections
      --key=""					identify secure client using this TLS key file
      --user=""					username[:password] for authentication (prompt if password is not supplied)
  -w, --write-out="simple"			set the output format (fields, json, protobuf, simple, table)
```

**恢复步骤**

```sh
# 恢复  --data-dir默认空必须指定
ETCDCTL_API=3 etcdctl --endpoints="https://127.0.0.1:2379"  --cert="/etc/kubernetes/pki/etcd/server.crt"  --key="/etc/kubernetes/pki/etcd/server.key"  --cacert="/etc/kubernetes/pki/etcd/ca.crt"   snapshot restore backup_etcd/snap-202206100917.db --skip-hash-check=true --data-dir=/var/lib/etcd


# 还原
mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests

# 查看 其他没有重启的组件calico-* coredns* kube-proxy 最好重启一下
kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-56fcbf9d6b-5qf99   1/1     Running   0          10m
calico-node-57dwd                          1/1     Running   0          10m
coredns-64897985d-fmp86                    1/1     Running   0          4m24s
coredns-64897985d-x7zd7                    1/1     Running   0          4m
etcd-master                                1/1     Running   0          80d
kube-apiserver-master                      1/1     Running   0          80d
kube-controller-manager-master             1/1     Running   0          80d
kube-proxy-flf9j                           1/1     Running   0          3m33s
kube-scheduler-master                      1/1     Running   0          80d
```

