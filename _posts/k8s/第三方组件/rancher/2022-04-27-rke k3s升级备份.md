---
layout:     post
rewards: false
title:   rancher k3s 升级 备份恢复
categories:
    - k8s
tags:
    - rancher
---

# rke

## 安装命令

cluster.yml 是配置文件

```
rke up -config cluster.yml
```

https://rancher.com/docs/rke/latest/en/example-yamls/ example



## 升级条件

RKE v0.2.0 及以上的版本使用`cluster.rkestate`文件管理集群状态。`cluster.rkestate`文件中含有集群的当前状态，包括 RKE 配置和证书等信息。

这个文件和`cluster.yml`位于同一目录下。

`cluster.rkestate`文件非常重要，控制集群和升级集群的时候都需要用到这个文件，请妥善保管该文件。



### 升级流程

RKE v1.1.0 及以上的版本提供了以下新功能：

- 支持不宕机升级，编辑或升级集群时，不会影响集群内的应用。
- 支持手动指定单个节点，并单独升级这个节点。
- 支持使用包含老版本的 Kubernetes 集群快照，将集群使用的 Kubernetes 恢复到先前的版本。这个功能使升级集群的过程变得更加安全。如果某次升级的过程中，集群中有些节点无法完成更新，导致该节点内的 pods 和应用无法访问，您可以将已经完成升级的集群所使用的 Kubernetes 降级为升级前使用的版本。

使用默认配置选项，输入`rke up`命令更新集群时，会依次触发以下事项：

- 逐个更新每个节点的 etcd plane。RKE 集群内的任何一个 etcd 节点在升级的过程中失效，会导致集群升级失败。
- 逐个更新 controlplane 节点，包括 controlplane 组件和 controlplane 节点的 worker plane 组件。
- 逐个更新每个 etcd 节点的 worker plane 组件。
- 批量更新 worker 节点，您可以配置每批更新的节点数量。在批量更新 worker 节点的过程中，可能会有部分节点不可用，为了降低这部分节点对于升级的影响，您可以配置最大不可用节点数量。默认的最大不可用节点数量是总节点数量的 10%，最小值为 1，如果是小数则向下取整至最近的一个整数，详情请参考下文。
- 更新[每个插件](https://docs.rancher.cn/docs/rke/config-options/add-ons/_index)。



## 升级k8s

### k8s

输入以下命令，快速获取支持K8s版本号

```sh
rke config --list-version --all
v1.15.3-rancher2-1
v1.13.10-rancher1-2
v1.14.6-rancher2-1
v1.16.0-beta.1-rancher1-1
```

指定k8s版本安装，修改配置文件

```yaml
kubernetes_version: v1.10.3-rancher2
```

[不同的版本需要到的镜像版本](https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go)

**升级 Kubernetes 版本**，打开`cluster.yml`文件，找到 `kubernetes_version`字符串，将原有的版本号修改为新的版本号即可。重新执行安装命令。

如果在`kubernetes_version`和`system_images`中都定义了 Kubernetes 版本，`system_images`中定义的版本会生效，而`kubernetes_version`中定义的版本不会生效。如果两者都没有定义 Kubernetes 版本，RKE 会使用默认的 Kubernetes 版本。

### docker

k8s支持的**docker版本**

https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG

```yaml
# If set to true, RKE will not fail when unsupported Docker version
# are found，默认是false
ignore_docker_version: false
```



## 升级服务

是指`cluster.yml`文件里面的service

可以修改服务的对象，或添加`extra_args`，然后运行`rke up`命令，升级服务。

> **说明：** `service_cluster_ip_range` 和 `cluster_cidr`不可修改。

## 升级system_images

system_images更新替换对应的镜像组件，但要注意组件兼容。**一般不随便替换**

## 添加/删除节点

https://rancher.com/docs/rke/latest/en/managing-clusters/

```sh
# Adding/Removing Nodes
rke up -config cluster.yml

# Adding/Removing Worker Nodes
rke up --update-only -config cluster.yml

# Removing Kubernetes Components from Nodes
rke remove
```

The cluster’s etcd snapshots are removed, including both local snapshots and snapshots that are stored on S3.

**Pods are not removed from the nodes. If the node is re-used, the pods will automatically be removed when the new Kubernetes cluster is created.**

- Clean each host from the directories left by the services:
  - /etc/kubernetes/ssl
  - /var/lib/etcd
  - /etc/cni
  - /opt/cni
  - /var/run/calico

## 创建快照

打开命令行工具，输入`rke etcd snapshot-save`命令，运行后即可保存 cluster config 文件内每个 etcd 节点的快照。RKE 会将节点快照保存在`/opt/rke/etcd-snapshots`路径下。运行上述命令时，RKE 会创建一个用于备份快照的容器。完成备份后，RKE 会删除该容器。

https://docs.rancher.cn/docs/rke/etcd-snapshots/one-time-snapshots/_index

```sh
# 运行以下命令，在本地创建一个一次性快照
rke etcd snapshot-save --config cluster.yml --name snapshot-name
```

## 恢复快照

https://docs.rancher.cn/docs/rke/etcd-snapshots/restoring-from-backup/_index

您的 Kubernetes 集群发生了灾难，您可以使用`rke etcd snapshot-restore`来恢复您的 etcd。这个命令可以将 etcd 恢复到特定的快照，应该**在遭受灾难的特定集群的 etcd 节点上运行**。

当您运行该命令时，将执行以下操作。

- 同步快照或从 S3 下载快照(如有必要)。
- 跨 etcd 节点检查快照校验和，确保它们是相同的。
- 通过运行`rke remove`删除您当前的集群并清理旧数据。这将删除整个 Kubernetes 集群，而不仅仅是 etcd 集群。
- 从选择的快照重建 etcd 集群。
- 通过运行`rke up`创建一个新的集群。
- 重新启动集群系统 pod。

> **警告：**在运行`rke etcd snapshot-restore`之前，您应该备份集群中的任何重要数据，因为该命令会删除您当前的 Kubernetes 集群，并用新的集群替换。

```sh
rke etcd snapshot-restore --config cluster.yml --name mysnapshot
```

## 轮换证书

- etcd
- kubelet (node certificate)
- kubelet（服务证书，如果[启用](https://docs.rancher.cn/docs/rke/config-options/services/_index#kubelet-选项))
- kube-apiserver
- kube-proxy
- kube-scheduler
- kube-controller-manager

**/etc/kubernetes/ssl**，证书目录

```sh
rke cert rotate --rotate-ca
```

因为证书改变，相应的`token`也会变化，所以在完成集群证书更新后，需要对连接`API SERVER`的 Pod 进行重建，以获取新的`token`。

例如：

搭建集群，添加网络后，coredns异常

```
kubectl get pods -n kube-system
NAME                                 READY   STATUS    RESTARTS   AGE
coredns-5dc887ffbb-2s8jd             0/1     Running   0          16h
```

coredns日志如下

```sh
085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Namespace: Unauthorized
pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Endpoints: Unauthorized
pkg/mod/k8s.io/client-go@v0.0.0-20190620085101-78d2af792bab/tools/cache/reflector.go:98: Failed to list *v1.Service: Unauthorized
```

查看api server日志

```sh
Unable to authenticate the request due to an error: [invalid bearer token, square/go-jose: error in cryptographic primitive

```

处理方式：

删除coredns-token和pods后恢复

```
kubectl delete secret  -n kube-system coredns-token-6n887
kubectl delete pods coredns-5dc887ffbb-zgff5
```

原理：

一般发生在你自定义ca，或者升级集群的时候。secret保留的 仍然是依据旧的ca 生成的token，这时需要手动删除token 和pods，让集群根据新的ca重新生成新的token,新的pod 也能使用新的token去访问。



# k3s

## 升级

为了在升级期间实现高可用性，K3s 容器在 K3s 服务停止时继续运行。

要停止所有 K3s 容器并重置 containerd 状态，`k3s-killall.sh`可以使用该脚本。

**killall 脚本清理容器、K3s 目录和网络组件，同时还删除 iptables 链以及所有相关规则。集群数据不会被删除。**

要从服务器节点运行 killall 脚本，请运行

```sh
/usr/local/bin/k3s-killall.sh
```

- 从[发布](https://github.com/rancher/k3s/releases)中下载所需版本的 K3s 二进制文件

-  将下载的二进制文件复制到`/usr/local/bin/k3s`（或您想要的位置）
-  重启k3s服务和代理

```sh
systemctl restart k3s
systemctl restart k3s-agent


service k3s restart
service k3s-agent restart
```

## 创建快照

默认情况下启用快照。

快照目录默认为`${data-dir}/server/db/snapshots`. data-dir 值默认为`/var/lib/rancher/k3s`并且可以通过设置`--data-dir`标志来更改。

要配置快照间隔或保留快照的数量，请参阅[选项。](https://rancher.com/docs/k3s/latest/en/backup-restore/#options)

## 从快照恢复集群

当 K3s 从备份中恢复时，旧的数据目录将被移动到`${data-dir}/server/db/etcd-old/`. 然后 K3s 将尝试通过创建一个新的数据目录来恢复快照，然后使用具有一个 etcd 成员的新 K3s 集群启动 etcd。

要从备份中恢复集群，请使用以下`--cluster-reset`选项运行 K3s，`--cluster-reset-restore-path`同时给出：

```
./k3s server \
  --cluster-reset \
  --cluster-reset-restore-path=<PATH-TO-SNAPSHOT>
```

**结果：** 日志中的一条消息说 K3s 可以在没有标志的情况下重新启动。再次启动 k3s，应该会成功运行并从指定的快照恢复。

### 选项

这些选项可以通过命令行传入，也可以在[配置文件中传入，](https://rancher.com/docs/k3s/latest/en/installation/install-options/#configuration-file)这样可能更容易使用。

| 选项                                | 描述                                                         |
| :---------------------------------- | :----------------------------------------------------------- |
| `--etcd-disable-snapshots`          | 禁用自动 etcd 快照                                           |
| `--etcd-snapshot-schedule-cron`价值 | cron 规范中的快照间隔时间。例如。每 5 小时`0 */5 * * *`(默认: `0 */12 * * *`) |
| `--etcd-snapshot-retention`价值     | 要保留的快照数量（默认值：5）                                |
| `--etcd-snapshot-dir`价值           | 保存数据库快照的目录。（默认位置：`${data-dir}/db/snapshots`） |
| `--cluster-reset`                   | 忘记所有对等点，成为新集群的唯一成员。这也可以使用环境变量来设置`[$K3S_CLUSTER_RESET]`。 |
| `--cluster-reset-restore-path`价值  | 要恢复的快照文件的路径                                       |