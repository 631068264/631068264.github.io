---
layout:     post
rewards: false
title:   安装rancher2.5.6 docker 单机版
categories:
    - k8s
tags:
  - rancher
---



# 安装rancher2.6.5 docker 单节点版

## 准备镜像仓库

必须使用https，而且ssl证书必须有SAN（**Subject Alternative Names**），不然会报错[x509: certificate relies on legacy Common Name field](https://jfrog.com/knowledge-base/general-what-should-i-do-if-i-get-an-x509-certificate-relies-on-legacy-common-name-field-error/)，和go version > 1.15有关，高版本弃用CN(CommonName) 字段

需要创建一个新的有效证书以包含**subjectAltName**属性，并且应该在使用 openssl 命令创建 SSL 自签名证书时通过指定*-addext*标志直接添加

```sh
openssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 -keyout harbor.key -out harbor.crt -subj "/CN=harbor.xxx.cn" -addext "subjectAltName = DNS:harbor.xxx.cn"
```



需要到的镜像

```
rancher/shell:v0.1.16
rancher/rancher-webhook:v0.2.5
rancher/fleet:v0.3.9
rancher/gitjob:v0.1.26
rancher/fleet-agent:v0.3.9
rancher/rke-tools:v0.1.80
rancher/hyperkube:v1.23.6-rancher1
rancher/mirrored-coreos-etcd:v3.5.3
rancher/mirrored-pause:3.6
rancher/mirrored-calico-cni:v3.22.0
rancher/mirrored-calico-pod2daemon-flexvol:v3.22.0
rancher/kube-api-auth:v0.1.8
rancher/mirrored-calico-node:v3.22.0
rancher/mirrored-flannelcni-flannel:v0.17.0
rancher/mirrored-cluster-proportional-autoscaler:1.8.5
rancher/mirrored-metrics-server:v0.6.1
rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.1.1
rancher/mirrored-coredns-coredns:1.9.0
rancher/mirrored-calico-kube-controllers:v3.22.0
rancher/nginx-ingress-controller:nginx-1.2.0-rancher1
rancher/rancher-agent:v2.6.5
```





## docker 命令

**生成的证书，*key, *crt都放到/root/harbor/cert下面**，然后映射到容器的**/container/certs**目录

```sh
docker run -d --name rancher2.6.5 --restart=unless-stopped -e CATTLE_SYSTEM_CATALOG=bundled -e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs -p 3280:80 -p 3443:443 --privileged xxx.xxx.xxx.203:8080/rancher/rancher:v2.6.5
```

[主要命令](https://rancher.com/docs/rancher/v2.6/en/installation/other-installation-methods/single-node-docker/)

```sh
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  rancher/rancher:latest
```

### 参数详解

[加载system-charts,其实默认已经在rancher镜像里面，这个变量告诉 Rancher 使用本地的，而不是尝试从 GitHub 获取它们。](https://rancher.com/docs/rancher/v2.6/en/installation/resources/local-system-charts/)

```sh
-e CATTLE_SYSTEM_CATALOG=bundled
```

[Custom CA Root Certificates，参考这里面的 docker配置](https://rancher.com/docs/rancher/v2.6/en/installation/resources/custom-ca-root-certificate/)，这里配置Rancher 需要访问的服务需要用的自签名证书，不然会报错**x509: certificate signed by unknown authority**

```sh
-e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs
```



## 配置私有仓库

根据这个[Private Registry Configuration](https://rancher.com/docs/k3s/latest/en/installation/private-registry/), 进到容器里面配置

```
docker exec -it rancher2.6.5 bash

vim /etc/rancher/k3s/registries.yaml
```

registries.yaml

```yaml
mirrors:
  "docker.io":
    endpoint:
      - "https://harbor.xxx.cn:4443"
configs:
  "docker.io":
    auth:
      username: admin
      password: Harbor12345
    tls:
      key_file: /container/certs/harbor.key
      cert_file: /container/certs/harbor.crt
      #ca_file: /container/certs/ca.crt
      insecure_skip_verify: true
```

重启容器

```
docker restart rancher2.6.5
```

**重新进入容器，然后配置hosts，不然使用域名解析不了，不是配置coredns**

```
echo "xxx.xxx.xxx.205 harbor.xxx.cn" >> /etc/hosts
```

重启后，要等containd启动，检查containd更新的配置

```
cat /var/lib/rancher/k3s/agent/etc/containerd/config.toml
```

测试拉镜像

```
crictl pull rancher/shell:v0.1.16
```



## 配置system-default-registry

![image-20220519111824934](https://tva1.sinaimg.cn/large/e6c9d24ely1h3q1eujsqkj22630u0n0o.jpg)



# 高可用rancher

## 安装helm

https://helm.sh/docs/intro/install/

## 安装cent-manager

cert-manager镜像，**docker load 到每个节点**，不然要改cert-manager helm chart

```
quay.io/jetstack/cert-manager-cainjector:v1.7.1
quay.io/jetstack/cert-manager-controller:v1.7.1
quay.io/jetstack/cert-manager-ctl:v1.7.1
quay.io/jetstack/cert-manager-webhook:v1.7.1
```

下载  [参考](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#4-install-cert-manager)

```sh
# If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart:

# 下载crd
https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

helm fetch jetstack/cert-manager --version 1.7.1
```

安装

```sh
kubectl apply -f cert-manager.crds.yaml

helm install cert-manager cert-manager-v1.7.1.tgz \
  --namespace cert-manager \
  --create-namespace \
  --version v1.7.1
```



验证

```sh
kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
```





## 安装rancher

下载

```sh
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm repo update

helm fetch rancher-stable/rancher --version 2.6.5
```

安装，[选项详解](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#5-install-rancher-with-helm-and-your-chosen-certificate-option)

```sh
kubectl create  namespace cattle-system

helm install rancher rancher-2.6.5.tgz \
  --namespace cattle-system \
  --set hostname=205.xxx.cn \
  --set bootstrapPassword=fafafa \
  --set replicas=1 \
  --set useBundledSystemChart=true \
  --set additionalTrustedCAs=true
```

- hostname rancher 域名

- bootstrapPassword：登录密码

- replicas： rancher副本数

- useBundledSystemChart： 是否使用system-charts packaged with Rancher server

- additionalTrustedCAs：信任第三方证书（harbor）配合使用

  ```sh
  kubectl -n cattle-system create secret generic tls-ca-additional --from-file=ca-additional.pem
  ```

  **ca-additional.pem是harbor的自签名cert证书**，要重命名为ca-additional.pem

### 安装使用自签名SAN证书

必须使用CA签名，不然纳管agent报错

```
Certificate chain is not complete, please check if all needed intermediate certificates are included in the server certificate (in the correct order) and if the cacerts setting in Rancher either contains the correct CA certificate (in the case of using self signed certificates) or is empty (in the case of using a certificate signed by a recognized CA). Certificate information is displayed above. error: Get \"https://ums.xxx.cn:31393\": x509: certificate signed by unknown authority
```

准备openssl.conf  证书生成参考https://www.golinuxcloud.com/openssl-subject-alternative-name/

```
[req]
distinguished_name = req_distinguished_name
req_extensions = req_ext
prompt = no

[req_distinguished_name]
CN = ums.xxx.cn

[req_ext]
subjectAltName = @alt_names

[alt_names]
IP.1 = 192.168.25.99
DNS.1 = ums.xxx.cn
DNS.2 = *.xxx.cn
```

**使用自签名证书不用certmanager**，要保证**hostname CN subjectAltName** 一致

```sh
# 生成ca
openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 36500 -out ca.crt

# 给生成的证书CA签名
openssl genrsa -out tls.key 2048
openssl req -new -key tls.key -out tls.csr -config openssl.conf
openssl x509 -req -days 36500 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt -extensions req_ext -extfile openssl.conf

# 验证SAN
openssl x509 -text -noout -in tls.crt | grep -A 1 "Subject Alternative Name"
```







```sh
kubectl -n cattle-system create secret tls tls-rancher-ingress \
  --cert=tls.crt \
  --key=tls.key
  
# Error from server (AlreadyExists): secrets "tls-rancher-ingress" already exists 报错的话先删除再重新创建
kubectl -n cattle-system delete secret tls-rancher-ingress


mv ca.crt cacerts.pem
kubectl -n cattle-system create secret generic tls-ca \
  --from-file=cacerts.pem=./cacerts.pem
```

自签名证书安装  Private CA signed certificate , add `--set privateCA=true` to the command:

```sh
helm install rancher rancher-2.6.5.tgz \
  --namespace cattle-system \
  --set hostname=205.xxx.cn \
  --set bootstrapPassword=fafafa \
  --set replicas=1 \
  --set useBundledSystemChart=true \
  --set additionalTrustedCAs=true \
  --set ingress.tls.source=secret \
  --set privateCA=true
```

验证

```sh
kubectl -n cattle-system get pod

NAME                              READY   STATUS    RESTARTS   AGE
rancher-6f7df66cf7-2fnw5          1/1     Running   0          3d23h
rancher-webhook-6994b4677-tpvf8   1/1     Running   0          3d23h
```

 

# 备份还原Rancher Backups (2.1.2)

[参考](https://rancher.com/docs/rancher/v2.6/en/backups/back-up-rancher/)

镜像

```
rancher/backup-restore-operator:v2.1.2
rancher/kubectl:v1.21.9
```

安装

![image-20220606161846013](https://tva1.sinaimg.cn/large/e6c9d24ely1h3q1ewekcqj227y0tajw0.jpg)

根据需要安装就行，可以保存到**pv或者s3**上

- **docker单节点有可能会遇到k8s版本太高，rancher backup 版本太低导致**

  ```sh
  no matches for kind "CustomResourceDefinition" in version "apiextensions.k8s.io/v1beata1"
  ```

  例如

  ![image-20220824101348866](https://tva1.sinaimg.cn/large/e6c9d24egy1h5hmx8lr9wj21uk0qqthc.jpg)

  这时候要直接`docker cp`适合版本的安装文件到容器里面，使用`helm_v3`安装。

  

验证

```sh
kubectl -n cattle-resources-system get pod

NAME                              READY   STATUS    RESTARTS   AGE
rancher-backup-7f9ff4c6cb-68jd5   1/1     Running   0          5d
```

刷新后多出选项

![image-20220606162818445](https://tva1.sinaimg.cn/large/e6c9d24ely1h3q1evxpkvj21yb0u00xt.jpg)

关于minio配置，**必须用https**

- [minio自身用https](https://github.com/minio/minio/issues/6820#issuecomment-439247023)

  在docker目录`/root/.minio`对应的挂载卷目录，创建`certs`目录，对应的密钥和证书重命名成 `private.key` and `public.crt`，**使用SAN完整证书链**，加入**docker container ip**(可以通过 docker log 查看到)，**server ip**

  ```sh
  docker run -d -p 9000:9000 -p 5000:5000 --name minio -e "MINIO_ROOT_USER=admin" -e "MINIO_ROOT_PASSWORD=12345678"  -v /mnt/data:/data   -v /mnt/config:/root/.minio minio/minio server --console-address ":5000" /data
  ```

  开启成功的话

  ```sh
  
  docker logs -f minio
  
  WARNING: Detected Linux kernel version older than 4.0.0 release, there are some known potential performance problems with this kernel version. MinIO recommends a minimum of 4.x.x linux kernel version for best performance
  API: https://172.17.0.3:9000  https://127.0.0.1:9000
  
  Console: https://172.17.0.3:5000 https://127.0.0.1:5000
  
  Documentation: https://docs.min.io
  Finished loading IAM sub-system (took 0.0s of 0.0s to load data).
  ```

- 关于账密

  创建secret

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: creds
  type: Opaque
  data:
    accessKey: <Enter your base64-encoded access key>
    secretKey: <Enter your base64-encoded secret key>
  ```

- 配置填写

  endpoint ca  必须填写**使用证书base64后的内容**，不要直接用证书明文。

  ![image-20220606163428904](https://tva1.sinaimg.cn/large/e6c9d24ely1h3q1evhd72j21uu0qq78q.jpg)

# 迁移rancher

假设集群A迁移到B，**A要先备份**，尽量[AB集群所用Kubernetes 版本相同，因为不同的版本apiVersion不一样](https://rancher.com/docs/rancher/v2.6/en/backups/migrating-rancher/#2-restore-from-backup-using-a-restore-custom-resource)

## 先备份

无论是docker或者集群安装的rancher，**备份必须使用rancher backup operator，而不是参考[docker backup](https://rancher.com/docs/rancher/v2.5/en/backups/docker-installs/docker-backups/)**，否则使用operator恢复备份文件时候会报错([参考Rancher backup panics when it encounters an invalid tarball](https://github.com/rancher/rancher/issues/31801))

```sh
goroutine 403 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic(0x1691bc0, 0xc0005a10c0)
        /go/pkg/mod/k8s.io/apimachinery@v0.18.0/pkg/util/runtime/runtime.go:74 +0xa3
k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
        /go/pkg/mod/k8s.io/apimachinery@v0.18.0/pkg/util/runtime/runtime.go:48 +0x82
panic(0x1691bc0, 0xc0005a10c0)
        /usr/local/go/src/runtime/panic.go:679 +0x1b2
github.com/rancher/backup-restore-operator/pkg/controllers/restore.getGVR(0xc0000aa000, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
        /go/src/github.com/rancher/backup-restore-operator/pkg/controllers/restore/controller.go:677 +0x2d4
github.com/rancher/backup-restore-operator/pkg/controllers/restore.(*handler).loadDataFromFile(0xc0000892c0, 0xc0003a3180, 0xc000248a00, 0x13d, 0x200, 0xc0003b8630, 0xc000291380, 0x0, 0xc0004ac120)
        /go/src/github.com/rancher/backup-restore-operator/pkg/controllers/restore/download.go:109 +0x17d
github.com/rancher/backup-restore-operator/pkg/controllers/restore.(*handler).LoadFromTarGzip(0xc0000892c0, 0xc0004ac120, 0x2f, 0xc0003b8630, 0xc000291380, 0x0, 0x0)
```

统一参考[备份还原Rancher Backups (2.1.2)](#备份还原Rancher Backups (2.1.2))

## minio可能会遇到的问题

检查minio和集群时区使用`date`,不然minio下载备份数据报错`The difference between the request time and the server's time is too large., requeuing`



```sh
# 使用timedatectl比较时区和rtc
root@d-ecs-38357230:~/af# timedatectl
                      Local time: Tue 2022-06-07 17:17:30 CST
                  Universal time: Tue 2022-06-07 09:17:30 UTC
                        RTC time: Tue 2022-06-07 16:58:33
                       Time zone: Asia/Shanghai (CST, +0800)
       System clock synchronized: no
systemd-timesyncd.service active: yes
                 RTC in local TZ: yes


# 可能会用到的命令 调整时区和
timedatectl set-timezone Asia/Shanghai
timedatectl set-local-rtc 1

# 修改时间 可以使用watch -n 1 date查看minio服务器实时时间
date -s "2022-06-07 17:15:40"
```

## 迁移流程

下载rancher-backup相关的包

```sh
helm repo add rancher-charts https://charts.rancher.io
helm repo update

helm fetch rancher-charts/rancher-backup-crd --version 2.1.2
helm fetch rancher-charts/rancher-backup --version 2.1.2
```



在B安装，[安装前可以使用这个脚本清理节点](https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/)

```sh
helm install rancher-backup-crd rancher-backup-crd-2.1.2.tgz -n cattle-resources-system --create-namespace

# docker pull 版本对应的镜像包，然后指定
helm install rancher-backup rancher-backup-2.1.2.tgz -n cattle-resources-system  -f values.yaml
```

values.yaml

```yaml
image:
  repository: harbor.xxx.cn:4443/rancher/backup-restore-operator
  tag: v2.1.2
global:
  kubectl:
    repository: harbor.xxx.cn:4443/rancher/kubectl
    tag: v1.21.9
```



验证

```sh
kubectl get pod -n cattle-resources-system
NAME                             READY   STATUS    RESTARTS   AGE
rancher-backup-94944dc7b-b87z9   1/1     Running   0          171m
```

在B创建和A相同的**minio secert**

```yaml
apiVersion: v1
data:
  accessKey: YWRtaW4=
  secretKey: MTIzNDU2Nzg=
kind: Secret
metadata: 
  name: s3minio
  namespace: default
type: Opaque
```

创建restore自定义资源，在恢复自定义资源中，`prune`必须设置为 false。**endpointCA 证书内容得base64编码**

```yaml
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-migration
spec:
  backupFilename: minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz
  prune: false
  storageLocation:
    s3:
      bucketName: rancher-backup
      credentialSecretName: s3minio
      credentialSecretNamespace: default
      endpoint: xxx.xxx.xxx.205:9000
      endpointCA: xxxx
      insecureTLSSkipVerify: true
```



验证

```sh
vim restore-apply.yaml
kubectl apply -f restore-apply.yaml
restore.resources.cattle.io/restore-migration created

# 看restore状态
kubectl get restore
NAME                BACKUP-SOURCE   BACKUP-FILE                                                              AGE   STATUS
restore-migration                   minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz   8s


# 看log
kubectl get pods -n cattle-resources-system
NAME                             READY   STATUS    RESTARTS   AGE
rancher-backup-94944dc7b-b87z9   1/1     Running   0          3h5m

kubectl logs -n cattle-resources-system --tail 100 -f rancher-backup-94944dc7b-b87z9



# 成功后可以开始安装rancher
kubectl get restore
NAME                BACKUP-SOURCE   BACKUP-FILE                                                              AGE     STATUS
restore-migration   S3              minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz   4m52s   Completed
```

参考上面安装[高可用rancher2.6.5](#高可用rancher)，**rancher安装命令hostname和A安装的保持一致**

## 迁移完成后的配置修改

检查server-url是否符合实际情况，和高可用安装时候的hostname一致

![image-20220819102625183](https://tva1.sinaimg.cn/large/e6c9d24egy1h5bv6r5qr0j21pw0r4acp.jpg)

针对从docker安装迁移到集群高可用安装的情况，主要是agent的连接从IP变成了域名，所以还需要配置好下游集群的agent

![image](https://tva1.sinaimg.cn/large/e6c9d24egy1h5bvkfe09yj20j604rt8x.jpg)

- 获取kubeconfig，[参考](https://forums.rancher.cn/t/rancher-v2-6-rancher-server-ip/729#kubeconfig-4)

  对于导入集群不需要考虑，创建的集群分两种情况

  - 有从UI界面获取过kubeconfig，可以通过切换context

    ```sh
    kubectl config get-contexts
    kubectl config use-context [context-name]
    ```

    Example:

    ```sh
    CURRENT   NAME                        CLUSTER                     AUTHINFO     NAMESPACE
    *         my-cluster                  my-cluster                  user-46tmn
              my-cluster-controlplane-1   my-cluster-controlplane-1   user-46tmn
    ```

    在本例中，当您使用`kubectl`第一个上下文时`my-cluster`，您将通过 Rancher 服务器进行身份验证。

    使用第二个上下文，`my-cluster-controlplane-1`您将使用授权的集群端点进行身份验证，直接与下游 RKE 集群通信。

  - 已经对开连接了，就下载不了，可以从下游集群具有`controlplane`的节点上生成kubeconfig。

    参考https://gist.github.com/superseb/b14ed3b5535f621ad3d2aa6a4cd6443b

    ```sh
    docker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination "/etc/kubernetes" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=io.cattle.agent=true) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\"full-cluster-state\" | jq -r .currentState.certificatesBundle.\"kube-admin\".config | sed -e "/^[[:space:]]*server:/ s_:.*_: \"https://127.0.0.1:6443\"_"' > kubeconfig_admin.yaml
    
    ```

    需要安装jq，比较麻烦。可以把方法拆分

    ```sh
    docker exec -it kube-apiserver bash
    export KUBECONFIG=/etc/kubernetes/ssl/kubecfg-kube-node.yaml
    
    kubectl get configmap -n kube-system full-cluster-state -o json > full-cluster-state.json
    
    ```

    拿到json后，在执行

    ```sh
     cat full-cluster-state.json|jq -r .data.\"full-cluster-state\" | jq -r .currentState.certificatesBundle.\"kube-admin\".config | sed -e "/^[[:space:]]*server:/ s_:.*_: \"https://127.0.0.1:6443\"_" > kubeconfig_admin.yaml
    
    ```

- 修改coredns

  ```sh
  kubectl edit cm coredns -n kube-system
  ```

  修改hosts

  ```yaml
  data:
    Corefile: |
      .:53 {
          errors
          health {
            lameduck 5s
          }
          hosts {
            xxxx 201.xxxx.cn
            fallthrough
          }
          ready
          kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
          }
          prometheus :9153
          forward . "/etc/resolv.conf"
          cache 30
          loop
          reload
          loadbalance
      } # STUBDOMAINS - Rancher specific change
  kind: ConfigMap
  ```

  delete 对应的coredns pod，让hosts起效

- 修改agent配置

  ```sh
  kubectl edit deploy cattle-cluster-agent -n cattle-system
  ```

  修改CATTLE_SERVER，改成新的域名，同时找到挂载的secret

  ```yaml
        containers:
        - env:
         ...
          - name: CATTLE_IS_RKE
            value: "true"
          - name: CATTLE_SERVER
            value: https://xxx.xxx.cn  #修改
          .....
          image: rancher/rancher-agent:v2.6.5
          
          .....
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-cdcb52a  #复制
  ```

  执行

  ```sh
  kubectl edit secret cattle-credentials-cdcb52a -n cattle-system
  ```

  修改url，填入CATTLE_SERVER值的base64编码

  ```yaml
  apiVersion: v1
  data:
    namespace: xxxx
    token: xxx
    url: aHR0cHM6Ly8yMDEudWlpbi5jbg==  # 修改
  kind: Secret
  .....
  type: Opaque
  ```

  最后delete cattle-cluster-agent对应的pod





# 升级rancher2.5.5-2.6.5

## 单机版

[参考](https://docs.rancher.cn/docs/rancher2.5/installation/other-installation-methods/single-node-docker/single-node-upgrades/_index)

主要使用`--volumes-from`实现容器间数据共享

```sh
# 停止旧版本容器
docker stop <OLD_RANCHER_CONTAINER_NAME>

# 备份
docker create --volumes-from <OLD_RANCHER_CONTAINER_NAME> --name rancher-data rancher/rancher:<OLD_RANCHER_CONTAINER_TAG>

# 在当前位置生成rancher数据压缩包
docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher

# 启动新rancher server
docker run -d --privileged --volumes-from rancher-data \
  --restart=unless-stopped  --name rancher2.6.5 \
  -e SSL_CERT_DIR="/container/certs" -v /root/harbor:/container/certs \
  -e CATTLE_SYSTEM_CATALOG=bundled \
  -p 1080:80 -p 1443:443 \
    rancher/rancher:v2.6.5

# 如果自定义证书
docker run -d --privileged --restart=unless-stopped \
    -p 80:80 -p 443:443 \
    -v /<CERT_DIRECTORY>/<FULL_CHAIN.pem>:/etc/rancher/ssl/cert.pem \
    -v /<CERT_DIRECTORY>/<PRIVATE_KEY.pem>:/etc/rancher/ssl/key.pem \
    -v /<CERT_DIRECTORY>/<CA_CERTS.pem>:/etc/rancher/ssl/cacerts.pem \
    -e CATTLE_SYSTEM_DEFAULT_REGISTRY=<REGISTRY.YOURDOMAIN.COM:PORT> \ # Set a default private registry to be used in Rancher
    -e CATTLE_SYSTEM_CATALOG=bundled \ #Available as of v2.3.0，use the packaged Rancher system charts
    <REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher:<RANCHER_VERSION_TAG>
    
    
# 删除旧容器
docker rm -f <OLD_RANCHER_CONTAINER_NAME>
```

回滚

```sh
# 解压数据到rancher-data

docker run  --volumes-from rancher-data \
-v $PWD:/backup busybox sh -c "rm /var/lib/rancher/* -rf \
&& tar zxvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz"

# 运行旧版本rancher
docker run -d --volumes-from rancher-data \
 --restart=unless-stopped \
 -p 80:80 -p 443:443 \
 --privileged \
 rancher/rancher:<PRIOR_RANCHER_VERSION>
```



## 高可用

升级rancher，**rancher依赖cert-manager的crd**，所以最好先找到升级版本的rancher对应的cert-manager，**cert-manager也要升级**

**例如rancher 2.6.5依赖cert-manager 1.7，[参考Install/Upgrade Rancher on a Kubernetes Cluster](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/)**



### 备份

备份可以参考上面，或者看https://docs.rancher.cn/docs/rancher2.5/backups/back-up-rancher/_index/



```sh
# 获取旧版本的values
helm get values rancher -n cattle-system -o yaml > values.yaml

# 卸载 Rancher
helm delete rancher -n cattle-system
```





### 升级cert-manager 

[参考](https://docs.rancher.cn/docs/rancher2.5/installation/resources/upgrading-cert-manager/_index/)



[备份现有资源](https://cert-manager.io/docs/tutorials/backup/)

```sh
kubectl get -o yaml --all-namespaces \
issuer,clusterissuer,certificates,certificaterequests > cert-manager-backup.yaml
```



```sh
# 卸载现有部署
helm uninstall cert-manager -n cert-manager
kubectl delete namespace cert-manager

# 删除旧crd
kubectl delete -f old-version-crd.yaml

# 安装新版cert-manager
kubectl apply -f new-verson-crds.yaml

helm install cert-manager cert-manager-v1.7.1.tgz \
  --namespace cert-manager \
  --create-namespace \
  --version v1.7.1
```

[恢复备份资源](https://cert-manager.io/docs/tutorials/backup/#restoring-resources)

```sh
kubectl apply -f cert-manager-backup.yaml


# 验证
kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m

```



### 升级rancher

```sh
# 安装新版rancher
helm install rancher rancher-2.6.5.tgz -n cattle-system -f values.yaml
```





# 自定义集群重新以导入方式纳管

[根据文档描述，直接UI界面删除自定义集群，会造成k8s的组件也会被删除，所以不能直接界面操作](https://docs.rancher.cn/docs/rancher2.5/cluster-admin/cleaning-cluster-nodes/_index/#%E5%88%A0%E9%99%A4%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F)

场景：A rancher的自定义下游集群想迁移到B上（AB可以一样）

- B先要创建导入集群，得到部署命令行，拿到对应的yaml

- `kubectl delete -f xxx.yaml`相当于删掉与A的连接，删除后kubectl会失效

  ```sh
  kubectl get pods -A
  error: You must be logged in to the server (Unauthorized)
  ```

- 重新获取kubeconfig [参考这个里面关于从kube-apiserver容器里面获取kubeconfig](#迁移完成后的配置修改)

- 使用新的kubeconfig，执行B的导入纳管命令

**重新使用rke管理集群**

 [参考这里获取kubeconfig，这里面有整个集群的信息](#迁移完成后的配置修改)

```sh
kubectl get configmap -n kube-system full-cluster-state -o json > cluster.rkestate
```

根据集群信息编写cluster.yml

```yaml
nodes:
  - address: xxxx
    user: root
    role: ["controlplane", "etcd", "worker"]
    ssh_key_path: /root/.ssh/id_rsa
    port: 22

kubernetes_version: v1.19.16-rancher1-5  # 参考cluster.rkestate的kubernetesVersion非常重要
cluster_name: xxxx  # 参考
```

尽量寻找支持该kubernetesVersion的[rke版本](https://github.com/rancher/rke/releases/tag/v1.3.13)，只要前面`v1.19.16-rancher1`一样就可以。

处理完这两个文件就可以，使用rke来操作了





# rancher error 处理

## template system-library-rancher-monitoring 和kubeversion 不匹配

template system-library-rancher-monitoring incompatible with rancher version or cluster's [xxx] kubernetes version

![image-20220713172222587](https://tva1.sinaimg.cn/large/e6c9d24egy1h57akaxgh3j20xy03aq3c.jpg)

参考 https://github.com/rancher/rancher/issues/37039



- [迁移相关](https://github.com/rancher/rancher/issues/37039#issuecomment-1170970740)，看是不是开启了v1版本的monitor，新版rancher用的是v2

  ```sh
  # 设置检查集群配置
  enable_cluster_alerting: false
  enable_cluster_monitoring: false
  ```

  或者使用[这个脚本](https://github.com/bashofmann/rancher-monitoring-v1-to-v2#check-why-monitoring-v1-is-not-disabled)检查，是否存在迁移，自定义证书要加`--insecure`参数

  得到

  ```
  The Monitoring V1 operator does not appear to exist in cluster *******. Migration to Monitoring V2 should be possible.
  ```

  应该没啥问题

  

- 检查`system-library-rancher-monitoring`这个 cr的内容

  ```
  kubectl edit catalogtemplates system-library-rancher-monitoring 
  ```

  修改versions的第一项

  ```yaml
  spec:
    catalogId: system-library
    defaultVersion: 0.3.2
    description: Provides monitoring for Kubernetes which is maintained by Rancher 2.
    displayName: rancher-monitoring
    folderName: rancher-monitoring
    icon: https://coreos.com/sites/default/files/inline-images/Overview-prometheus_0.png
    projectURL: https://github.com/coreos/prometheus-operator
    versions:
    - digest: 08fbaee28d5a0efb79db02d9372629e2
      externalId: catalog://?catalog=system-library&template=rancher-monitoring&version=0.3.2
      kubeVersion: < 1.22.0-0  # 这个地方 改成 '>=1.21.0-0'
      rancherMinVersion: 2.6.1-alpha1
      version: 0.3.2
      versionDir: charts/rancher-monitoring/v0.3.2
      versionName: rancher-monitoring
  ```






## could not find tenant ID context deadline exceeded

![image](https://tva1.sinaimg.cn/large/e6c9d24egy1h57akd79d1j222s0u0tc0.jpg)

创建微软云凭证时候报错，**凭证有效能用**，POST **/meta/aksCheckCredentials**时候，报错。

```
{"error":"could not find tenant ID: Request failed: subscriptions.Client#Get: Failure sending request: StatusCode=0 -- Original Error: context deadline exceeded"}
```

通过错误提示，找到并阅读源码，`goCtx`用于控制请求超时时间，报错也和时间有关。

```go
func FindTenantID(ctx context.Context, env azure.Environment, subscriptionID string) (string, error) {
	goCtx, cancel := context.WithTimeout(ctx, findTenantIDTimeout)
	defer cancel()
....
}
```

![image-20220716094424410](https://tva1.sinaimg.cn/large/e6c9d24egy1h57akcbesnj20xy03aq3c.jpg)

函数作用

- 调用azure sdk访问azure http api，验证cred。

排查方向

- 网络方面真的超时，通过debug，大概摸清访问的url，参数等，尝试请求发现可以访问，响应也快。
- 通过debug怀疑到时区问题引发的问题

系统默认是UTC，通过把时区设置成当地时区，并且reboot机器问题解决，只是restart rancher pod不起效

```sh
root@xxx:~# timedatectl
                      Local time: Tue 2022-07-19 03:20:00 UTC
                  Universal time: Tue 2022-07-19 03:20:00 UTC
                        RTC time: Tue 2022-07-19 03:20:01
                       Time zone: UTC (UTC, +0000)
       System clock synchronized: yes
systemd-timesyncd.service active: yes
                 RTC in local TZ: no
                 
# 修改时区
sudo timedatectl set-timezone Asia/Shanghai
```
