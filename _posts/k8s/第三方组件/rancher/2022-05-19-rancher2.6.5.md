---
layout:     post
rewards: false
title:   安装rancher2.5.6 docker 单机版
categories:
    - k8s
tags:
  - rancher
---



# 安装rancher2.6.5 docker 单节点版

## 准备镜像仓库

必须使用https，而且ssl证书必须有SAN（**Subject Alternative Names**），不然会报错[x509: certificate relies on legacy Common Name field](https://jfrog.com/knowledge-base/general-what-should-i-do-if-i-get-an-x509-certificate-relies-on-legacy-common-name-field-error/)，和go version > 1.15有关，高版本弃用CN(CommonName) 字段

需要创建一个新的有效证书以包含**subjectAltName**属性，并且应该在使用 openssl 命令创建 SSL 自签名证书时通过指定*-addext*标志直接添加

```sh
openssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 -keyout harbor.key -out harbor.crt -subj "/CN=harbor.xxx.cn" -addext "subjectAltName = DNS:harbor.xxx.cn"
```



需要到的镜像

```
rancher/rancher:v2.6.5
rancher/shell:v0.1.16
rancher/rancher-webhook:v0.2.5
rancher/fleet:v0.3.9
rancher/gitjob:v0.1.26
rancher/fleet-agent:v0.3.9
rancher/rke-tools:v0.1.80
rancher/hyperkube:v1.23.6-rancher1
rancher/mirrored-coreos-etcd:v3.5.3
rancher/mirrored-pause:3.6
rancher/mirrored-calico-cni:v3.22.0
rancher/mirrored-calico-pod2daemon-flexvol:v3.22.0
rancher/kube-api-auth:v0.1.8
rancher/mirrored-calico-node:v3.22.0
rancher/mirrored-flannelcni-flannel:v0.17.0
rancher/mirrored-cluster-proportional-autoscaler:1.8.5
rancher/mirrored-metrics-server:v0.6.1
rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.1.1
rancher/mirrored-coredns-coredns:1.9.0
rancher/mirrored-calico-kube-controllers:v3.22.0
rancher/nginx-ingress-controller:nginx-1.2.0-rancher1
rancher/rancher-agent:v2.6.5
```



```sh
# 获取某个版本的镜像
rke config --system-images --all
```



## docker 命令

**生成的证书，*key, *crt都放到/root/harbor/cert下面**，然后映射到容器的**/container/certs**目录

```sh
docker run -d --name rancher2.6.5 --restart=unless-stopped -e CATTLE_SYSTEM_CATALOG=bundled -e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs -p 3280:80 -p 3443:443 --privileged  --add-host harbor.xx.cn:10.xx.xx.205 rancher/rancher:v2.6.5
```

[主要命令](https://rancher.com/docs/rancher/v2.6/en/installation/other-installation-methods/single-node-docker/)

```sh
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  rancher/rancher:latest
```

### 参数详解

[加载system-charts,其实默认已经在rancher镜像里面，这个变量告诉 Rancher 使用本地的，而不是尝试从 GitHub 获取它们。](https://rancher.com/docs/rancher/v2.6/en/installation/resources/local-system-charts/)

```sh
-e CATTLE_SYSTEM_CATALOG=bundled
```

[Custom CA Root Certificates，参考这里面的 docker配置](https://rancher.com/docs/rancher/v2.6/en/installation/resources/custom-ca-root-certificate/)，这里配置Rancher 需要访问的服务需要用的自签名证书，不然会报错**x509: certificate signed by unknown authority**

```sh
-e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs
```



## 配置私有仓库

根据这个[Private Registry Configuration](https://rancher.com/docs/k3s/latest/en/installation/private-registry/), 进到容器里面配置

```
docker exec -it rancher2.6.5 bash

vim /etc/rancher/k3s/registries.yaml
```

registries.yaml

```yaml
mirrors:
  "docker.io":
    endpoint:
      - "https://harbor.xxx.cn:4443"
configs:
  "docker.io":
    auth:
      username: admin
      password: Harbor12345
    tls:
      key_file: /container/certs/harbor.key
      cert_file: /container/certs/harbor.crt
      #ca_file: /container/certs/ca.crt
      insecure_skip_verify: true
```

重启容器

```
docker restart rancher2.6.5
```

**重新进入容器，然后配置hosts，不然使用域名解析不了，不是配置coredns**

```
echo "xxx.xxx.xxx.205 harbor.xxx.cn" >> c
```

重启后，要等containd启动，检查containd更新的配置

```
cat /var/lib/rancher/k3s/agent/etc/containerd/config.toml
```

测试拉镜像

```
crictl pull rancher/shell:v0.1.16
```



## 配置system-default-registry

![image-20220519111824934](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24ely1h3q1eujsqkj22630u0n0o.jpg)



# 高可用rancher

## 安装helm

https://helm.sh/docs/intro/install/

## 安装cent-manager

cert-manager镜像，**docker load 到每个节点**，不然要改cert-manager helm chart

```
quay.io/jetstack/cert-manager-cainjector:v1.7.1
quay.io/jetstack/cert-manager-controller:v1.7.1
quay.io/jetstack/cert-manager-ctl:v1.7.1
quay.io/jetstack/cert-manager-webhook:v1.7.1
```

下载  [参考](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#4-install-cert-manager)

```sh
# If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart:

# 下载crd
https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

helm fetch jetstack/cert-manager --version 1.7.1
```

安装

```sh
kubectl apply -f cert-manager.crds.yaml

helm install cert-manager cert-manager-v1.7.1.tgz \
  --namespace cert-manager \
  --create-namespace \
  --version v1.7.1
```



验证

```sh
kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
```





## 安装rancher

下载

```sh
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm repo update

helm fetch rancher-stable/rancher --version 2.6.5
```

安装，[选项详解](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#5-install-rancher-with-helm-and-your-chosen-certificate-option)

```sh
kubectl create  namespace cattle-system

helm install rancher rancher-2.6.5.tgz \
  --namespace cattle-system \
  --set hostname=205.xxx.cn \
  --set bootstrapPassword=fafafa \
  --set replicas=1 \
  --set useBundledSystemChart=true \
  --set additionalTrustedCAs=true
```

- hostname rancher 域名

- bootstrapPassword：登录密码

- replicas： rancher副本数

- useBundledSystemChart： 是否使用system-charts packaged with Rancher server

- additionalTrustedCAs：信任第三方证书（harbor）配合使用

  ```sh
  kubectl -n cattle-system create secret generic tls-ca-additional --from-file=ca-additional.pem
  ```

  **ca-additional.pem是harbor的自签名cert证书**，要重命名为ca-additional.pem

### 安装使用自签名SAN证书

必须使用CA签名，不然纳管agent报错

```
Certificate chain is not complete, please check if all needed intermediate certificates are included in the server certificate (in the correct order) and if the cacerts setting in Rancher either contains the correct CA certificate (in the case of using self signed certificates) or is empty (in the case of using a certificate signed by a recognized CA). Certificate information is displayed above. error: Get \"https://ums.xxx.cn:31393\": x509: certificate signed by unknown authority
```

准备openssl.conf  证书生成参考https://www.golinuxcloud.com/openssl-subject-alternative-name/

```
[req]
distinguished_name = req_distinguished_name
req_extensions = req_ext
prompt = no

[req_distinguished_name]
CN = ums.xxx.cn

[req_ext]
subjectAltName = @alt_names

[alt_names]
IP.1 = 192.168.25.99
DNS.1 = ums.xxx.cn
DNS.2 = *.xxx.cn
```

**使用自签名证书不用certmanager**，要保证**hostname CN subjectAltName** 一致

```sh
# 生成ca
openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 36500 -out ca.crt -subj "/C=xx/ST=x/L=x/O=x/OU=x/CN=ca/emailAddress=x/"

# 给生成的证书CA签名
openssl genrsa -out tls.key 2048
openssl req -new -key tls.key -out tls.csr -config openssl.conf
openssl x509 -req -days 36500 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt -extensions req_ext -extfile openssl.conf

# 验证SAN
openssl x509 -text -noout -in tls.crt | grep -A 1 "Subject Alternative Name"
```







```sh
kubectl -n cattle-system create secret tls tls-rancher-ingress \
  --cert=tls.crt \
  --key=tls.key
  
# Error from server (AlreadyExists): secrets "tls-rancher-ingress" already exists 报错的话先删除再重新创建
kubectl -n cattle-system delete secret tls-rancher-ingress


mv ca.crt cacerts.pem
kubectl -n cattle-system create secret generic tls-ca \
  --from-file=cacerts.pem=./cacerts.pem
```

自签名证书安装  Private CA signed certificate , add `--set privateCA=true` to the command:

```sh
helm install rancher rancher-2.6.5.tgz \
  --namespace cattle-system \
  --set hostname=205.xxx.cn \
  --set bootstrapPassword=fafafa \
  --set replicas=1 \
  --set useBundledSystemChart=true \
  --set additionalTrustedCAs=true \
  --set ingress.tls.source=secret \
  --set privateCA=true
```

验证

```sh
kubectl -n cattle-system get pod

NAME                              READY   STATUS    RESTARTS   AGE
rancher-6f7df66cf7-2fnw5          1/1     Running   0          3d23h
rancher-webhook-6994b4677-tpvf8   1/1     Running   0          3d23h
```

 

# 备份还原Rancher Backups (2.1.2)

[参考](https://rancher.com/docs/rancher/v2.6/en/backups/back-up-rancher/)

镜像

```
rancher/backup-restore-operator:v2.1.2
rancher/kubectl:v1.21.9
```

安装

![image-20220606161846013](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24ely1h3q1ewekcqj227y0tajw0.jpg)

根据需要安装就行，可以保存到**pv或者s3**上

- **docker单节点有可能会遇到k8s版本太高，rancher backup 版本太低导致**

  ```sh
  no matches for kind "CustomResourceDefinition" in version "apiextensions.k8s.io/v1beata1"
  ```

  例如

  ![image-20220824101348866](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h5hmx8lr9wj21uk0qqthc.jpg)

  这时候要直接`docker cp`适合版本的安装文件到容器里面，使用`helm_v3`安装。

  

验证

```sh
kubectl -n cattle-resources-system get pod

NAME                              READY   STATUS    RESTARTS   AGE
rancher-backup-7f9ff4c6cb-68jd5   1/1     Running   0          5d
```

刷新后多出选项

![image-20220606162818445](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24ely1h3q1evxpkvj21yb0u00xt.jpg)

关于minio配置，**必须用https**

- [minio自身用https](https://github.com/minio/minio/issues/6820#issuecomment-439247023)

  在docker目录`/root/.minio`对应的挂载卷目录，创建`certs`目录，对应的密钥和证书重命名成 `private.key` and `public.crt`，**使用SAN完整证书链**，加入**docker container ip**(可以通过 docker log 查看到)，**server ip**

  ```sh
  docker run -d -p 19000:9000 -p 15000:5000 --name minio -e "MINIO_ROOT_USER=admin" -e "MINIO_ROOT_PASSWORD=12345678"  -v /data/minio/data:/data   -v /data/minio/config:/root/.minio minio/minio server --console-address ":5000" /data
  ```

  开启成功的话

  ```sh
  docker restart minio
  
  docker logs -f minio
  
  WARNING: Detected Linux kernel version older than 4.0.0 release, there are some known potential performance problems with this kernel version. MinIO recommends a minimum of 4.x.x linux kernel version for best performance
  API: https://172.17.0.3:9000  https://127.0.0.1:9000
  
  Console: https://172.17.0.3:5000 https://127.0.0.1:5000
  
  Documentation: https://docs.min.io
  Finished loading IAM sub-system (took 0.0s of 0.0s to load data).
  ```

- 关于账密

  创建secret

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: creds
  type: Opaque
  data:
    accessKey: <Enter your base64-encoded access key>
    secretKey: <Enter your base64-encoded secret key>
  ```

- 配置填写

  endpoint ca  必须填写**使用证书base64后的内容**，不要直接用证书明文。

  ![image-20220606163428904](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24ely1h3q1evhd72j21uu0qq78q.jpg)

# 迁移rancher

假设集群A迁移到B，**A要先备份**，尽量[AB集群所用Kubernetes 版本相同，因为不同的版本apiVersion不一样](https://rancher.com/docs/rancher/v2.6/en/backups/migrating-rancher/#2-restore-from-backup-using-a-restore-custom-resource)

## 先备份

无论是docker或者集群安装的rancher，**备份必须使用rancher backup operator，而不是参考[docker backup](https://rancher.com/docs/rancher/v2.5/en/backups/docker-installs/docker-backups/)**，否则使用operator恢复备份文件时候会报错([参考Rancher backup panics when it encounters an invalid tarball](https://github.com/rancher/rancher/issues/31801))

```sh
goroutine 403 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic(0x1691bc0, 0xc0005a10c0)
        /go/pkg/mod/k8s.io/apimachinery@v0.18.0/pkg/util/runtime/runtime.go:74 +0xa3
k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
        /go/pkg/mod/k8s.io/apimachinery@v0.18.0/pkg/util/runtime/runtime.go:48 +0x82
panic(0x1691bc0, 0xc0005a10c0)
        /usr/local/go/src/runtime/panic.go:679 +0x1b2
github.com/rancher/backup-restore-operator/pkg/controllers/restore.getGVR(0xc0000aa000, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
        /go/src/github.com/rancher/backup-restore-operator/pkg/controllers/restore/controller.go:677 +0x2d4
github.com/rancher/backup-restore-operator/pkg/controllers/restore.(*handler).loadDataFromFile(0xc0000892c0, 0xc0003a3180, 0xc000248a00, 0x13d, 0x200, 0xc0003b8630, 0xc000291380, 0x0, 0xc0004ac120)
        /go/src/github.com/rancher/backup-restore-operator/pkg/controllers/restore/download.go:109 +0x17d
github.com/rancher/backup-restore-operator/pkg/controllers/restore.(*handler).LoadFromTarGzip(0xc0000892c0, 0xc0004ac120, 0x2f, 0xc0003b8630, 0xc000291380, 0x0, 0x0)
```

统一参考[备份还原Rancher Backups (2.1.2)](#备份还原Rancher Backups (2.1.2))

## minio可能会遇到的问题

检查minio和集群时区使用`date`,不然minio下载备份数据报错`The difference between the request time and the server's time is too large., requeuing`



```sh
# 使用timedatectl比较时区和rtc
root@d-ecs-38357230:~/af# timedatectl
                      Local time: Tue 2022-06-07 17:17:30 CST
                  Universal time: Tue 2022-06-07 09:17:30 UTC
                        RTC time: Tue 2022-06-07 16:58:33
                       Time zone: Asia/Shanghai (CST, +0800)
       System clock synchronized: no
systemd-timesyncd.service active: yes
                 RTC in local TZ: yes


# 可能会用到的命令 调整时区和
timedatectl set-timezone Asia/Shanghai
timedatectl set-local-rtc 1

# 修改时间 可以使用watch -n 1 date查看minio服务器实时时间
date -s "2022-06-07 17:15:40"
```

## 迁移流程

下载rancher-backup相关的包

```sh
helm repo add rancher-charts https://charts.rancher.io
helm repo update

helm fetch rancher-charts/rancher-backup-crd --version 2.1.2
helm fetch rancher-charts/rancher-backup --version 2.1.2
```



在B安装，[安装前可以使用这个脚本清理节点](https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/)

```sh
helm install rancher-backup-crd rancher-backup-crd-2.1.2.tgz -n cattle-resources-system --create-namespace

# docker pull 版本对应的镜像包，然后指定
helm install rancher-backup rancher-backup-2.1.2.tgz -n cattle-resources-system  -f values.yaml
```

values.yaml

```yaml
image:
  repository: harbor.xxx.cn:4443/rancher/backup-restore-operator
  tag: v2.1.2
global:
  kubectl:
    repository: harbor.xxx.cn:4443/rancher/kubectl
    tag: v1.21.9
```



验证

```sh
kubectl get pod -n cattle-resources-system
NAME                             READY   STATUS    RESTARTS   AGE
rancher-backup-94944dc7b-b87z9   1/1     Running   0          171m
```

在B创建和A相同的**minio secert**

```yaml
apiVersion: v1
data:
  accessKey: YWRtaW4=
  secretKey: MTIzNDU2Nzg=
kind: Secret
metadata: 
  name: s3minio
  namespace: default
type: Opaque
```

创建restore自定义资源，在恢复自定义资源中，`prune`必须设置为 false。**endpointCA 证书内容得base64编码**

```yaml
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-migration
spec:
  backupFilename: minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz
  prune: false
  storageLocation:
    s3:
      bucketName: rancher-backup
      credentialSecretName: s3minio
      credentialSecretNamespace: default
      endpoint: xxx.xxx.xxx.205:9000
      endpointCA: xxxx
      insecureTLSSkipVerify: true
```



验证

```sh
vim restore-apply.yaml
kubectl apply -f restore-apply.yaml
restore.resources.cattle.io/restore-migration created

# 看restore状态
kubectl get restore
NAME                BACKUP-SOURCE   BACKUP-FILE                                                              AGE   STATUS
restore-migration                   minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz   8s


# 看log
kubectl get pods -n cattle-resources-system
NAME                             READY   STATUS    RESTARTS   AGE
rancher-backup-94944dc7b-b87z9   1/1     Running   0          3h5m

kubectl logs -n cattle-resources-system --tail 100 -f rancher-backup-94944dc7b-b87z9



# 成功后可以开始安装rancher
kubectl get restore
NAME                BACKUP-SOURCE   BACKUP-FILE                                                              AGE     STATUS
restore-migration   S3              minio-59ae5e34-b0b5-484a-9c51-d4df16766257-2022-06-06T07-19-59Z.tar.gz   4m52s   Completed
```

参考上面安装[高可用rancher2.6.5](#高可用rancher)，**rancher安装命令hostname和A安装的保持一致**

## 迁移完成后的配置修改

检查server-url是否符合实际情况，和高可用安装时候的hostname一致

![image-20220819102625183](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h5bv6r5qr0j21pw0r4acp.jpg)

针对从docker安装迁移到集群高可用安装的情况，主要是agent的连接从IP变成了域名，所以还需要配置好下游集群的agent

![image](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h5bvkfe09yj20j604rt8x.jpg)

- 获取kubeconfig，[参考](https://forums.rancher.cn/t/rancher-v2-6-rancher-server-ip/729#kubeconfig-4)

  对于导入集群不需要考虑，创建的集群分两种情况

  - 有从UI界面获取过kubeconfig，可以通过切换context

    ```sh
    kubectl config get-contexts
    kubectl config use-context [context-name]
    ```

    Example:

    ```sh
    CURRENT   NAME                        CLUSTER                     AUTHINFO     NAMESPACE
    *         my-cluster                  my-cluster                  user-46tmn
              my-cluster-controlplane-1   my-cluster-controlplane-1   user-46tmn
    ```

    在本例中，当您使用`kubectl`第一个上下文时`my-cluster`，您将通过 Rancher 服务器进行身份验证。

    使用第二个上下文，`my-cluster-controlplane-1`您将使用授权的集群端点进行身份验证，直接与下游 RKE 集群通信。

  - 已经对开连接了，就下载不了，可以从下游集群具有`controlplane`的节点上生成kubeconfig。

    参考https://gist.github.com/superseb/b14ed3b5535f621ad3d2aa6a4cd6443b

    ```sh
    docker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination "/etc/kubernetes" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=io.cattle.agent=true) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\"full-cluster-state\" | jq -r .currentState.certificatesBundle.\"kube-admin\".config | sed -e "/^[[:space:]]*server:/ s_:.*_: \"https://127.0.0.1:6443\"_"' > kubeconfig_admin.yaml
    
    ```

    需要安装jq，比较麻烦。可以把方法拆分

    ```sh
    docker exec -it kube-apiserver bash
    export KUBECONFIG=/etc/kubernetes/ssl/kubecfg-kube-node.yaml
    
    kubectl get configmap -n kube-system full-cluster-state -o json > full-cluster-state.json
    
    ```

    拿到json后，在执行

    ```sh
     cat full-cluster-state.json|jq -r .data.\"full-cluster-state\" | jq -r .currentState.certificatesBundle.\"kube-admin\".config | sed -e "/^[[:space:]]*server:/ s_:.*_: \"https://127.0.0.1:6443\"_" > kubeconfig_admin.yaml
    
    ```

- 修改coredns

  ```sh
  kubectl edit cm coredns -n kube-system
  ```

  修改hosts

  ```yaml
  data:
    Corefile: |
      .:53 {
          errors
          health {
            lameduck 5s
          }
          hosts {
            xxxx 201.xxxx.cn
            fallthrough
          }
          ready
          kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
          }
          prometheus :9153
          forward . "/etc/resolv.conf"
          cache 30
          loop
          reload
          loadbalance
      } # STUBDOMAINS - Rancher specific change
  kind: ConfigMap
  ```

  delete 对应的coredns pod，让hosts起效

  ```sh
  kubectl rollout restart deployment  coredns -n kube-system
  ```

  

- 修改agent配置

  ```sh
  kubectl edit deploy cattle-cluster-agent -n cattle-system
  ```

  修改CATTLE_SERVER，改成新的域名，同时找到挂载的secret

  ```yaml
        containers:
        - env:
         ...
          - name: CATTLE_IS_RKE
            value: "true"
          - name: CATTLE_SERVER
            value: https://xxx.xxx.cn  #修改
          .....
          image: rancher/rancher-agent:v2.6.5
          
          .....
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-cdcb52a  #复制
  ```

  执行

  ```sh
  kubectl edit secret -n cattle-system cattle-credentials-cdcb52a
  ```

  修改url，填入CATTLE_SERVER值的base64编码

  ```yaml
  apiVersion: v1
  data:
    namespace: xxxx
    token: xxx
    url: aHR0cHM6Ly8yMDEudWlpbi5jbg==  # 修改
  kind: Secret
  .....
  type: Opaque
  ```

  最后delete cattle-cluster-agent对应的pod
  
  ```sh
  kubectl rollout restart deployment  cattle-cluster-agent -n cattle-system
  ```
  
  





# 升级rancher2.5.5-2.6.5

## 单机版

[参考](https://docs.rancher.cn/docs/rancher2.5/installation/other-installation-methods/single-node-docker/single-node-upgrades/_index)

主要使用`--volumes-from`实现容器间数据共享

```sh
# 停止旧版本容器
docker stop <OLD_RANCHER_CONTAINER_NAME>

# 备份
docker create --volumes-from <OLD_RANCHER_CONTAINER_NAME> --name rancher-data rancher/rancher:<OLD_RANCHER_CONTAINER_TAG>

# 在当前位置生成rancher数据压缩包
docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher

# 启动新rancher server
docker run -d --privileged --volumes-from rancher-data \
  --restart=unless-stopped  --name rancher2.6.5 \
  -e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs \
  -e CATTLE_SYSTEM_CATALOG=bundled \
  -p 1080:80 -p 1443:443 \
    rancher/rancher:v2.6.5



docker run -d --privileged --volumes-from rancher-data \
  --restart=unless-stopped  --name rancher2.6.5 \
  -e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs \
  -e CATTLE_SYSTEM_CATALOG=bundled \
  --add-host harbor.xxx.cn:10.xx.xx.205 \
  -p 1080:80 -p 1443:443 \
    rancher/rancher:v2.6.5


docker run -d --privileged --volumes-from rancher-data \
  --restart=unless-stopped  --name rancher2.6.5 \
  -e SSL_CERT_DIR="/container/certs" -v /root/harbor/cert:/container/certs \
  -e CATTLE_SYSTEM_CATALOG=bundled \
  --add-host harbor.uniin.cn:10.81.25.149    --add-host gitlab.uniin.cn:10.41.31.1 \
  -p 1080:80 -p 1443:443 \
    rancher/rancher:v2.6.5

# 如果自定义证书
docker run -d --privileged --restart=unless-stopped \
    -p 80:80 -p 443:443 \
    -v /<CERT_DIRECTORY>/<FULL_CHAIN.pem>:/etc/rancher/ssl/cert.pem \
    -v /<CERT_DIRECTORY>/<PRIVATE_KEY.pem>:/etc/rancher/ssl/key.pem \
    -v /<CERT_DIRECTORY>/<CA_CERTS.pem>:/etc/rancher/ssl/cacerts.pem \
    -e CATTLE_SYSTEM_DEFAULT_REGISTRY=<REGISTRY.YOURDOMAIN.COM:PORT> \ # Set a default private registry to be used in Rancher
    -e CATTLE_SYSTEM_CATALOG=bundled \ #Available as of v2.3.0，use the packaged Rancher system charts
    <REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher:<RANCHER_VERSION_TAG>
    
    
# 删除旧容器
docker rm -f <OLD_RANCHER_CONTAINER_NAME>
```

回滚

```sh
# 解压数据到rancher-data

docker run  --volumes-from rancher-data \
-v $PWD:/backup busybox sh -c "rm /var/lib/rancher/* -rf \
&& tar zxvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz"

# 运行旧版本rancher
docker run -d --volumes-from rancher-data \
 --restart=unless-stopped \
 -p 80:80 -p 443:443 \
 --privileged \
 rancher/rancher:<PRIOR_RANCHER_VERSION>
```



## 高可用

升级rancher，**rancher依赖cert-manager的crd**，所以最好先找到升级版本的rancher对应的cert-manager，**cert-manager也要升级**

**例如rancher 2.6.5依赖cert-manager 1.7，[参考Install/Upgrade Rancher on a Kubernetes Cluster](https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/)**



### 备份

备份可以参考上面，或者看https://docs.rancher.cn/docs/rancher2.5/backups/back-up-rancher/_index/



```sh
# 获取旧版本的values
helm get values rancher -n cattle-system -o yaml > values.yaml

# 卸载 Rancher
helm delete rancher -n cattle-system
```





### 升级cert-manager 

[参考](https://docs.rancher.cn/docs/rancher2.5/installation/resources/upgrading-cert-manager/_index/)



[备份现有资源](https://cert-manager.io/docs/tutorials/backup/)

```sh
kubectl get -o yaml --all-namespaces \
issuer,clusterissuer,certificates,certificaterequests > cert-manager-backup.yaml
```



```sh
# 卸载现有部署
helm uninstall cert-manager -n cert-manager
kubectl delete namespace cert-manager

# 删除旧crd
kubectl delete -f old-version-crd.yaml

# 安装新版cert-manager
kubectl apply -f new-verson-crds.yaml

helm install cert-manager cert-manager-v1.7.1.tgz \
  --namespace cert-manager \
  --create-namespace \
  --version v1.7.1
```

[恢复备份资源](https://cert-manager.io/docs/tutorials/backup/#restoring-resources)

```sh
kubectl apply -f cert-manager-backup.yaml


# 验证
kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m

```



### 升级rancher

```sh
# 安装新版rancher
helm install rancher rancher-2.6.5.tgz -n cattle-system -f values.yaml
```





# 自定义集群重新以导入方式纳管

[根据文档描述，直接UI界面删除自定义集群，会造成k8s的组件也会被删除，所以不能直接界面操作](https://docs.rancher.cn/docs/rancher2.5/cluster-admin/cleaning-cluster-nodes/_index/#%E5%88%A0%E9%99%A4%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F)

场景：A rancher的自定义下游集群想迁移到B上（AB可以一样）

- B先要创建导入集群，得到部署命令行，拿到对应的yaml

- `kubectl delete -f xxx.yaml`相当于删掉与A的连接，删除后kubectl会失效

  ```sh
  kubectl get pods -A
  error: You must be logged in to the server (Unauthorized)
  ```

- 重新获取kubeconfig [参考这个里面关于从kube-apiserver容器里面获取kubeconfig](#迁移完成后的配置修改)

- 使用新的kubeconfig，执行B的导入纳管命令

**重新使用rke管理集群**

 [参考这里获取kubeconfig，这里面有整个集群的信息](#迁移完成后的配置修改)

![image-20220922120337027](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h6f92a8l0yj21x40jkn03.jpg)

使用key `full-cluster-state`的内容创建**cluster.rkestate**，用于rke命令操作集群

```sh
kubectl get configmap -n kube-system full-cluster-state -o jsonpath='{.data.full-cluster-state}' > cluster.rkestate
```



根据集群信息编写**cluster.yml**，[完整示例](https://docs.rancher.cn/docs/rke/example-yamls/_index/#%E6%9C%80%E5%B0%8F%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B)

```yaml
nodes:
  - address: xxxx
    user: root
    role: ["controlplane", "etcd", "worker"]
    ssh_key_path: /root/.ssh/id_rsa
    port: 22
		hostname_override: "" # 参考节点名，缺了可能相同IP两个节点

kubernetes_version: v1.19.16-rancher1-5  # 参考cluster.rkestate的kubernetesVersion非常重要
cluster_name: xxxx  # 参考
```



尽量寻找支持该kubernetesVersion的[rke版本](https://github.com/rancher/rke/releases/tag/v1.3.11)，只要前面`v1.19.16-rancher1`一样就可以。

**cluster.rkestate和cluster.yml要在同一个目录**，处理完这两个文件后，使用rke来操作了。同时保证**cluster.rkestate真实反映集群节点现状**

```sh
rke up -config cluster.yml
```

**遇到问题**

- [Centos系统不能用root作为SSH用户](https://docs.rancher.cn/docs/rke/os/_index#rhel%E3%80%81oel-%E6%88%96-centos)

  ```sh
  Can't retrieve Docker Info: error during connect: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info": Unable to access the Docker socket (/var/run/docker.sock). Please check if the configured user can execute `docker ps` on the node, and if the SSH server version is at least version 6.7 or higher. If you are using RedHat/CentOS, you can't use the user `root`. Please refer to the documentation for more instructions. Error: ssh: rejected: administratively prohibited (open failed) 
  ```

  新增用户

  ```sh
  # 所有机器 新增rancher用户，添加到docker组(rke安全限制)
  useradd rancher -G docker
  echo "123456" | passwd --stdin rancher
  
  ssh-keygen
  ssh-copy-id -i ~/.ssh/id_rsa.pub rancher@192.168.0.22
  ```

  根据节点修改cluster.yml的ssh_key_path和user
  
  ```yaml
  nodes:
    - address: xxxx
      user: rancher
      role: ["controlplane", "etcd", "worker"]
      ssh_key_path: /home/rancher/.ssh/id_rsa
      port: 22
  
  ```
  
  







# 端口开放准备

[rancher 端口准备](https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/installation-requirements/port-requirements)

## k3s

入站规则

| Protocol | Port  | Source                                                       | Description                                          |
| -------- | ----- | ------------------------------------------------------------ | ---------------------------------------------------- |
| TCP      | 80    | Load balancer/proxy that does external SSL termination       | Rancher UI/API when external SSL termination is used |
| TCP      | 443   | server nodesagent nodeshosted/registered Kubernetesany source that needs to be able to use the Rancher UI or API | Rancher agent, Rancher UI/API, kubectl               |
| TCP      | 6443  | K3s server nodes                                             | Kubernetes API                                       |
| UDP      | 8472  | K3s server and agent nodes                                   | Required only for Flannel VXLAN.                     |
| TCP      | 10250 | K3s server and agent nodes                                   | kubelet                                              |

出站规则

| Protocol | Port | Destination                                       | Description                                   |
| -------- | ---- | ------------------------------------------------- | --------------------------------------------- |
| TCP      | 22   | Any node IP from a node created using Node Driver | SSH provisioning of nodes using Node Driver   |
| TCP      | 443  | git.rancher.io                                    | Rancher catalog                               |
| TCP      | 2376 | Any node IP from a node created using Node driver | Docker daemon TLS port used by Docker Machine |
| TCP      | 6443 | Hosted/Imported Kubernetes API                    | Kubernetes API server                         |



## RKE

节点间流量规则

| Protocol | Port  | Description                                     |
| -------- | ----- | ----------------------------------------------- |
| TCP      | 443   | Rancher agents                                  |
| TCP      | 2379  | etcd client requests                            |
| TCP      | 2380  | etcd peer communication                         |
| TCP      | 6443  | Kubernetes apiserver                            |
| TCP      | 8443  | Nginx Ingress's Validating Webhook              |
| UDP      | 8472  | Canal/Flannel VXLAN overlay networking          |
| TCP      | 9099  | Canal/Flannel livenessProbe/readinessProbe      |
| TCP      | 10250 | Metrics server communication with all nodes     |
| TCP      | 10254 | Ingress controller livenessProbe/readinessProbe |

入站规则

| Protocol | Port | Source                                                       | Description                     |
| -------- | ---- | ------------------------------------------------------------ | ------------------------------- |
| TCP      | 22   | RKE CLI                                                      | SSH provisioning of node by RKE |
| TCP      | 80   | Load Balancer/Reverse Proxy                                  | HTTP traffic to Rancher UI/API  |
| TCP      | 443  | Load Balancer/Reverse ProxyIPs of all cluster nodes and other API/UI clients | HTTPS traffic to Rancher UI/API |
| TCP      | 6443 | Kubernetes API clients                                       | HTTPS traffic to Kubernetes API |

出站规则

| Protocol | Port               | Source                                                | Description                                |
| -------- | ------------------ | ----------------------------------------------------- | ------------------------------------------ |
| TCP      | 443                | `35.160.43.145`,`35.167.242.46`,`52.33.59.17`         | Rancher catalog (git.rancher.io)           |
| TCP      | 22                 | Any node created using a node driver                  | SSH provisioning of node by node driver    |
| TCP      | 2376               | Any node created using a node driver                  | Docker daemon TLS port used by node driver |
| TCP      | 6443               | Hosted/Imported Kubernetes API                        | Kubernetes API server                      |
| TCP      | Provider dependent | Port of the Kubernetes API endpoint in hosted cluster | Kubernetes API                             |

## RKE2

入站规则

| Protocol | Port        | Source                                                       | Description                                                  |
| -------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| TCP      | 9345        | RKE2 agent nodes                                             | Kubernetes API                                               |
| TCP      | 6443        | RKE2 agent nodes                                             | Kubernetes API                                               |
| UDP      | 8472        | RKE2 server and agent nodes                                  | Required only for Flannel VXLAN                              |
| TCP      | 10250       | RKE2 server and agent nodes                                  | kubelet                                                      |
| TCP      | 2379        | RKE2 server nodes                                            | etcd client port                                             |
| TCP      | 2380        | RKE2 server nodes                                            | etcd peer port                                               |
| TCP      | 30000-32767 | RKE2 server and agent nodes                                  | NodePort port range                                          |
| TCP      | 5473        | Calico-node pod connecting to typha pod                      | Required when deploying with Calico                          |
| HTTP     | 8080        | Load balancer/proxy that does external SSL termination       | Rancher UI/API when external SSL termination is used         |
| HTTPS    | 8443        | hosted/registered Kubernetesany source that needs to be able to use the Rancher UI or API | Rancher agent, Rancher UI/API, kubectl. Not needed if you have LB doing TLS termination. |

通常允许所有出站流量。

## 总结

常用端口

```sh
TCP Ports
22, 80, 443, 2376, 2379, 2380, 6443, 9099, 9796, 10250, 10254, 30000-32767
UDP Ports
8472, 30000-32767
```

[网络插件端口](https://docs.ranchermanager.rancher.io/faq/container-network-interface-providers)，默认使用Canal

- WAVE插件 **TCP** 6783 **UDP** 6783-6784

- Calico插件 **TCP** 179,5473 **UDP** 4789

- Cilium插件 **TCP** 8472，4240

端口开放检测脚本

```sh
#!/bin/bash

TCP_PORTS="22088 22 80 443 2376 2379 2380 6443 9099 9796 10250 10254"
UDP_PORTS="8472"
REMOTE_HOST=$1
TIMEOUT_SEC=5
LOCAL_IP='hostname -I | cut -d' ' -f1'
function check() {
    res=$1
    PORT=$2
    proto=$3
    if [[ $res -eq 0 ]]
    then
        echo "$proto $PORT OPEN"
    elif [[ $res -eq 1 ]]
    then
        echo "$proto $PORT OPEN BUT NOT LISTEN"
    elif [[ $res -eq 124 ]]
    then
        echo "$proto $PORT NOT OPEN"
    else
        echo "$proto $PORT UNKONWN ERROR"
    fi
}
echo "check $LOCAL_IP -----> $REMOTE_HOST port"
for PORT in $TCP_PORTS
do
    timeout $TIMEOUT_SEC bash -c "</dev/tcp/$REMOTE_HOST/$PORT" &>/dev/null; res=$?
    check $res $PORT "tcp"
done

for PORT in $UDP_PORTS
do
    timeout $TIMEOUT_SEC bash -c "</dev/tcp/$REMOTE_HOST/$PORT" &>/dev/null; res=$?
    check $res $PORT "udp"
done




```



## 端口测试

测试命令

```sh
# 添加规则 禁止tcp/9099 被访问
iptables -A INPUT -p tcp --dport 9099 -j DROP
# 删除规则
iptables -D INPUT -p tcp --dport 9099 -j DROP
```

- tcp/6443

  ![image-20220921151546035](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h6eakd16mjj227y0pc43i.jpg)



- tcp/2380,2379

  ```sh
  kubectl get pods -A
  Error from server: etcdserver: request timed out
  ```

  ![image-20220921151516344](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h6e8zhzpxdj227y0ng79y.jpg)

- udp/8472

  pod 正常，dns解析 访问pod ip都有问题

  ```sh
  / # nslookup default-http-backend
  ;; connection timed out; no servers could be reached
  
  
  / # wget 10.43.85.213
  Connecting to 10.43.85.213 (10.43.85.213:80)
  wget: can't connect to remote host (10.43.85.213): Operation timed out
  ```

  

- tcp/10254  nginx-ingress-controller 起不来

  ```sh
  kubectl get pods -n  ingress-nginx
  NAME                                    READY   STATUS             RESTARTS   AGE
  default-http-backend-6db58c58cd-bfk2h   1/1     Running            0          4h27m
  nginx-ingress-controller-lrx64          1/1     Running            0          4h13m
  nginx-ingress-controller-ztww2          0/1     CrashLoopBackOff   6          4h27m
  ```

- tcp/9099 canal 运行有问题

  ```sh
  kubectl get pods -n kube-system
  NAME                                       READY   STATUS      RESTARTS   AGE
  calico-kube-controllers-5898bd695c-cgl6f   1/1     Running     0          4h35m
  canal-2mwgg                                1/2     Running     1          4h35m
  canal-ffn6k                                2/2     Running     1          4h21m
  ```

- tcp/10250  Metrics server communication with all nodes

  节点CPU 内存会变成N/A  rancher看log会看不了

  ```sh
  kubectl logs -f --tail 200 xxx-5768967f5c-wmc2k -n xxx
  Error from server: Get "https://xxx:10250/containerLogs/xxx/xxx-5768967f5c-wmc2k/xxx?follow=true&tailLines=200": dial tcp xxxxx:10250: connect: no route to host
  ```

# rancher error 处理

## Failed to pull image "xxx": rpc error: code = Unknown desc = Error response from daemon: pull access denied for xxx, repository does not exist or may require '[docker](https://cloud.tencent.com/product/tke?from=10680) login'

pod莫名其妙拉不了镜像，宿主机可以拉，推测集群有问题，而集群靠kubelet来管理容器

```sh
cp ~/.docker/config.json /var/lib/kubelet/config.json

docker restart kubelet
```



## template system-library-rancher-monitoring 和kubeversion 不匹配

template system-library-rancher-monitoring incompatible with rancher version or cluster's [xxx] kubernetes version

![image-20220713172222587](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h57akaxgh3j20xy03aq3c.jpg)

参考 [https://github.com/rancher/rancher/issues/37039#issuecomment-1176320933](https://github.com/rancher/rancher/issues/37039#issuecomment-1176320933)



- [迁移相关](https://github.com/rancher/rancher/issues/37039#issuecomment-1170970740)，看是不是开启了v1版本的monitor，新版rancher用的是v2

  ```sh
  # 设置检查集群配置
  enable_cluster_alerting: false
  enable_cluster_monitoring: false
  ```

  或者使用[这个脚本](https://github.com/bashofmann/rancher-monitoring-v1-to-v2#check-why-monitoring-v1-is-not-disabled)检查，是否存在迁移，自定义证书要加`--insecure`参数

  得到

  ```
  The Monitoring V1 operator does not appear to exist in cluster *******. Migration to Monitoring V2 should be possible.
  ```

  应该没啥问题

  

- 检查`system-library-rancher-monitoring`这个 cr的内容

  ```
  kubectl edit catalogtemplates system-library-rancher-monitoring 
  ```

  修改versions的第一项

  ```yaml
  spec:
    catalogId: system-library
    defaultVersion: 0.3.2
    description: Provides monitoring for Kubernetes which is maintained by Rancher 2.
    displayName: rancher-monitoring
    folderName: rancher-monitoring
    icon: https://coreos.com/sites/default/files/inline-images/Overview-prometheus_0.png
    projectURL: https://github.com/coreos/prometheus-operator
    versions:
    - digest: 08fbaee28d5a0efb79db02d9372629e2
      externalId: catalog://?catalog=system-library&template=rancher-monitoring&version=0.3.2
      kubeVersion: < 1.22.0-0  # 这个地方 改成 '>=1.21.0-0'
      rancherMinVersion: 2.6.1-alpha1
      version: 0.3.2
      versionDir: charts/rancher-monitoring/v0.3.2
      versionName: rancher-monitoring
  ```

## Template system-library-rancher-monitoring incompatible with rancher version or cluster's [local] kubernetes version

**最后我通过编辑clusters.management.cattle.io** CR  **conditions**，[里面的报错来修复它](https://github.com/rancher/rancher/issues/37039#issuecomment-1408222266)，因为即使我卸载了 system-monitor ，错误仍然存在。

Example

```sh
kubectl edit  clusters.management.cattle.io/local


conditions:
  - status: "True"
    type: Ready
  - lastUpdateTime: "2022-05-31T09:00:06Z"
    status: "True"
    type: BackingNamespaceCreated
  - lastUpdateTime: "2022-05-31T09:00:09Z"
    status: "True"
    type: DefaultProjectCreated
  - lastUpdateTime: "2022-05-31T09:00:09Z"
    status: "True"
    type: SystemProjectCreated
  - lastUpdateTime: "2022-05-31T09:00:07Z"
    status: "True"
    type: CreatorMadeOwner
  - lastUpdateTime: "2022-05-31T09:00:08Z"
```

从system-library-rancher-monitoring获取相关的参数

```sh
kubectl edit catalogtemplates system-library-rancher-monitoring -n cattle-global-data

  versions:
  - digest: 08fbaee28d5a0efb79db02d9372629e2
    externalId: catalog://?catalog=system-library&template=rancher-monitoring&version=0.3.2
    kubeVersion: =1.23.6+k3s1
    rancherMinVersion: 2.6.1-alpha1
    version: 0.3.2
    versionDir: charts/rancher-monitoring/v0.3.2
    versionName: rancher-monitoring
  - digest: 08fbaee28d5a0efb79db02d9372629e2
    externalId: catalog://?catalog=system-library&template=rancher-monitoring&version=0.3.2
    kubeVersion: '>=1.22.0-0'
    rancherMinVersion: 2.6.1-alpha1
    version: 0.3.2
    versionDir: charts/rancher-monitoring/v0.3.2
    versionName: rancher-monitoring

```

通过阅读源码，做了一些其他debug，其实是不可能会报错的，所以直接改cluster的cr也不会重新报错。

```go
func (m *Manager) LatestAvailableTemplateVersion(template *v3.CatalogTemplate, clusterName string) (*v32.TemplateVersionSpec, error) {
	versions := template.DeepCopy().Spec.Versions
	if len(versions) == 0 {
		return nil, errors.New("empty catalog template version list")
	}

	sort.Slice(versions, func(i, j int) bool {
		val1, err := semver.ParseTolerant(versions[i].Version)
		if err != nil {
			return false
		}

		val2, err := semver.ParseTolerant(versions[j].Version)
		if err != nil {
			return false
		}

		return val2.LT(val1)
	})

	for _, templateVersion := range versions {
		catalogTemplateVersion := &v3.CatalogTemplateVersion{
			Spec: templateVersion,
		}

		if err := m.ValidateChartCompatibility(catalogTemplateVersion, clusterName, ""); err == nil {
			return &templateVersion, nil
		}
	}

	return nil, errors.Errorf("template %s incompatible with rancher version or cluster's [%s] kubernetes version", template.Name, clusterName)
}
```








## could not find tenant ID context deadline exceeded

![image](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h57akd79d1j222s0u0tc0.jpg)

创建微软云凭证时候报错，**凭证有效能用**，POST **/meta/aksCheckCredentials**时候，报错。

```
{"error":"could not find tenant ID: Request failed: subscriptions.Client#Get: Failure sending request: StatusCode=0 -- Original Error: context deadline exceeded"}
```

通过错误提示，找到并阅读源码，`goCtx`用于控制请求超时时间，报错也和时间有关。

```go
func FindTenantID(ctx context.Context, env azure.Environment, subscriptionID string) (string, error) {
	goCtx, cancel := context.WithTimeout(ctx, findTenantIDTimeout)
	defer cancel()
....
}
```

![image-20220716094424410](https://cdn.jsdelivr.net/gh/631068264/img/e6c9d24egy1h57akcbesnj20xy03aq3c.jpg)

函数作用

- 调用azure sdk访问azure http api，验证cred。

排查方向

- 网络方面真的超时，通过debug，大概摸清访问的url，参数等，尝试请求发现可以访问，响应也快。
- 通过debug怀疑到时区问题引发的问题

系统默认是UTC，通过把时区设置成当地时区，并且reboot机器问题解决，只是restart rancher pod不起效

```sh
root@xxx:~# timedatectl
                      Local time: Tue 2022-07-19 03:20:00 UTC
                  Universal time: Tue 2022-07-19 03:20:00 UTC
                        RTC time: Tue 2022-07-19 03:20:01
                       Time zone: UTC (UTC, +0000)
       System clock synchronized: yes
systemd-timesyncd.service active: yes
                 RTC in local TZ: no
                 
# 修改时区
sudo timedatectl set-timezone Asia/Shanghai
```

## 下游集群重新导入Cluster agent is not connected

agent和rancher完全没有具体报错，agent卡在**Connecting to proxy**

```sh
INFO: Environment: CATTLE_ADDRESS=10.42.1.43 
....
time="2022-09-28T08:22:36Z" level=info msg="Rancher agent version v2.6.5 is starting"
time="2022-09-28T08:22:36Z" level=info msg="Listening on /tmp/log.sock"
time="2022-09-28T08:22:36Z" level=info msg="Connecting to wss://rancher.xxx.cn:31628/v3/connect/register with token starting with 2b2ch2cnsp6wxkzdm5djjbttr55"
time="2022-09-28T08:22:36Z" level=info msg="Connecting to proxy" url="wss://rancher.xxx.cn:31628/v3/connect/register"
```

**解决方法：修改rancher 集群信息AgentDeployed**

```sh
# rancher 导入集群命令所用的yaml路径例子，其中c-zqpccj就是集群id
https://xxxx/v3/import/xxxx_c-zqpcc.yaml
```

修改集群信息

```sh
kubectl edit clusters.management.cattle.io c-zqpcc
```

重置agent部署状态AgentDeployed

```yaml
  conditions:         
  ...
   - lastUpdateTime: "2022-02-16T07:16:07Z"                           
    status: "True"      # 改成False                                 
    type: AgentDeployed
    
    
```

清楚原来agent，然后重新运行导入命令就可以了



# 迁移到2.5.7

需要镜像

```sh
rancher/rancher-webhook:v0.2.6
rancher/shell:v0.1.18
rancher/rancher:v2.6.7
rancher/gitjob:v0.1.30
rancher/fleet:v0.3.10
rancher/fleet-agent:v0.3.10
rancher/rancher-agent:v2.6.7
rancher/mirrored-pause:3.6
rancher/rke-tools:v0.1.87
rancher/hyperkube:v1.24.2-rancher1
rancher/mirrored-coreos-etcd:v3.4.16-rancher1
rancher/mirrored-calico-cni:v3.23.1
rancher/mirrored-calico-pod2daemon-flexvol:v3.23.1
rancher/kube-api-auth:v0.1.8
rancher/mirrored-calico-node:v3.23.1
rancher/mirrored-flannelcni-flannel:v0.17.0
rancher/mirrored-cluster-proportional-autoscaler:1.8.5
rancher/mirrored-metrics-server:v0.6.1
rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.1.1
rancher/mirrored-coredns-coredns:1.9.3
rancher/mirrored-calico-kube-controllers:v3.23.1
rancher/nginx-ingress-controller:nginx-1.2.1-rancher1
```

[重新纳管集群参考](#迁移完成后的配置修改)

修改agent配置，那块只需要改secret，不需要改agent deploy

