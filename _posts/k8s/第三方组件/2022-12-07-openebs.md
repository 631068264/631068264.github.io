# OpenEBS

OpenEBS 管理每个 Kubernetes 节点上可用的存储，并使用该存储为有状态工作负载提供[本地](https://openebs.io/docs#local-volumes)或[分布式（也称为复制）](https://openebs.io/docs#replicated-volumes)持久卷。

![image-20221207104156989](https://cdn.jsdelivr.net/gh/631068264/img/008vxvgGgy1h8v1soll9aj31pz0u0n3w.jpg)

- local
  - OpenEBS 可以使用原始块设备或分区，或使用Hostpath上的子目录，或使用 LVM、ZFS 或稀疏文件来创建PV。
  - 本地卷直接安装到 Stateful Pod 中，数据路径中没有来自 OpenEBS 的任何额外开销，从而减少了延迟。
  - OpenEBS 为本地卷提供了额外的工具，用于监控、备份/恢复、灾难恢复、ZFS 或 LVM 支持的快照、基于容量的调度等。
-  [Distributed (aka Replicated) Volumes](https://openebs.io/docs#replicated-volumes)
  - OpenEBS 使用其引擎之一（Mayastor、cStor 或 Jiva）为每个分布式持久卷创建微服务。
  - Stateful Pod 将数据写入 OpenEBS 引擎，这些引擎将数据同步复制到集群中的多个节点。OpenEBS 引擎本身部署为 pod，由 Kubernetes 编排。当运行有状态 pod 的节点发生故障时，该 pod 将被重新调度到集群中的另一个节点，OpenEBS 使用其他节点上的可用数据副本提供对数据的访问。
  - Stateful Pod 使用 iSCSI（cStor 和 Jiva）或 NVMeoF（Mayastor）连接到 OpenEBS 分布式持久卷。
  - OpenEBS cStor 和 Jiva 专注于存储的易用性和耐用性。这些引擎分别使用定制版本的 ZFS 和 Longhorn 技术将数据写入存储。
  - OpenEBS Mayastor 是最新的引擎，以耐用性和性能为设计目标而开发；OpenEBS Mayastor 有效地管理计算（hugepages、内核）和存储（NVMe 驱动器）以提供快速的分布式块存储。

OpenEBS 贡献者更喜欢将分布式块存储卷称为**复制卷**，以避免与传统的分布式块存储混淆，原因如下：

- 分布式块存储倾向于将卷的数据块分片到集群中的多个节点上。复制卷将卷的所有数据块保存在节点上，并且为了持久性将整个数据复制到集群中的其他节点。
- 在访问卷数据时，分布式块存储依赖于元数据哈希算法来定位块所在的节点，而复制卷可以从任何持久保存数据的节点（也称为副本节点）访问数据。
- 与传统的分布式块存储相比，复制卷的爆炸半径更小。
- 复制卷专为云原生有状态工作负载而设计，这些工作负载需要大量容量的卷，这些容量通常可以从单个节点提供服务，而不是数据在集群中的多个节点之间分片的单个大卷。



```
openebs/node-disk-manager:2.0.0
openebs/node-disk-operator:2.0.0
openebs/provisioner-localpv:3.3.0
openebs/m-apiserver:2.12.2
openebs/openebs-k8s-provisioner:2.12.2
openebs/provisioner-localpv:3.3.0
openebs/snapshot-controller:2.12.2
openebs/snapshot-provisioner:2.12.2
openebs/admission-server:2.12.2
openebs/linux-utils:3.3.0
openebs/m-exporter:2.12.2
openebs/jiva:2.12.2
openebs/cstor-pool:2.12.2
openebs/cstor-pool-mgmt:2.12.2
openebs/cstor-istgt:2.12.2
openebs/cstor-volume-mgmt:2.12.2
```





[先决条件](https://openebs.io/docs/user-guides/prerequisites)



[openceb helm 参数用法](https://github.com/openebs/charts/tree/d-master/charts/openebs)





rancher rke config

```yaml
    kubelet: 
      extra_binds: 
      - "/etc/iscsi:/etc/iscsi"
      - "/sbin/iscsiadm:/sbin/iscsiadm"
      - "/var/lib/iscsi:/var/lib/iscsi"
      - "/lib/modules"
      - "/var/openebs/local:/var/openebs/local"
      - "/usr/lib64/libcrypto.so.10:/usr/lib/libcrypto.so.10"
      - "/usr/lib64/libopeniscsiusr.so.0.2.0:/usr/lib/libopeniscsiusr.so.0.2.0"
```





```sh
helm repo add openebs https://openebs.github.io/charts
helm repo update
helm fetch openebs/openebs


helm install openebs openebs-3.3.1.tgz -n openebs --create-namespace


kubectl get pods -n openebs

NAME                                           READY   STATUS    RESTARTS   AGE
openebs-localpv-provisioner-77c9bcfd96-bb9fz   1/1     Running   0          54s
openebs-ndm-2blrl                              1/1     Running   0          54s
openebs-ndm-5vrsc                              1/1     Running   0          54s
openebs-ndm-kqfkx                              1/1     Running   0          54s
openebs-ndm-operator-5d55748dfd-gtstj          1/1     Running   0          54s

kubectl get sc
NAME               PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
openebs-device     openebs.io/local   Delete          WaitForFirstConsumer   false                  2m4s
openebs-hostpath   openebs.io/local   Delete          WaitForFirstConsumer   false                  2m4s
```

check hostpath

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-hostpath-pvc
spec:
  storageClassName: openebs-hostpath
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5G
      
---

apiVersion: v1
kind: Pod
metadata:
  name: hello-local-hostpath-pod
spec:
  volumes:
  - name: local-storage
    persistentVolumeClaim:
      claimName: local-hostpath-pvc
  containers:
  - name: hello-container
    image: busybox
    command:
       - sh
       - -c
       - 'while true; do echo "`date` [`hostname`] Hello from OpenEBS Local PV." >> /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done'
    volumeMounts:
    - mountPath: /mnt/store
      name: local-storage
```



```sh
kubectl get pod hello-local-hostpath-pod

NAME                       READY   STATUS    RESTARTS   AGE
hello-local-hostpath-pod   1/1     Running   0          24s

kubectl get pvc local-hostpath-pvc

NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
local-hostpath-pvc   Bound    pvc-7eb39fb7-6bf0-40f2-8416-e7730c8c3433   5G         RWO            openebs-hostpath   33s
```

# ECK

https://www.elastic.co/guide/en/cloud-on-k8s/2.0/k8s-install-helm.html



```sh
helm repo add elastic https://helm.elastic.co
helm repo update


helm fetch elastic/eck-operator-crds --version 2.0.0
helm fetch elastic/eck-operator --version 2.0.0

helm install elastic-operator-crds elastic/eck-operator-crds
helm install elastic-operator eck-operator-2.0.0.tgz -n elastic-system --create-namespace -f values.yaml
```



values.yaml

```yaml
image:
  repository: harbor.xx.cn:20000/eck/eck-operator

config:
  containerRegistry: harbor.xx.cn:20000
  metricsPort: "10254"

podMonitor:
  enabled: true
  labels:
    "app": "eck"
                                                                                                                                 
```

image

```
docker.elastic.co/eck/eck-operator:2.0.0
docker.elastic.co/elasticsearch/elasticsearch:8.5.2
docker.elastic.co/kibana/kibana:8.5.2
```







创建Elasticsearch Kibana 实例

```yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 8.5.2
  nodeSets:
  - name: default
    count: 1
    config:
      node.store.allow_mmap: false

---

apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
spec:
  version: 8.5.2
  count: 1
  elasticsearchRef:
    name: quickstart
```

# kafka



- [Strimzi 概述指南](https://strimzi.io/docs/operators/latest/overview.html)
- [Deploying and Upgrading](https://strimzi.io/docs/operators/latest/deploying.html#deploy-intro_str)
- [CRD example](https://github.com/strimzi/strimzi-kafka-operator/tree/0.32.0/examples)



```
quay.io/strimzi/operator:0.32.0
quay.io/strimzi/kafka:0.32.0-kafka-3.3.1
```









[下载helm chart](https://github.com/strimzi/strimzi-kafka-operator/releases/tag/0.32.0)

```sh
helm install strimzi-kafka-operator strimzi-kafka-operator-helm-3-chart-0.32.0.tgz -n kafka --create-namespace -f values.yaml
```

values.yaml

```yaml
defaultImageRegistry: harbor.xx.cn:20000
```



```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    version: 3.3.1
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.3"
    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 100Gi
        deleteClaim: false
  zookeeper:
    replicas: 1
    storage:
      type: persistent-claim
      size: 100Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
```





# minio

[安装参考](https://github.com/minio/operator/tree/master/helm/operator)

```sh
helm repo add minio https://operator.min.io/

helm repo update

helm fetch minio/operator
helm fetch minio/tenant

helm install minio-operator  operator-4.5.5.tgz -n minio-operator --create-namespace
```



```sh
NOTES:
1. Get the JWT for logging in to the console:
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: console-sa-secret
  namespace: minio-operator
  annotations:
    kubernetes.io/service-account.name: console-sa
type: kubernetes.io/service-account-token
EOF
kubectl -n minio-operator  get secret console-sa-secret -o jsonpath="{.data.token}" | base64 --decode

2. Get the Operator Console URL by running these commands:
  kubectl --namespace minio-operator port-forward svc/console 9090:9090
  echo "Visit the Operator Console at http://127.0.0.1:9090"
```



```sh
helm install --namespace tenant-ns --create-namespace tenant tenant-4.5.5.tgz -f values.yaml
```



```yaml
tenant:
  image:
    repository: minio/minio
```





```yaml
apiVersion: minio.min.io/v2
kind: Tenant
metadata:
  name: storage
  namespace: minio-tenant
spec:
  ## Specification for MinIO Pool(s) in this Tenant.
  pools:
    ## Servers specifies the number of MinIO Tenant Pods / Servers in this pool.
    ## For standalone mode, supply 1. For distributed mode, supply 4 or more.
    ## Note that the operator does not support upgrading from standalone to distributed mode.
    - servers: 4
      ## custom pool name
      name: pool-0
      ## volumesPerServer specifies the number of volumes attached per MinIO Tenant Pod / Server.
      volumesPerServer: 2
      ## This VolumeClaimTemplate is used across all the volumes provisioned for MinIO Tenant in this Pool.
      volumeClaimTemplate:
        metadata:
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
```



# Prometheus

[创建ServiceMonitor](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)

[prometheus-operator guide](https://prometheus-operator.dev/docs/user-guides/getting-started/)

[prometheus官方expoter](https://prometheus.io/docs/instrumenting/exporters/)

## Ingress-nginx

https://kubernetes.github.io/ingress-nginx/user-guide/monitoring/

修改nginx-ingress-controller

```yaml
containers:
      - args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        - --election-id=ingress-controller-leader
        - --ingress-class=nginx
        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
        - --annotations-prefix=nginx.ingress.kubernetes.io
        - --enable-metrics=true # add

annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"

ports:
  - containerPort: 10254
  	name: prometheus
  	protocol: TCP
```

建立Service

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "10254"
    prometheus.io/scrape: "true"
  labels:
    app: ingress-nginx
    k8s-app: ingress-nginx
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  ports:
  - name: prometheus
    port: 10254
    protocol: TCP
    targetPort: 10254
  selector:
    xxxx:xxx
  type: NodePort
status:
  loadBalancer: {}

```

检验

```sh
curl http://127.0.0.1:nodeport/metrics

# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 1.1714e-05
go_gc_duration_seconds{quantile="0.25"} 2.5922e-05
go_gc_duration_seconds{quantile="0.5"} 4.1336e-05
go_gc_duration_seconds{quantile="0.75"} 6.3554e-05
go_gc_duration_seconds{quantile="1"} 0.007973384
go_gc_duration_seconds_sum 0.137099061
go_gc_duration_seconds_count 1951
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 95
```

建立ServiceMonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-apps: ingress-nginx
  name: xxx
  namespace: xxx
spec:
  endpoints:
  - interval: 15s
    port: prometheus
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - ingress-nginx
  selector:
    matchLabels:
      k8s-app: ingress-nginx

```

能数据收集一段时间

![image-20221212100822636](https://cdn.jsdelivr.net/gh/631068264/img/008vxvgGgy1h90sxcou9dj31540sy797.jpg)

### 增强exporter

https://github.com/martin-helmich/prometheus-nginxlog-exporter,依赖access.log实现status统计

通过nginx-ingress-controller，sidecar部署

```yaml

       volumeMounts:
       ....
        - mountPath: /var/log/nginx
          name: shared-data


- args:
	- mnt/nginxlogs/access.log
  image: nginx/prometheus-nginxlog-exporter:v1.10.0
  imagePullPolicy: IfNotPresent
  name: exporter
  ports:
  - containerPort: 4040
  	name: exporter2
  	protocol: TCP
  volumeMounts:
  - mountPath: /mnt/nginxlogs
  	name: shared-data
  
	volumes:
  - emptyDir: {}
  	name: shared-data
```

配置**configmap**:ingress-nginx-controller  **ns**:ingress-nginx  [参考](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/)

```yaml
log-format-upstream: $remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"
```







### lua exporter

参考：https://github.com/knyar/nginx-lua-prometheus

主要使用ingress-nginx的[自定义**nginx.conf**模板](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/custom-template/),因为使用lua exporter需要对nginx.conf进行修改。

修改ingress-nginx-controller

```yaml
	   ports:
	   - containerPort: 9145
     	 name: lua
       protocol: TCP
          
        volumeMounts:
        - mountPath: /etc/nginx/template
          name: nginx-template
        - mountPath: /etc/nginx/nginx-lua-prometheus
          name: nginx-lua-prometheus
          
      volumes:
      - configMap:
          name: nginx-template # 修改过后的模板
        name: nginx-template   
      - configMap:
          name: nginx-lua-prometheus # lua依赖
        name: nginx-lua-prometheus
```

获取模板

```sh
docker cp container-id:/etc/nginx/template/nginx.tmpl .
```

修改模板  在http部分，可以根据修改内容，但是修改的位置大概就是这些

```nginx
http {
		lua_shared_dict prometheus_metrics 10M;  # add
    #update
    lua_package_path "/etc/nginx/lua/?.lua;/etc/nginx/nginx-lua-prometheus/?.lua;;"; 
   
    # add
  	log_by_lua_block {
      metric_requests:inc(1, {ngx.var.server_name, ngx.var.status})
      metric_latency:observe(tonumber(ngx.var.request_time), {ngx.var.server_name})
    }
  
  
      init_worker_by_lua_block {
        lua_ingress.init_worker()
        balancer.init_worker()
        {{ if $all.EnableMetrics }}
        monitor.init_worker({{ $all.MonitorMaxBatchSize }})
        {{ end }}

        plugins.run()
				
        # add
        prometheus = require("prometheus").init("prometheus_metrics")

        metric_requests = prometheus:counter(
          "nginx_http_requests_total", "Number of HTTP requests", {"host", "status"})
        metric_latency = prometheus:histogram(
          "nginx_http_request_duration_seconds", "HTTP request latency", {"host"})
        metric_connections = prometheus:gauge(
          "nginx_http_connections", "Number of HTTP connections", {"state"})
    }
  
  
  ....
    # add
  	server {
	  listen 9145;
	  location /metrics {
		content_by_lua_block {
		  metric_connections:set(ngx.var.connections_reading, {"reading"})
		  metric_connections:set(ngx.var.connections_waiting, {"waiting"})
		  metric_connections:set(ngx.var.connections_writing, {"writing"})
		  prometheus:collect()
		}
	  }
	}
  
}
```

模板修改完后放入到configmap，同时

![image-20221221121955494](https://cdn.jsdelivr.net/gh/631068264/img/008vxvgGgy1h9bbayc53fj30qs068mxn.jpg)

lua依赖也放到configmap。









## es

https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-elasticsearch-exporter

```sh
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm fetch prometheus-community/prometheus-elasticsearch-exporter

helm install esexporter prometheus-elasticsearch-exporter-5.0.0.tgz -f values.yaml

```



```
quay.io/prometheuscommunity/elasticsearch-exporter:v1.5.0
```



values.yaml

```yaml
image:
  repository: prometheuscommunity/elasticsearch-exporter
  
es:
  uri: http://quickstart-es-http:9200
```

ServiceMonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: elasticsearch-exporter
  namespace: default
spec:
  endpoints:
  - interval: 15s
    port: http
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      app: prometheus-elasticsearch-exporter

```

## kafka

[metrics 例子](https://github.com/strimzi/strimzi-kafka-operator/blob/main/examples/metrics/)

[官网说明](https://strimzi.io/docs/operators/latest/deploying.html#assembly-metrics-str)



基于jmxPrometheusExporter

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    version: 3.3.1
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2
      inter.broker.protocol.version: "3.3"
    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 100Gi
        deleteClaim: false
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: kafka-metrics-config.yml
  zookeeper:
    replicas: 3
    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    storage:
      type: persistent-claim
      size: 100Gi
      deleteClaim: false
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: zookeeper-metrics-config.yml
  entityOperator:
    topicOperator: {}
    userOperator: {}
  kafkaExporter:
    topicRegex: ".*"
    groupRegex: ".*"
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kafka-metrics
  labels:
    app: strimzi
data:
  kafka-metrics-config.yml: |
    # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics
    lowercaseOutputName: true
    rules:
    # Special cases and very specific rules
    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
      name: kafka_server_$1_$2
      type: GAUGE
      labels:
       clientId: "$3"
       topic: "$4"
       partition: "$5"
    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>Value
      name: kafka_server_$1_$2
      type: GAUGE
      labels:
       clientId: "$3"
       broker: "$4:$5"
    - pattern: kafka.server<type=(.+), cipher=(.+), protocol=(.+), listener=(.+), networkProcessor=(.+)><>connections
      name: kafka_server_$1_connections_tls_info
      type: GAUGE
      labels:
        cipher: "$2"
        protocol: "$3"
        listener: "$4"
        networkProcessor: "$5"
    - pattern: kafka.server<type=(.+), clientSoftwareName=(.+), clientSoftwareVersion=(.+), listener=(.+), networkProcessor=(.+)><>connections
      name: kafka_server_$1_connections_software
      type: GAUGE
      labels:
        clientSoftwareName: "$2"
        clientSoftwareVersion: "$3"
        listener: "$4"
        networkProcessor: "$5"
    - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+):"
      name: kafka_server_$1_$4
      type: GAUGE
      labels:
       listener: "$2"
       networkProcessor: "$3"
    - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+)
      name: kafka_server_$1_$4
      type: GAUGE
      labels:
       listener: "$2"
       networkProcessor: "$3"
    # Some percent metrics use MeanRate attribute
    # Ex) kafka.server<type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)><>MeanRate
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>MeanRate
      name: kafka_$1_$2_$3_percent
      type: GAUGE
    # Generic gauges for percents
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>Value
      name: kafka_$1_$2_$3_percent
      type: GAUGE
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*, (.+)=(.+)><>Value
      name: kafka_$1_$2_$3_percent
      type: GAUGE
      labels:
        "$4": "$5"
    # Generic per-second counters with 0-2 key/value pairs
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
    # Generic gauges with 0-2 key/value pairs
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
    # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's.
    # Note that these are missing the '_sum' metric!
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        "$6": "$7"
        quantile: "0.$8"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        quantile: "0.$6"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        quantile: "0.$4"
  zookeeper-metrics-config.yml: |
    # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics
    lowercaseOutputName: true
    rules:
    # replicated Zookeeper
    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+)><>(\\w+)"
      name: "zookeeper_$2"
      type: GAUGE
    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+)><>(\\w+)"
      name: "zookeeper_$3"
      type: GAUGE
      labels:
        replicaId: "$2"
    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+)><>(Packets\\w+)"
      name: "zookeeper_$4"
      type: COUNTER
      labels:
        replicaId: "$2"
        memberType: "$3"
    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+)><>(\\w+)"
      name: "zookeeper_$4"
      type: GAUGE
      labels:
        replicaId: "$2"
        memberType: "$3"
    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+), name3=(\\w+)><>(\\w+)"
      name: "zookeeper_$4_$5"
      type: GAUGE
      labels:
        replicaId: "$2"
        memberType: "$3"
```

PodMonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: kafka-exporter
  namespace: kafka
spec:
  selector:
    matchLabels:
       strimzi.io/cluster: my-cluster
  podMetricsEndpoints:
  - port: tcp-prometheus
```







