---
layout:     post
rewards: false
title:   karmada 安装
categories:
    - k8s
tags:
    - karmada
---

# 安装karmada

## helm chart安装

helm https://github.com/karmada-io/karmada/tree/release-1.2/charts

```
bitnami/kubectl:latest
cfssl/cfssl
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/kube-apiserver:v1.21.7
k8s.gcr.io/kube-controller-manager:v1.21.7
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-aggregated-apiserver:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-controller-manager:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-webhook:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-search:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-descheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler-estimator:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-agent:latest
```

karmada.yaml 修改了镜像地址

```yaml
etcd:
  internal:
    image:
      repository: karmada/etcd
kubeControllerManager:
  image:
    repository: karmada/kube-controller-manager
apiServer:
  image:
    repository: karmada/kube-apiserver


search:
  image:
    repository: karmada/karmada-search
descheduler:
  image:
    repository: karmada/karmada-descheduler
schedulerEstimator:
  image:
    repository: karmada/karmada-scheduler-estimator
agent:
  image:
    repository: karmada/karmada-agent
aggregatedApiServer:
  image:
    repository: karmada/karmada-aggregated-apiserver
controllerManager:
  image:
    repository: karmada/karmada-controller-manager
webhook:
  image:
    repository: karmada/karmada-webhook
scheduler:
  image:
    repository: karmada/karmada-scheduler
    
certs:
  auto:
    hosts: [
      "kubernetes.default.svc",
      "*.etcd.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc",
      "localhost",
      "127.0.0.1",
      "host-server-ip"  # 修改成host集群IP
    ]
```

[关于helm chart安装证书问题  pull agent连不上host集群](https://github.com/karmada-io/karmada/issues/1880)



安装

```sh
helm install karmada -n karmada-system --create-namespace karmada-0.0.3.tgz -f karmada.yaml
```



```
NAME                                              READY   STATUS    RESTARTS        AGE
etcd-0                                            1/1     Running   0               4m49s
karmada-aggregated-apiserver-79b7b5654c-8rw8j     1/1     Running   2 (4m15s ago)   4m49s
karmada-apiserver-9d86c4949-b488p                 1/1     Running   0               4m49s
karmada-controller-manager-778c7b7b56-6qrn2       1/1     Running   2 (4m45s ago)   4m49s
karmada-kube-controller-manager-6dd5bdc55-7z5h9   1/1     Running   2 (4m14s ago)   4m49s
karmada-scheduler-d6b87bcf9-h74bn                 1/1     Running   0               4m49s
karmada-webhook-68dd9586c9-ntb9r                  1/1     Running   2 (4m44s ago)   4m49s
```



卸载

```sh
helm uninstall karmada -n karmada-system

kubectl delete sa/karmada-pre-job -n karmada-system
kubectl delete clusterRole/karmada-pre-job 
kubectl delete clusterRoleBinding/karmada-pre-job
kubectl delete ns karmada-system
```







## kubectl-karmada 安装

https://github.com/karmada-io/karmada/releases

```sh
tar -zxf kubectl-karmada-linux-amd64.tgz

which kubectl
cp kubectl-karmada /usr/local/bin

```

move kubectl-karmada executable file to PATH path

```sh
kubectl karmada version

kubectl karmada version: version.Info{GitVersion:"v1.2.1", GitCommit:"de4972b74f848f78a58f9a0f4a4e85f243ba48f8", GitTreeState:"clean", BuildDate:"2022-07-14T09:33:32Z", GoVersion:"go1.17.11", Compiler:"gc", Platform:"linux/amd64"}
```



## install agent

**host 集群上**

```sh
# 获取karmada config
kubectl get secret -n karmada-system karmada-kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d > karmada-config
```

karmada-config 修改server成`https://host-server-ip:5443`

```yaml
apiVersion: v1
kind: Config
clusters:
  - cluster:
      certificate-authority-data: xxxx
      insecure-skip-tls-verify: false
      server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443
    name: karmada-apiserver
users:
  - user:
      client-certificate-data: xxxx
      client-key-data: xxxx
    name: karmada-apiserver
contexts:
  - context:
      cluster: karmada-apiserver
      user: karmada-apiserver
    name: karmada-apiserver
current-context: karmada-apiserver
```

**worker集群上**

agent.yaml 根据karmada-config填写证书信息需要**base64 decode**，根据不同的worker集群修改**clusterName**

```yaml
installMode: "agent"
agent:
  image:
    repository: karmada/karmada-agent
  clusterName: "member"
  kubeconfig:
    caCrt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    crt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END RSA PRIVATE KEY-----
    server: "https://host-server-ip:5443"

```

安装

```sh
# 安装
helm install karmada-agent -n karmada-system --create-namespace  karmada-0.0.3.tgz -f agent.yaml

# 卸载
helm uninstall karmada-agent -n karmada-system
```



测试

worker集群

```sh
kubectl get pods -n karmada-system

NAME                             READY   STATUS    RESTARTS   AGE
karmada-agent-86864d7d8b-kfmk9   1/1     Running   0          20s
```

host集群

```sh
kubectl get cluster --kubeconfig karmada-config

NAME      VERSION        MODE   READY   AGE
member1   v1.19.5+k3s2   Pull   True    4m39s
```



## 安装ANP

anp安装参考 https://github.com/karmada-io/karmada/blob/master/docs/working-with-anp.md

不然会出现错误，参考[这个](https://github.com/karmada-io/karmada/issues/2291)

```sh
karmadactl get pod --kubeconfig karmada-config

Error: [cluster(member1) is inaccessible, please check authorization or network, cluster(member2) is inaccessible, please check authorization or network]
```

安装ANP为了，让以pull模式加入member集群和Karmada control plane的网络互通，这样才能让karmada-aggregated-apiserver就可以访问到member集群，方便用户通过karmada来访问成员集群。



### 构建proxy镜像和证书

```sh
# 下载代码
git clone -b v0.0.24/dev https://github.com/mrlihanbo/apiserver-network-proxy.git
cd apiserver-network-proxy/

# 构建，然后push
docker build . --build-arg ARCH=amd64 -f artifacts/images/agent-build.Dockerfile -t karmada/proxy-agent:0.0.24
docker build . --build-arg ARCH=amd64 -f artifacts/images/server-build.Dockerfile -t karmada/proxy-server:0.0.24


# 创建证书，在certs目录，把IP放到证书的SAN里面
make certs PROXY_SERVER_IP=x.x.x.x
```

### 部署proxy

#### server

proxy-server.yaml 放到项目根目录

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy-server
  namespace: karmada-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: proxy-server
  template:
    metadata:
      labels:
        app: proxy-server
    spec:
      containers:
      - command:
        - /proxy-server
        args:
          - --health-port=8092
          - --cluster-ca-cert=/var/certs/server/cluster-ca-cert.crt
          - --cluster-cert=/var/certs/server/cluster-cert.crt 
          - --cluster-key=/var/certs/server/cluster-key.key
          - --mode=http-connect 
          - --proxy-strategies=destHost 
          - --server-ca-cert=/var/certs/server/server-ca-cert.crt
          - --server-cert=/var/certs/server/server-cert.crt 
          - --server-key=/var/certs/server/server-key.key
        image: karmada/proxy-server:0.0.24
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8092
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 60
        name: proxy-server
        volumeMounts:
        - mountPath: /var/certs/server
          name: cert
      restartPolicy: Always
      hostNetwork: true
      volumes:
      - name: cert
        secret:
          secretName: proxy-server-cert
---
apiVersion: v1
kind: Secret
metadata:
  name: proxy-server-cert
  namespace: karmada-system
type: Opaque
data:
  server-ca-cert.crt: |
    {{server_ca_cert}}
  server-cert.crt: |
    {{server_cert}}
  server-key.key: |
    {{server_key}}
  cluster-ca-cert.crt: |
    {{cluster_ca_cert}}
  cluster-cert.crt: |
    {{cluster_cert}}
  cluster-key.key: |
    {{cluster_key}}
```

replace-proxy-server.sh  证书替换脚本

```sh
#!/bin/bash

cert_yaml=proxy-server.yaml

SERVER_CA_CERT=$(cat certs/frontend/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_ca_cert}}/${SERVER_CA_CERT}/g" ${cert_yaml}

SERVER_CERT=$(cat certs/frontend/issued/proxy-frontend.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_cert}}/${SERVER_CERT}/g" ${cert_yaml}

SERVER_KEY=$(cat certs/frontend/private/proxy-frontend.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_key}}/${SERVER_KEY}/g" ${cert_yaml}

CLUSTER_CA_CERT=$(cat certs/agent/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_ca_cert}}/${CLUSTER_CA_CERT}/g" ${cert_yaml}

CLUSTER_CERT=$(cat certs/agent/issued/proxy-frontend.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_cert}}/${CLUSTER_CERT}/g" ${cert_yaml}


CLUSTER_KEY=$(cat certs/agent/private/proxy-frontend.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_key}}/${CLUSTER_KEY}/g" ${cert_yaml}
```

部署到host集群

```sh
kubectl apply -f proxy-server.yaml
```

#### agent

proxy-agent.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: proxy-agent
  name: proxy-agent
  namespace: karmada-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: proxy-agent
  template:
    metadata:
      labels:
        app: proxy-agent
    spec:
      containers:
        - command:
            - /proxy-agent
          args:
            - '--ca-cert=/var/certs/agent/ca.crt'
            - '--agent-cert=/var/certs/agent/proxy-agent.crt'
            - '--agent-key=/var/certs/agent/proxy-agent.key'
            - '--proxy-server-host={{proxy_server_addr}}'
            - '--proxy-server-port=8091'
            - '--agent-identifiers=host={{identifiers}}'
          image: karmada/proxy-agent:0.0.24
          imagePullPolicy: IfNotPresent
          name: proxy-agent
          livenessProbe:
            httpGet:
              scheme: HTTP
              port: 8093
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 60
          volumeMounts:
            - mountPath: /var/certs/agent
              name: cert
      volumes:
        - name: cert
          secret:
            secretName: proxy-agent-cert
---
apiVersion: v1
kind: Secret
metadata:
  name: proxy-agent-cert
  namespace: karmada-system
type: Opaque
data:
  ca.crt: |
    {{proxy_agent_ca_crt}}
  proxy-agent.crt: |
    {{proxy_agent_crt}}
  proxy-agent.key: |
    {{proxy_agent_key}}
```

replace-proxy-agent.sh

```sh
#!/bin/bash

cert_yaml=proxy-agent.yaml

karmada_controlplan_addr=$1
member3_cluster_addr=$2
sed -i'' -e "s/{{proxy_server_addr}}/${karmada_controlplan_addr}/g" ${cert_yaml}
sed -i'' -e "s/{{identifiers}}/${member3_cluster_addr}/g" ${cert_yaml}

PROXY_AGENT_CA_CRT=$(cat certs/agent/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_ca_crt}}/${PROXY_AGENT_CA_CRT}/g" ${cert_yaml}

PROXY_AGENT_CRT=$(cat certs/agent/issued/proxy-agent.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_crt}}/${PROXY_AGENT_CRT}/g" ${cert_yaml}

PROXY_AGENT_KEY=$(cat certs/agent/private/proxy-agent.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_key}}/${PROXY_AGENT_KEY}/g" ${cert_yaml}
```

运行脚本

```sh
chmod +x replace-proxy-agent.sh
bash replace-proxy-agent.sh proxy_ip member_cluster_ip
```

修改karmada-agent deployment

```sh
kubectl edit deploy karmada-agent -n karmada-system
```

添加

- --cluster-api-endpoint    k8s集群的apiendpoint，可以参考集群的kubeconfig
- --proxy-server-address  http://<karmada_controlplan_addr>:8088 , 代理服务访问到proxy sever

```yaml
......

      containers:
      - command:
        - /bin/karmada-agent
        - --karmada-kubeconfig=/etc/kubeconfig/kubeconfig
        - --cluster-name=member1
        - --cluster-api-endpoint=https://<member ip>:6443  # add newline
        - --proxy-server-address=http://<proxy server ip>:8088  # add newline
        - --cluster-status-update-frequency=10s
        - --v=4
......
```

8088可以通过源码修改

https://github.com/mrlihanbo/apiserver-network-proxy/blob/v0.0.24/dev/cmd/server/app/server.go#L267.





# Submariner

Karmada使用 Submariner 连接成员集群之间的网络，[Submariner](https://github.com/submariner-io/submariner)将连接的集群之间的网络扁平化，并实现 Pod 和服务之间的 IP 可达性，与网络插件 (CNI) 无关。

**member集群之间的Pod CIDR 和 Service CIDR必须不一样**



## helm安装

### 安装准备

**准备charts**

```sh
helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts

helm repo update

# list charts version
helm search repo submariner-latest/submariner-k8s-broker --versions

helm search repo submariner-latest/submariner-k8s-broker --versions
helm search repo submariner-latest/submariner-operator --versions

# 下载指定version chart
helm fetch submariner-latest/submariner-k8s-broker --version xxxx
helm fetch submariner-latest/submariner-operator
```

**准备镜像**

[目前还没官方方法支持离线安装](https://github.com/submariner-io/submariner/issues/1790)，准备需要到镜像

```sh
#SUBMARINER_VER=$(subctl version | cut -d: -f2 | cut -dv -f2)
SUBMARINER_VER=0.12.2
repo=xxxx

for i in submariner-operator submariner-route-agent submariner-globalnet submariner-gateway submariner-networkplugin-syncer submariner-operator-index lighthouse-coredns lighthouse-agent nettest
do
  docker pull quay.io/submariner/${i}:${SUBMARINER_VER}
  docker tag quay.io/submariner/${i}:${SUBMARINER_VER} ${repo}/submariner/${i}:${SUBMARINER_VER}
  docker push ${repo}/submariner/${i}:${SUBMARINER_VER}
done

```

**安装subctl**

https://github.com/submariner-io/releases/releases  下载

在host集群

```sh
cp subctl-vxx.xx-linux-amd64 /usr/local/bin/subctl
chmod +x /usr/local/bin/subctl
```

**配置member集群节点**

member集群其中一个节点配置就可以了

```sh
kubectl label nodes xxx submariner.io/gateway=true
kubectl annotate node xxx gateway.submariner.io/public-ip=ipv4:1.2.3.4
```

参考

- https://github.com/submariner-io/submariner/issues/1926#issuecomment-1188216877
- https://submariner.io/operations/nat-traversal/
- https://github.com/submariner-io/submariner/issues/1649



[不配label gateway](https://github.com/submariner-io/submariner/issues/1926#issuecomment-1188216877)

```
subctl diagnose all

✗ Checking gateway connections
✗ There are no gateways detected
 
 
✗ Checking Submariner support for the kube-proxy mode 
✗ Error spawning the network pod: timed out waiting for the condition
```

没有gateway这个pod

[不配pubilc ip  gateway会报错](https://github.com/submariner-io/submariner/issues/1649#issuecomment-1006322078)

```sh
E0106 06:33:10.090003 1 public_ip.go:80] Error resolving public IP with resolver api:api.ipify.org: retrieving public IP from https://api.ipify.org: Get "https://api.ipify.org": dial tcp 54.91.59.199:443: i/o timeout.
E0106 06:33:40.090557 1 public_ip.go:80] Error resolving public IP with resolver api:api.my-ip.io/ip: retrieving public IP from https://api.my-ip.io/ip: Get "https://api.my-ip.io/ip": dial tcp 161.35.189.70:443: i/o timeout
E0106 06:34:10.090773 1 public_ip.go:80] Error resolving public IP with resolver api:ip4.seeip.org: retrieving public IP from https://ip4.seeip.org: Get "https://ip4.seeip.org": dial tcp 23.128.64.141:443: i/o timeout
F0106 06:34:10.090872 1 main.go:134] Error creating local endpoint object from types.SubmarinerSpecification{ClusterCidr:[]string{"10.244.0.0/16"}, ColorCodes:[]string{"blue"}, GlobalCidr:[]string{}, ServiceCidr:[]string{"10.10.0.0/16"}, Broker:"k8s", CableDriver:"libreswan", ClusterID:"cluster-b", Namespace:"submariner-operator", PublicIP:"", Token:"", Debug:false, NATEnabled:false, HealthCheckEnabled:true, HealthCheckInterval:0x1, HealthCheckMaxPacketLossCount:0x5}: could not determine public IP: Unable to resolve public IP by any of the resolver methods: [api:api.ipify.org api:api.my-ip.io/ip api:ip4.seeip.org]
```





### host 集群

```sh
# 安装
helm install submariner-broker submariner-k8s-broker-0.12.2.tgz --create-namespace -n submariner-broker

NAME: submariner-broker
LAST DEPLOYED: Tue Aug 16 10:13:32 2022
NAMESPACE: submariner-broker
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Submariner Kubernetes Broker is now setup.

You can retrieve the server URL by running

  $ SUBMARINER_BROKER_URL=$(kubectl -n default get endpoints kubernetes -o jsonpath="{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name=='https')].port}")

The broker client token and CA can be retrieved by running

  $ SUBMARINER_BROKER_CA=$(kubectl -n submariner-broker get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='submariner-broker-submariner-k8s-broker-client')].data['ca\.crt']}")
  $ SUBMARINER_BROKER_TOKEN=$(kubectl -n submariner-broker get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='submariner-broker-submariner-k8s-broker-client')].data.token}"|base64 --decode)
```

### member集群 安装 operator

```sh
SUBMARINER_PSK=$(LC_CTYPE=C tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 64 | head -n 1)

6jo7djPJww0KoFm0ESuTzqRZSrUmi4oCnNiykmk0p6yDEQA4agEpZTeqfT3wmib2
```

values.yaml  clusterId,clusterCidr,serviceCidr根据member集群实际情况填写，k3s可以`cat /etc/systemd/system/k3s.service`

```yaml
---
submariner:
  clusterId: "member1"
  clusterCidr: ""
  serviceCidr: ""
  images:
    repository: harbor.xxx.cn:20000/submariner
    tag: "0.12.2"
broker:
  server: "${SUBMARINER_BROKER_URL}"
  token: "${SUBMARINER_BROKER_TOKEN}"
  namespace: "submariner-broker"
  ca: "${SUBMARINER_BROKER_CA}"
ipsec:
  psk: "${SUBMARINER_PSK}"
operator:
  image:
    repository: harbor.xxx.cn:20000/submariner/submariner-operator
    tag: "0.12.2"
gateway:
  image:
    repository: harbor.xxx.cn:20000/submariner/submariner-gateway
    tag: "0.12.2"


```



```sh
# 安装
helm install submariner-operator submariner-operator-0.12.2.tgz --create-namespace -n submariner-operator -f values.yaml

# 验证
kubectl get pods -n submariner-operator

NAME                                            READY   STATUS    RESTARTS   AGE
submariner-gateway-zvps6                        1/1     Running   0          17h
submariner-lighthouse-agent-6dd4bc9b4b-ft6cq    1/1     Running   0          22h
submariner-lighthouse-coredns-8bf8fccdb-92kpd   1/1     Running   0          22h
submariner-lighthouse-coredns-8bf8fccdb-gxpxs   1/1     Running   0          22h
submariner-operator-d4cd68cb9-4qdv5             1/1     Running   1          22h
submariner-routeagent-8mwrh                     1/1     Running   0          22h
```

### 验证

可以把member集群的kubeconfig聚集到host集群，修改

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: xxxx
    server: https://xxx:6443  # 修改对应IP
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: member1  # 修改 member name
current-context: member1  # 修改 member name
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: xxxx
    client-key-data: xxx
```

检查安装配置排查原因

```sh
export KUBECONFIG=member1-config  # 可以省下--kubeconfig 


subctl show all --kubeconfig ${member_KUBE_CONFIG}
subctl diagnose all --kubeconfig ${member_KUBE_CONFIG}
```

验证正常member集群A, B直接互相访问

**Deploy ClusterIP Service**

```sh
kubectl --kubeconfig kubeconfig.cluster-b create deployment nginx --image=nginx
kubectl --kubeconfig kubeconfig.cluster-b expose deployment nginx --port=80
subctl export service --kubeconfig kubeconfig.cluster-b --namespace default nginx
```

**Verify**

```sh
kubectl --kubeconfig kubeconfig.cluster-a -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest:0.12.2 -- /bin/bash
curl nginx.default.svc.clusterset.local

```


