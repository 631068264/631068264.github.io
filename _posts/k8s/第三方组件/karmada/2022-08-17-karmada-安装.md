---
layout:     post
rewards: false
title:   karmada 安装
categories:
    - k8s
tags:
    - karmada
---

# 安装karmada



## helm chart安装

helm https://github.com/karmada-io/karmada/tree/release-1.2/charts

```
bitnami/kubectl:latest
cfssl/cfssl
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/kube-apiserver:v1.21.7
k8s.gcr.io/kube-controller-manager:v1.21.7
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-aggregated-apiserver:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-controller-manager:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-webhook:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-search:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-descheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler-estimator:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-agent:latest
```

karmada.yaml 修改了镜像地址

```yaml
etcd:
  internal:
    image:
      repository: karmada/etcd
kubeControllerManager:
  image:
    repository: karmada/kube-controller-manager
apiServer:
  image:
    repository: karmada/kube-apiserver


search:
  image:
    repository: karmada/karmada-search
descheduler:
  image:
    repository: karmada/karmada-descheduler
schedulerEstimator:
  image:
    repository: karmada/karmada-scheduler-estimator
agent:
  image:
    repository: karmada/karmada-agent
aggregatedApiServer:
  image:
    repository: karmada/karmada-aggregated-apiserver
controllerManager:
  image:
    repository: karmada/karmada-controller-manager
webhook:
  image:
    repository: karmada/karmada-webhook
scheduler:
  image:
    repository: karmada/karmada-scheduler
    
certs:
  auto:
    hosts: [
      "kubernetes.default.svc",
      "*.etcd.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc",
      "localhost",
      "127.0.0.1",
      "host-server-ip"  # 修改成host集群IP，不然agent因为证书问题连不上host集群
    ]
```

[关于helm chart安装证书问题  pull agent连不上host集群](https://github.com/karmada-io/karmada/issues/1880)



安装

```sh
helm install karmada -n karmada-system --create-namespace karmada-0.0.3.tgz -f karmada.yaml
```



```
kubectl get pods -n karmada-system

NAME                                              READY   STATUS    RESTARTS        AGE
etcd-0                                            1/1     Running   0               4m49s
karmada-aggregated-apiserver-79b7b5654c-8rw8j     1/1     Running   2 (4m15s ago)   4m49s
karmada-apiserver-9d86c4949-b488p                 1/1     Running   0               4m49s
karmada-controller-manager-778c7b7b56-6qrn2       1/1     Running   2 (4m45s ago)   4m49s
karmada-kube-controller-manager-6dd5bdc55-7z5h9   1/1     Running   2 (4m14s ago)   4m49s
karmada-scheduler-d6b87bcf9-h74bn                 1/1     Running   0               4m49s
karmada-webhook-68dd9586c9-ntb9r                  1/1     Running   2 (4m44s ago)   4m49s
```



卸载

```sh
helm uninstall karmada -n karmada-system

kubectl delete sa/karmada-pre-job -n karmada-system
kubectl delete clusterRole/karmada-pre-job
kubectl delete clusterRoleBinding/karmada-pre-job
kubectl delete ns karmada-system
```







## kubectl-karmada 安装

https://github.com/karmada-io/karmada/releases

```sh
tar -zxf kubectl-karmada-linux-amd64.tgz

which kubectl
cp kubectl-karmada /usr/local/bin

```

move kubectl-karmada executable file to PATH path

```sh
kubectl karmada version

kubectl karmada version: version.Info{GitVersion:"v1.2.1", GitCommit:"de4972b74f848f78a58f9a0f4a4e85f243ba48f8", GitTreeState:"clean", BuildDate:"2022-07-14T09:33:32Z", GoVersion:"go1.17.11", Compiler:"gc", Platform:"linux/amd64"}
```



## install agent

**host 集群上**

```sh
# 获取karmada config
kubectl get secret -n karmada-system karmada-kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d > karmada-apiserver.config
```

karmada-apiserver.config 修改server成`https://host-server-ip:5443`

```yaml
apiVersion: v1
kind: Config
clusters:
  - cluster:
      certificate-authority-data: xxxx
      insecure-skip-tls-verify: false
      server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443
    name: karmada-apiserver
users:
  - user:
      client-certificate-data: xxxx
      client-key-data: xxxx
    name: karmada-apiserver
contexts:
  - context:
      cluster: karmada-apiserver
      user: karmada-apiserver
    name: karmada-apiserver
current-context: karmada-apiserver
```

**agent集群上**

agent.yaml 根据karmada-apiserver.config填写证书信息需要**base64 decode**，根据不同的worker集群修改**clusterName**

```yaml
installMode: "agent"
agent:
  image:
    repository: karmada/karmada-agent
  clusterName: "member"
  kubeconfig:
    caCrt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    crt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END RSA PRIVATE KEY-----
    server: "https://host-server-ip:5443"

```

安装

```sh
# 安装
helm install karmada-agent -n karmada-system --create-namespace  karmada-0.0.3.tgz -f agent.yaml

# 卸载
helm uninstall karmada-agent -n karmada-system
```



测试

worker集群

```sh
kubectl get pods -n karmada-system

NAME                             READY   STATUS    RESTARTS   AGE
karmada-agent-86864d7d8b-kfmk9   1/1     Running   0          20s
```

host集群

```sh
kubectl get cluster --kubeconfig karmada-apiserver.config

NAME      VERSION        MODE   READY   AGE
member1   v1.19.5+k3s2   Pull   True    4m39s
```



## 安装ANP

anp安装参考 https://karmada.io/docs/userguide/clustermanager/working-with-anp

不然会出现错误，参考[这个](https://github.com/karmada-io/karmada/issues/2291)

```sh
karmadactl get pod --kubeconfig karmada-apiserver.config

Error: [cluster(member1) is inaccessible, please check authorization or network, cluster(member2) is inaccessible, please check authorization or network]
```

安装ANP为了，让以pull模式加入member集群和Karmada control plane的网络互通，这样才能让karmada-aggregated-apiserver就可以访问到member集群，方便用户通过karmada来访问成员集群。



### 构建proxy镜像和证书

```sh
# 下载代码
git clone -b v0.0.24/dev https://github.com/mrlihanbo/apiserver-network-proxy.git
cd apiserver-network-proxy/

# 构建，然后push
docker build . --build-arg ARCH=amd64 -f artifacts/images/agent-build.Dockerfile -t karmada/proxy-agent:0.0.24
docker build . --build-arg ARCH=amd64 -f artifacts/images/server-build.Dockerfile -t karmada/proxy-server:0.0.24


# 创建证书，在certs目录，把IP放到证书的SAN里面
make certs PROXY_SERVER_IP=x.x.x.x
```

### 部署proxy

#### server

proxy-server.yaml 放到项目根目录

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy-server
  namespace: karmada-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: proxy-server
  template:
    metadata:
      labels:
        app: proxy-server
    spec:
      containers:
      - command:
        - /proxy-server
        args:
          - --health-port=8092
          - --cluster-ca-cert=/var/certs/server/cluster-ca-cert.crt
          - --cluster-cert=/var/certs/server/cluster-cert.crt 
          - --cluster-key=/var/certs/server/cluster-key.key
          - --mode=http-connect 
          - --proxy-strategies=destHost 
          - --server-ca-cert=/var/certs/server/server-ca-cert.crt
          - --server-cert=/var/certs/server/server-cert.crt 
          - --server-key=/var/certs/server/server-key.key
        image: karmada/proxy-server:0.0.24
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8092
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 60
        name: proxy-server
        volumeMounts:
        - mountPath: /var/certs/server
          name: cert
      restartPolicy: Always
      hostNetwork: true
      volumes:
      - name: cert
        secret:
          secretName: proxy-server-cert
---
apiVersion: v1
kind: Secret
metadata:
  name: proxy-server-cert
  namespace: karmada-system
type: Opaque
data:
  server-ca-cert.crt: |
    {{server_ca_cert}}
  server-cert.crt: |
    {{server_cert}}
  server-key.key: |
    {{server_key}}
  cluster-ca-cert.crt: |
    {{cluster_ca_cert}}
  cluster-cert.crt: |
    {{cluster_cert}}
  cluster-key.key: |
    {{cluster_key}}
```

replace-proxy-server.sh  证书替换脚本

```sh
#!/bin/bash

cert_yaml=proxy-server.yaml

SERVER_CA_CERT=$(cat certs/frontend/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_ca_cert}}/${SERVER_CA_CERT}/g" ${cert_yaml}

SERVER_CERT=$(cat certs/frontend/issued/proxy-frontend.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_cert}}/${SERVER_CERT}/g" ${cert_yaml}

SERVER_KEY=$(cat certs/frontend/private/proxy-frontend.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{server_key}}/${SERVER_KEY}/g" ${cert_yaml}

CLUSTER_CA_CERT=$(cat certs/agent/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_ca_cert}}/${CLUSTER_CA_CERT}/g" ${cert_yaml}

CLUSTER_CERT=$(cat certs/agent/issued/proxy-frontend.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_cert}}/${CLUSTER_CERT}/g" ${cert_yaml}


CLUSTER_KEY=$(cat certs/agent/private/proxy-frontend.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{cluster_key}}/${CLUSTER_KEY}/g" ${cert_yaml}
```

部署到host集群

```sh
kubectl apply -f proxy-server.yaml
```

#### agent

proxy-agent.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: proxy-agent
  name: proxy-agent
  namespace: karmada-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: proxy-agent
  template:
    metadata:
      labels:
        app: proxy-agent
    spec:
      containers:
        - command:
            - /proxy-agent
          args:
            - '--ca-cert=/var/certs/agent/ca.crt'
            - '--agent-cert=/var/certs/agent/proxy-agent.crt'
            - '--agent-key=/var/certs/agent/proxy-agent.key'
            - '--proxy-server-host={{proxy_server_addr}}'
            - '--proxy-server-port=8091'
            - '--agent-identifiers=host={{identifiers}}'
          image: karmada/proxy-agent:0.0.24
          imagePullPolicy: IfNotPresent
          name: proxy-agent
          livenessProbe:
            httpGet:
              scheme: HTTP
              port: 8093
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 60
          volumeMounts:
            - mountPath: /var/certs/agent
              name: cert
      volumes:
        - name: cert
          secret:
            secretName: proxy-agent-cert
---
apiVersion: v1
kind: Secret
metadata:
  name: proxy-agent-cert
  namespace: karmada-system
type: Opaque
data:
  ca.crt: |
    {{proxy_agent_ca_crt}}
  proxy-agent.crt: |
    {{proxy_agent_crt}}
  proxy-agent.key: |
    {{proxy_agent_key}}
```

replace-proxy-agent.sh

```sh
#!/bin/bash

cert_yaml=proxy-agent.yaml

karmada_controlplan_addr=$1
member3_cluster_addr=$2
sed -i'' -e "s/{{proxy_server_addr}}/${karmada_controlplan_addr}/g" ${cert_yaml}
sed -i'' -e "s/{{identifiers}}/${member3_cluster_addr}/g" ${cert_yaml}

PROXY_AGENT_CA_CRT=$(cat certs/agent/issued/ca.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_ca_crt}}/${PROXY_AGENT_CA_CRT}/g" ${cert_yaml}

PROXY_AGENT_CRT=$(cat certs/agent/issued/proxy-agent.crt | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_crt}}/${PROXY_AGENT_CRT}/g" ${cert_yaml}

PROXY_AGENT_KEY=$(cat certs/agent/private/proxy-agent.key | base64 | tr "\n" " "|sed s/[[:space:]]//g)
sed -i'' -e "s/{{proxy_agent_key}}/${PROXY_AGENT_KEY}/g" ${cert_yaml}
```

运行脚本

```sh
chmod +x replace-proxy-agent.sh
bash replace-proxy-agent.sh proxy_ip member_cluster_ip
```

修改karmada-agent deployment

```sh
kubectl edit deploy karmada-agent -n karmada-system
```

添加

- --cluster-api-endpoint    k8s集群的apiendpoint，可以参考集群的kubeconfig
- --proxy-server-address  http://<karmada_controlplan_addr>:8088 , 代理服务访问到proxy sever

```yaml
......

      containers:
      - command:
        - /bin/karmada-agent
        - --karmada-kubeconfig=/etc/kubeconfig/kubeconfig
        - --cluster-name=member1
        - --cluster-api-endpoint=https://<member ip>:6443  # add newline
        - --proxy-server-address=http://<proxy server ip>:8088  # add newline
        - --cluster-status-update-frequency=10s
        - --v=4
......
```

8088可以通过源码修改

https://github.com/mrlihanbo/apiserver-network-proxy/blob/v0.0.24/dev/cmd/server/app/server.go#L267.





# Submariner

Karmada使用 Submariner 连接成员集群之间的网络，[Submariner](https://github.com/submariner-io/submariner)将连接的集群之间的网络扁平化，并实现 Pod 和服务之间的 IP 可达性，与网络插件 (CNI) 无关。

**member集群之间的Pod CIDR 和 Service CIDR必须不一样**



## helm安装

参考https://submariner.io/operations/deployment/helm/

### 安装准备

**准备charts**

```sh
helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts

helm repo update

# list charts version
helm search repo submariner-latest/submariner-k8s-broker --versions

helm search repo submariner-latest/submariner-k8s-broker --versions
helm search repo submariner-latest/submariner-operator --versions

# 下载指定version chart
helm fetch submariner-latest/submariner-k8s-broker --version xxxx
helm fetch submariner-latest/submariner-operator
```

**准备镜像**

[目前还没官方方法支持离线安装](https://github.com/submariner-io/submariner/issues/1790)，准备需要到镜像

```sh
#SUBMARINER_VER=$(subctl version | cut -d: -f2 | cut -dv -f2)
SUBMARINER_VER=0.12.2
repo=xxxx

for i in submariner-operator submariner-route-agent submariner-globalnet submariner-gateway submariner-networkplugin-syncer submariner-operator-index lighthouse-coredns lighthouse-agent nettest
do
  docker pull quay.io/submariner/${i}:${SUBMARINER_VER}
  docker tag quay.io/submariner/${i}:${SUBMARINER_VER} ${repo}/submariner/${i}:${SUBMARINER_VER}
  docker push ${repo}/submariner/${i}:${SUBMARINER_VER}
done

```

**安装subctl**

https://github.com/submariner-io/releases/releases  下载

在host集群

```sh
cp subctl-vxx.xx-linux-amd64 /usr/local/bin/subctl
chmod +x /usr/local/bin/subctl
```

**配置member集群节点**

member集群其中一个节点配置就可以了

```sh
kubectl label nodes xxx submariner.io/gateway=true
kubectl annotate node xxx gateway.submariner.io/public-ip=ipv4:1.2.3.4
```

参考

- https://github.com/submariner-io/submariner/issues/1926#issuecomment-1188216877
- https://submariner.io/operations/nat-traversal/
- https://github.com/submariner-io/submariner/issues/1649

### 可能遇到的错误

[不配label gateway](https://github.com/submariner-io/submariner/issues/1926#issuecomment-1188216877)

```
subctl diagnose all

✗ Checking gateway connections
✗ There are no gateways detected
 
 
✗ Checking Submariner support for the kube-proxy mode 
✗ Error spawning the network pod: timed out waiting for the condition
```

没有gateway这个pod

[不配pubilc ip  gateway会报错](https://github.com/submariner-io/submariner/issues/1649#issuecomment-1006322078)

```sh
E0106 06:33:10.090003 1 public_ip.go:80] Error resolving public IP with resolver api:api.ipify.org: retrieving public IP from https://api.ipify.org: Get "https://api.ipify.org": dial tcp 54.91.59.199:443: i/o timeout.
E0106 06:33:40.090557 1 public_ip.go:80] Error resolving public IP with resolver api:api.my-ip.io/ip: retrieving public IP from https://api.my-ip.io/ip: Get "https://api.my-ip.io/ip": dial tcp 161.35.189.70:443: i/o timeout
E0106 06:34:10.090773 1 public_ip.go:80] Error resolving public IP with resolver api:ip4.seeip.org: retrieving public IP from https://ip4.seeip.org: Get "https://ip4.seeip.org": dial tcp 23.128.64.141:443: i/o timeout
F0106 06:34:10.090872 1 main.go:134] Error creating local endpoint object from types.SubmarinerSpecification{ClusterCidr:[]string{"10.244.0.0/16"}, ColorCodes:[]string{"blue"}, GlobalCidr:[]string{}, ServiceCidr:[]string{"10.10.0.0/16"}, Broker:"k8s", CableDriver:"libreswan", ClusterID:"cluster-b", Namespace:"submariner-operator", PublicIP:"", Token:"", Debug:false, NATEnabled:false, HealthCheckEnabled:true, HealthCheckInterval:0x1, HealthCheckMaxPacketLossCount:0x5}: could not determine public IP: Unable to resolve public IP by any of the resolver methods: [api:api.ipify.org api:api.my-ip.io/ip api:ip4.seeip.org]
```





### host 集群

```sh
# 安装
helm install submariner-broker submariner-k8s-broker-0.12.2.tgz --create-namespace -n submariner-broker

NAME: submariner-broker
LAST DEPLOYED: Tue Aug 16 10:13:32 2022
NAMESPACE: submariner-broker
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Submariner Kubernetes Broker is now setup.

You can retrieve the server URL by running

  $ SUBMARINER_BROKER_URL=$(kubectl -n default get endpoints kubernetes -o jsonpath="{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name=='https')].port}")

The broker client token and CA can be retrieved by running

  $ SUBMARINER_BROKER_CA=$(kubectl -n submariner-broker get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='submariner-broker-submariner-k8s-broker-client')].data['ca\.crt']}")
  $ SUBMARINER_BROKER_TOKEN=$(kubectl -n submariner-broker get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='submariner-broker-submariner-k8s-broker-client')].data.token}"|base64 --decode)
```

### member集群 安装 operator

```sh
SUBMARINER_PSK=$(LC_CTYPE=C tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 64 | head -n 1)

6jo7djPJww0KoFm0ESuTzqRZSrUmi4oCnNiykmk0p6yDEQA4agEpZTeqfT3wmib2
```

values.yaml  clusterId,clusterCidr,serviceCidr根据member集群实际情况填写，k3s可以`cat /etc/systemd/system/k3s.service`

```yaml
---
submariner:
  clusterId: "member1"  #必填
  clusterCidr: "" #必填
  serviceCidr: "" #必填
  globalCidr: ""
  images:
    repository: harbor.xxx.cn:20000/submariner
    tag: "0.12.2"
broker:
  server: "${SUBMARINER_BROKER_URL}" # ip:6443
  token: "${SUBMARINER_BROKER_TOKEN}"
  namespace: "submariner-broker"
  ca: "${SUBMARINER_BROKER_CA}"
	globalnet: false  # 开启globalnet
ipsec:
  psk: "${SUBMARINER_PSK}"
operator:
  image:
    repository: harbor.xxx.cn:20000/submariner/submariner-operator
    tag: "0.12.2"
gateway:
  image:
    repository: harbor.xxx.cn:20000/submariner/submariner-gateway
    tag: "0.12.2"


```



```sh
# 安装
helm install submariner-operator submariner-operator-0.12.2.tgz --create-namespace -n submariner-operator -f values.yaml

# 验证
kubectl get pods -n submariner-operator

NAME                                            READY   STATUS    RESTARTS   AGE
submariner-gateway-zvps6                        1/1     Running   0          17h
submariner-lighthouse-agent-6dd4bc9b4b-ft6cq    1/1     Running   0          22h
submariner-lighthouse-coredns-8bf8fccdb-92kpd   1/1     Running   0          22h
submariner-lighthouse-coredns-8bf8fccdb-gxpxs   1/1     Running   0          22h
submariner-operator-d4cd68cb9-4qdv5             1/1     Running   1          22h
submariner-routeagent-8mwrh                     1/1     Running   0          22h


# 使用globalnet的话
kubectl get pods -n submariner-operator
NAME                                            READY   STATUS              RESTARTS     AGE
submariner-gateway-bg6df                        1/1     Running             0            3s
submariner-globalnet-dpfrz                      1/1     Running             0            2s
submariner-lighthouse-agent-5dd6dc54f-89ddb     0/1     ContainerCreating   0            2s
submariner-lighthouse-coredns-8bf8fccdb-8chpt   0/1     ContainerCreating   0            1s
submariner-lighthouse-coredns-8bf8fccdb-gpq4m   0/1     ContainerCreating   0            1s
submariner-operator-d4cd68cb9-rml8w             1/1     Running             1 (7s ago)   11s
submariner-routeagent-6ztvl                     1/1     Running             0            2s
```

## 验证

可以把member集群的kubeconfig聚集到host集群，修改

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: xxxx
    server: https://xxx:6443  # 修改对应IP
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: member1  # 修改 member name
current-context: member1  # 修改 member name
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: xxxx
    client-key-data: xxx
```

检查安装配置排查原因

```sh
export KUBECONFIG=member1-config  # 可以省下--kubeconfig 


subctl show all --kubeconfig ${member_KUBE_CONFIG}
subctl diagnose all --kubeconfig ${member_KUBE_CONFIG}
```

验证正常member集群A, B直接pod service可以互相访问

pod之间可以直接访问

```sh
kubectl --kubeconfig member1-config get pod -owide

NAME                     READY   STATUS    RESTARTS      AGE     IP           NODE                                          NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-b7xmw   1/1     Running   1 (45h ago)   7d3h    10.44.0.6    server-605ae265-df2b-4e91-9efa-19069d84f2d0   <none>           <none>
serve-59994d98f6-2lkmp   1/1     Running   0             3h42m   10.44.0.40   server-605ae265-df2b-4e91-9efa-19069d84f2d0   <none>           <none>

kubectl --kubeconfig member2-config run tmp-shell --rm -i --tty --image submariner/nettest:0.12.2 -- /bin/bash
If you don't see a command prompt, try pressing enter.
bash-5.1# curl 10.44.0.6
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
```



service靠service export

**Deploy ClusterIP Service**

```sh
kubectl --kubeconfig kubeconfig.cluster-b create deployment nginx --image=nginx
kubectl --kubeconfig kubeconfig.cluster-b expose deployment nginx --port=80
subctl export service --kubeconfig kubeconfig.cluster-b --namespace default nginx
```

**Verify**

```sh
kubectl --kubeconfig kubeconfig.cluster-a -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest:0.12.2 -- /bin/bash
curl nginx.default.svc.clusterset.local

```



## 原理

[根据submariner官方文档，Lighthouse Agent会自动做这件事](https://submariner.io/getting-started/architecture/service-discovery/)

![image-20220905144757719](https://tva1.sinaimg.cn/large/e6c9d24egy1h5vqa0awkpj20p80ly3yw.jpg)

把上面内容放到mcs.yaml

```sh
kubectl --kubeconfig karmada-apiserver.config apply -f mcs.yaml
```

完成后在member2看到以`<service-name>-<service-namespace>-<cluster-id>`的ServiceImport，可以通过这个svc连接到member1

```sh
kubectl --kubeconfig member2-config get svcim -A
NAMESPACE             NAME                    TYPE           IP                  AGE
submariner-operator   serve-default-member1   ClusterSetIP   ["10.45.84.26"]     13m
```

[根据Lighthouse DNS Server](https://submariner.io/getting-started/architecture/service-discovery/)工作原理需要用到通过clusterset.local进行DNS转发，**所以加入dnsConfig更加符合实际使用情况。**

mcs_test.yaml

```sh
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
spec:
  containers:
  - name: dnsutils
    image: quay.io/submariner/nettest:0.12.2
    imagePullPolicy: IfNotPresent
    command: ["sleep","3600"]
  dnsConfig:
    searches:
      - svc.clusterset.local
```

**验证**

```sh
kubectl --kubeconfig member2-config  apply -f mcs_test.yaml

kubectl --kubeconfig member2-config exec -it dnsutils -- /bin/bash
bash-5.1# curl serve.default
'hello from cluster 1 (Node: server-605ae265-df2b-4e91-9efa-19069d84f2d0 Pod: serve-59994d98f6-4f2t8 Address: 10.44.0.29)'
```







# Multi-cluster Service Discovery(MCS)

- https://karmada.io/docs/userguide/service/multi-cluster-service

- [集群必须在1.21+（新版本EndpointSlice 是v1的）mcs mci功能才能正常使用](https://github.com/karmada-io/karmada/issues/2460)

## 部署MCS程序

前提是调试好**Submariner**

![image-20220915102433713](https://tva1.sinaimg.cn/large/e6c9d24egy1h672uzis81j21gy0pogo8.jpg)

install crd

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: ClusterPropagationPolicy
metadata:
  name: serviceexport-policy
spec:
  resourceSelectors:
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: serviceexports.multicluster.x-k8s.io
  placement:
    clusterAffinity:
      clusterNames:
        - member1
        - member2
---        

apiVersion: policy.karmada.io/v1alpha1
kind: ClusterPropagationPolicy
metadata:
  name: serviceimport-policy
spec:
  resourceSelectors:
    - apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: serviceimports.multicluster.x-k8s.io
  placement:
    clusterAffinity:
      clusterNames:
        - member1
        - member2
```



把应用下发到member1

```yaml

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: serve
spec:
  replicas: 1
  selector:
    matchLabels:
      app: serve
  template:
    metadata:
      labels:
        app: serve
    spec:
      containers:
      - name: serve
        image: jeremyot/serve:0a40de8
        args:
        - "--message='hello from cluster 1 (Node: {{env \"NODE_NAME\"}} Pod: {{env \"POD_NAME\"}} Address: {{addr}})'"
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
---      
apiVersion: v1
kind: Service
metadata:
  name: serve
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: serve
---
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: mcs-workload
spec:
  resourceSelectors:
    - apiVersion: apps/v1
      kind: Deployment
      name: serve
    - apiVersion: v1
      kind: Service
      name: serve
  placement:
    clusterAffinity:
      clusterNames:
        - member1

```

把ServiceExport下发到member1

```yaml
---
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceExport
metadata:
  name: serve
---
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: serve-export-policy
spec:
  resourceSelectors:
    - apiVersion: multicluster.x-k8s.io/v1alpha1
      kind: ServiceExport
      name: serve
  placement:
    clusterAffinity:
      clusterNames:
        - member1
```

把ServiceImport下发到member2

```yaml
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceImport
metadata:
  name: serve
spec:
  type: ClusterSetIP
  ports:
  - port: 80
    protocol: TCP
---
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: serve-import-policy
spec:
  resourceSelectors:
    - apiVersion: multicluster.x-k8s.io/v1alpha1
      kind: ServiceImport
      name: serve
  placement:
    clusterAffinity:
      clusterNames:
        - member2
```

部署

```yaml
kubectl --kubeconfig karmada-apiserver.config apply -f mcs.yaml
```

可以看到`derived-`前缀的svc在member2，[`imported-`前缀的endpointSlice](https://github.com/karmada-io/karmada/issues/2460)

```sh
kubectl --kubeconfig member2-config get svc
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
derived-serve   ClusterIP   10.47.99.218   <none>        80/TCP    3m53s


kubectl --kubeconfig member2-config get endpointSlice
NAME                           ADDRESSTYPE   PORTS   ENDPOINTS      AGE
imported-member1-serve-pz5fv   IPv4          8080    10.44.0.27     5s
```

## 测试MCS

```sh
kubectl --kubeconfig member2-config run tmp-shell --rm -i --tty --image submariner/nettest:0.12.2 -- /bin/bash

curl derived-serve
'hello from cluster 1 (Node: server-605ae265-df2b-4e91-9efa-19069d84f2d0 Pod: serve-59994d98f6-4f2t8 Address: 10.44.0.7)'
```





# Multi-cluster Ingress(MCI)

https://karmada.io/docs/userguide/service/multi-cluster-ingress

- [必须保证MCI部署集群和应用测试集群直接pod可以互相访问，集群版本1.21+](https://github.com/karmada-io/karmada/issues/2479)

## 移除traefik

k3s 默认[安装traefik](https://rancher.com/docs/k3s/latest/en/networking/#traefik-ingress-controller)，[移除参考](https://github.com/k3s-io/k3s/issues/1160#issuecomment-561518578)

```sh
# 删除traefik服务
kubectl -n kube-system get helmcharts.helm.cattle.io traefik -o yaml > traefik_helm.yaml
kubectl -n kube-system delete helmcharts.helm.cattle.io traefik

# 停止k3s服务
systemctl stop k3s
```

编辑服务文件`vim /etc/systemd/system/k3s.service`并将此行添加到`ExecStart` 

```
--disable traefik \
```

从自动部署文件夹中删除清单文件

```sh
mv /var/lib/rancher/k3s/server/manifests/traefik.yaml traefik.yaml
```

重启服务

```sh
systemctl daemon-reload
systemctl k3s start
```



## multi-cluster-ingress

https://karmada.io/docs/userguide/service/multi-cluster-ingress



### build image

根据[karmada官网](https://github.com/karmada-io/multi-cluster-ingress-nginx)fork来自[ingress  controller-v1.1.1](https://github.com/kubernetes/ingress-nginx/releases/tag/helm-chart-4.0.15)

```sh
git clone https://github.com/karmada-io/multi-cluster-ingress-nginx.git
```

在根目录，运行构建`ingress-controller/controller:1.0.0-dev`参考`build/dev-env.sh`文件

```sh
export TAG=1.0.0-dev
export REGISTRY=${REGISTRY:-ingress-controller}

DEV_IMAGE=${REGISTRY}/controller:${TAG}

make build image
docker tag "${REGISTRY}/controller:${TAG}" "${DEV_IMAGE}"
```

### 部署

![image-20220915102349202](https://tva1.sinaimg.cn/large/e6c9d24egy1h672u8xpbcj21gq0q00vf.jpg)

ingress_values.yaml ，替换controller的image

```yaml
controller:
  name: controller
  image:
    registry: "harbor.xxx.cn:xxx"
    image: ingress-nginx/multi-cluster-controller  # 替换
    tag: "v1.1.1"
    digest: ""
  admissionWebhooks:
    patch:
      image:
        registry: "harbor.xxx.cn:xxx"
        image: ingress-nginx/kube-webhook-certgen
        tag: v1.1.1
        digest: ""
  config:
    worker-processes: "1"
  podLabels:
    deploy-date: "1662515412"
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  hostPort:
    enabled: true
  terminationGracePeriodSeconds: 0
  service:
    type: NodePort

defaultBackend:
  image:
    registry: "harbor.xxx.cn:xxx"
    image: ingress-nginx/defaultbackend-amd64
    tag: "1.5"

```

部署

[multi-cluster-ingress-nginx项目](https://github.com/karmada-io/multi-cluster-ingress-nginx)，打包helm charts

```sh
cd charts
helm package ingress-nginx
```

部署

```sh
helm install ingress-nginx -n ingress-nginx --create-namespace ingress-nginx-4.0.15.tgz -f ingress_values.yaml
```

**为了让nginx-ingress-controller有监听资源的权限（multiclusteringress, endpointslices, and service）。要把karmada-apiserver.config加到nginx-ingress-controller**

karmada-kubeconfig-secret.yaml

```yaml
# karmada-kubeconfig-secret.yaml
apiVersion: v1
data:
  kubeconfig: {data} # karmada-apiserver.config Base64-encoded
kind: Secret
metadata:
  name: kubeconfig
  namespace: ingress-nginx
type: Opaque
```

修改deployment

```sh
kubectl apply -f karmada-kubeconfig-secret.yaml
kubectl -n ingress-nginx edit deployment ingress-nginx-controller
```

Example

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  ...
spec:
  ...
  template:
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --karmada-kubeconfig=/etc/kubeconfig  # new line
        ...
        volumeMounts:
        ...
        - mountPath: /etc/kubeconfig            # new line
          name: kubeconfig                      # new line
          subPath: kubeconfig                   # new line
      volumes:
      ...
      - name: kubeconfig                        # new line
        secret:                                 # new line
          secretName: kubeconfig                # new line
```

显示

```
kubectl get pods -n ingress-nginx
NAME                                       READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-5b8b6d64d-b4kqn   1/1     Running   0          16m
```





### 测试

ingress_test.yaml 

```yaml
apiVersion: networking.karmada.io/v1alpha1
kind: MultiClusterIngress
metadata:
  name: demo-localhost
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: demo.localdev.me
    http:
      paths:
      - backend:
          service:
            name: serve
            port:
              number: 80
        path: /web
        pathType: Prefix
```

应用部署先[部署MCS程序](#部署MCS程序)，然后再部署MultiClusterIngress

访问

```sh
kubectl apply -f ingress_test.yaml
curl -H 'Host:demo.localdev.me' http://127.0.0.1:80/web
'hello from cluster xxxxx'
```



# 通过proxy api管理member集群

https://karmada.io/docs/userguide/globalview/aggregated-api-endpoint

cluster-proxy-rbac.yaml

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-proxy-clusterrole
rules:
- apiGroups:
  - 'cluster.karmada.io'
  resources:
  - clusters/proxy
  resourceNames:
  - member1
  - member2
  - member3
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-proxy-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-proxy-clusterrole
subjects:
  - kind: User
    name: "system:admin"
```



```sh
kubectl --kubeconfig karmada-apiserver.config apply -f cluster-proxy-rbac.yaml
```

验证

```sh
kubectl --kubeconfig karmada-apiserver.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy/api/v1/nodes
```

## 通过Bearer token访问proxy api

- [根据这个配置Bearer token的使用](https://github.com/karmada-io/karmada/issues/2448)
- [告别KubeConfig切换，Karmada开启 “多云一卡通”](https://mp.weixin.qq.com/s?__biz=MzIzNzU5NTYzMA==&mid=2247497323&idx=1&sn=4dd7eb1966c7923a3317d036a385fd52#rd)

member-proxy-rbac.yaml

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: custom-admin
  namespace: karmada-custom-user


apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  name: custom-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:karmada-custom-user
```

赋予sa，member集群的管理权限

```sh
kubectl --kubeconfig=member1-config  -f  member-proxy-rbac.yaml
```

cluster-proxy-rbac.yaml  同样的sa

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: custom-admin
  namespace: karmada-custom-user

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-cluster-proxy-clusterrole
rules:
- apiGroups:
  - 'cluster.karmada.io'
  resources:
  - clusters/proxy
  resourceNames:
  - member1
  - member2
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: custom-cluster-proxy-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-cluster-proxy-clusterrole
subjects:
- kind: ServiceAccount
  name: custom-admin
  namespace: karmada-custom-user
- kind: Group
  name: "system:serviceaccounts"
- kind: Group
  name: "system:serviceaccounts:karmada-custom-user"
```

在host集群apply

```sh
kubectl --kubeconfig karmada-apiserver.config apply -f cluster-proxy-rbac.yaml
```

获取sa的token

```sh
# 实际情况注意namespace 和 secret name
kubectl --kubeconfig karmada-apiserver.config -n karmada-custom-user describe secret $(kubectl -n karmada-custom-user get secret | grep admin | awk '{print $1}')
```



```sh
curl -k -X GET 'https://xxxx:5443/apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes' \
-H 'Authorization: Bearer {token}'
```

