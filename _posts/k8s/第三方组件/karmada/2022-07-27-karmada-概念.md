---
layout:     post
rewards: false
title:   karmada 概念
categories:
    - k8s


---

[参考](https://blog.csdn.net/DaoCloud_daoke/article/details/122497365?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165828780816781683993152%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=165828780816781683993152&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-2-122497365-null-null.185^v2^control&utm_term=Karmada)

# why

为了解决**异构多云**，**让应用无感知的多云多集群**。

解决问题

- 应用下发的模型是什么样的？
- 应用的模型怎样被调度到不同的集群？
- 在不同的集群下发的模型怎样做到差异化？
- 怎样对需要计算资源的工作负载调度到不同集群，来均衡所有集群的资源？
- 应用下发的模型怎样做到多集群，多机房，多地的灾备和双活的能力？
- 多集群的统一访问入口的网关怎样建设？
- 跨集群的内部服务发现怎样建设，以及跨集群的网络连通？
- 怎样统一内部的访问方式的地址，举例全局域名？
- 怎样在不同的集群间，保证镜像的一致，以及数据的同步？
- 怎样设计统一的下发流程，举例不同环境的下发流程可能会不一样？
- 怎样从顶层设计和规划资源的分配和使用？
- 怎样打通多云部署带来的统一观测性能力的建设？
- 怎样设计和解决在多集群的背景下，有状态服务的调度机制和访问机制？
- 怎样解决云边场景以及混合云的统一应用管理能力？

# 简介

Karmada 是 CNCF 的云原生项目，主要的能力是纳管多个 Kubernetes 集群，以及基于原生的 Kubernetes 的资源对象，将其下发到多个集群。**对于一些有计算资源需求的 Deployment，Job 等 workload 具体副本数调度能力，让不同的 workload 按照一些的策略运行在不同的集群上。**以此来达到多云分发的能力的这么一个项目。


Karmada 本身需要运行在 Kubernetes 集群中

- Host Cluster (宿主集群)

  主要是用来运行 Karmada 控制平面的组件，其中包含 Karmada 的 etcd，karmada-api server， karmada-controller manager， Kubernetes controller manager，karmada-scheduler，karmada-webhook， karmada-scheduler-estimator 等控制面的组件。

- Workload Cluster

  负责真正运行工作负载的集群，会真正运行业务的容器、一些 Kubernetes 的资源对象、存储、网络、dns 等，同时对于 pull 模式的部署方式，还会运行 Karmada 的 agent 组件，用于和控制面组件通信，完成工作负载的下发能力。

## 概念介绍

![img](https://tva1.sinaimg.cn/large/e6c9d24egy1h4lhiq9817j20t00qmq54.jpg)

## ResourceTemplate

在 Karmada 中没有真正的 crd 类型是 ResourceTemplate，这里的ResourceTemplate 只是对 Karmada 可分发的资源对象的一种抽象，这里的Resource 包含 Kubernetes 中所有支持的资源对象的类型，包括常见的 Deployment，Service，Pod，Ingress，PVC，PV， Job 等等，同时也原生的支持 CRD。

## Cluster

Cluster 对象会被保存在 karmada-cluster 这个 namespace 中，这个 namespace 的作用有点类似 Kubernetes 的 kube-system，是系统预留的 namespace。

Cluster 对象代表一个完整的，单独的一套 Kubernetes 集群，是可用于运行工作负载的集群的抽象和连接配置。在 Karmada 中，集群不仅仅有 spec，还有 status，这个 status 中描述了集群的基本信息，支持的 crd，以及集群的可用物理资源的统计等，用于在后续的调度器中使用这些信息进行调度处理。

## 策略

- PropagationPolicy

  为了**定义资源对象被下发的策略**，如下发到哪些集群，以及下发到这些集群中的需要计算资源的工作负载的副本数应该怎样分配。

  resource selector这个选择器 让PropagationPolicy 知道自己的策略是作用于哪些资源对象

- OverridePolicy

  **定义在下发到不同集群中的配置**，例如不同的集群所对应的镜像仓库的地址是不一样的，那就需要设置在不同集群中的工作负载的镜像地址是不一样，例如在不同的环境下，需要设置不同的环境变量等。

  OverridePolicy 的作用时机是在 PropagationPolicy 之后，以及在真正在下发到集群之前，主要处理逻辑是 binding controller 中处理的，会在后续的篇幅中提到。其中可以使用的 override 的能力目前包括 Plaintext，ImageOverrider，CommandOverrider，ArgsOverrider 这几种，用来完成不同集群的差异化配置能力。同样，OverridePolicy 也会通过 resource selector 的机制来匹配到需要查建议配置的资源对象。


## ResourceBinding

ResourceBinding 这个 crd 不是用于给最终用户使用的，而是 Karmada 自身机制工作需要的，**主要的用途是说明了某一个资源被绑定到了哪个集群上了**，这个和集群绑定的逻辑是在 detector 的 PropagationPolicy 的 controller 中来控制，可以理解成 ResourceBinding 是 PropagationPolicy 派生出来的一个 cr 对象。

同时 Karmada 的调度器的处理对象就是以一个 **ResourceBinding 为处理单元，在此基础上来完成资源对象所需要副本数的计算和调度，最终会由 binding controller 完成对 ResourceBinding 的处理，从而生成 Work 对象，**同步下发到对应的集群上。

## work

Work 这个对象代表了所有资源对象，以及所有资源对象需要下发到多个集群的时候所对应的业务模型。

举例来说，**一个 Deployment 需要下发到两个集群中去，那对应到这个 Deployment 的 work 的对象就会有两个**，因为 work 是**集群单位的对象**，主要的作用就是对应和代表一个资源对象在一个集群中的逻辑对象。这个逻辑对象 (work)，它保存在控制平面，在后面的 execution controller 中会介绍到 work。work 和 execution 的概念，这里就是 Karmada 在做多集群下发中完成 Host Cluster 和 Workload Cluster 之间真正业务能力的体现。

## Execution Namespace

**在 Karmada 的控制平面中**，会为每一个被纳管的集群创建一个集群相关的 namespace，这个 namespace 中存放的资源对象是 work。每一个和这个集群相关的 Kubernetes 资源对象都会对应到这个 namespace 下的一个 work。

举例，在 Karmada 的控制平面创建了一个 service 资源对象，同时通过 PropagationPolicy 下发到两个集群中，那在每一个集群对应的 execution namespace 中都会有一个 work 对象对应到当前集群需要负责的 service对象上。


## 跨集群服务发现

ServiceImport 的作用是配合 ServiceExport 来完成跨集群的服务发现，要想在某一个集群中去发现另一个集群的服务，除了要在暴露服务的集群中创建 ServiceExport，还要在需要发现和使用这个服务的集群中，创建 ServiceImport 对象，以此来在集群中发现其它集群的服务，和使用这个其它集群的服务。

不是 Karmada 自己的 CRD，而是 Kubernetes 社区在解决跨集群服务发现的场景下定义的一套 mcs api 规范。

# 架构

![img](https://tva1.sinaimg.cn/large/e6c9d24egy1h4lj2b8yihj219k0t442k.jpg)

## karmada agent

karmada agent 的作用主要是使用 pull 模式的时候才会需要，这里的 **pull 指的是 workload 集群中的 agent 主动去控制平面去 pull 需要下发的资源对象。**

考虑在云边的场景下，控制面 Host Cluster 运行在公有云，Workload Cluster 运行在私有云的场景下，这个时候是需要安装 agent，每一个集群安装一个 agent 就可以了。也可以在私有环境使用 pull 模式，取决于场景的需要。

## karmada scheduler estimator

estimator 的组件是负责更加精确的计算副本分配的能力，所以整个工作过程中，只会当被调度处理的资源对象是 Deployment 和 Job 的时候，才会和 estimator 组件进行通信。

引入了 estimator 组件，**解决因为集群的总体资源是充足的，但是每一台机器的资源不足的，导致最终的 Pod 调度不出去。**每一个 estimator 会负责一个独立的集群的可调度副本数的计算。estimator 会根据调度的资源请求，计算出这样的资源规格。



# 安装

[关于helm chart安装，证书问题](https://github.com/karmada-io/karmada/issues/1880)

## helm chart安装

helm https://github.com/karmada-io/karmada/tree/release-1.2/charts

```
bitnami/kubectl:latest
cfssl/cfssl
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/kube-apiserver:v1.21.7
k8s.gcr.io/kube-controller-manager:v1.21.7
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-aggregated-apiserver:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-controller-manager:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-webhook:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-search:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-descheduler:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-scheduler-estimator:latest
swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-agent:latest
```

karmada.yaml 修改了镜像地址

```yaml
etcd:
  internal:
    image:
      repository: karmada/etcd
kubeControllerManager:
  image:
    repository: karmada/kube-controller-manager
apiServer:
  image:
    repository: karmada/kube-apiserver


search:
  image:
    repository: karmada/karmada-search
descheduler:
  image:
    repository: karmada/karmada-descheduler
schedulerEstimator:
  image:
    repository: karmada/karmada-scheduler-estimator
agent:
  image:
    repository: karmada/karmada-agent
aggregatedApiServer:
  image:
    repository: karmada/karmada-aggregated-apiserver
controllerManager:
  image:
    repository: karmada/karmada-controller-manager
webhook:
  image:
    repository: karmada/karmada-webhook
scheduler:
  image:
    repository: karmada/karmada-scheduler
    
certs:
  auto:
    hosts: [
      "kubernetes.default.svc",
      "*.etcd.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}",
      "*.{{ .Release.Namespace }}.svc",
      "localhost",
      "127.0.0.1",
      "host-server-ip"
    ]
```

安装

```sh
helm install karmada -n karmada-system --create-namespace karmada-0.0.3.tgz -f karmada.yaml
```



```
NAME                                              READY   STATUS    RESTARTS        AGE
etcd-0                                            1/1     Running   0               4m49s
karmada-aggregated-apiserver-79b7b5654c-8rw8j     1/1     Running   2 (4m15s ago)   4m49s
karmada-apiserver-9d86c4949-b488p                 1/1     Running   0               4m49s
karmada-controller-manager-778c7b7b56-6qrn2       1/1     Running   2 (4m45s ago)   4m49s
karmada-kube-controller-manager-6dd5bdc55-7z5h9   1/1     Running   2 (4m14s ago)   4m49s
karmada-scheduler-d6b87bcf9-h74bn                 1/1     Running   0               4m49s
karmada-webhook-68dd9586c9-ntb9r                  1/1     Running   2 (4m44s ago)   4m49s
```



卸载

```sh
helm uninstall karmada -n karmada-system

kubectl delete sa/karmada-pre-job -n karmada-system
kubectl delete clusterRole/karmada-pre-job 
kubectl delete clusterRoleBinding/karmada-pre-job
kubectl delete ns karmada-system
```







## kubectl-karmada 安装

https://github.com/karmada-io/karmada/releases

```sh
tar -zxf kubectl-karmada-linux-amd64.tgz

which kubectl
cp kubectl-karmada /usr/local/bin

```

move kubectl-karmada executable file to PATH path

```sh
kubectl karmada version

kubectl karmada version: version.Info{GitVersion:"v1.2.1", GitCommit:"de4972b74f848f78a58f9a0f4a4e85f243ba48f8", GitTreeState:"clean", BuildDate:"2022-07-14T09:33:32Z", GoVersion:"go1.17.11", Compiler:"gc", Platform:"linux/amd64"}
```



## install agent

**host 集群上**

```sh
# 获取karmada config
kubectl get secret -n karmada-system karmada-kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d > karmada-config
```

karmada-config 修改server成`https://host-server-ip:5443`

```yaml
apiVersion: v1
kind: Config
clusters:
  - cluster:
      certificate-authority-data: xxxx
      insecure-skip-tls-verify: false
      server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443
    name: karmada-apiserver
users:
  - user:
      client-certificate-data: xxxx
      client-key-data: xxxx
    name: karmada-apiserver
contexts:
  - context:
      cluster: karmada-apiserver
      user: karmada-apiserver
    name: karmada-apiserver
current-context: karmada-apiserver
```

**worker集群上**

agent.yaml 根据karmada-config填写证书信息需要**base64 decode**，根据不同的worker集群修改**clusterName**

```yaml
installMode: "agent"
agent:
  image:
    repository: karmada/karmada-agent
  clusterName: "member"
  kubeconfig:
    caCrt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    crt: |
      -----BEGIN CERTIFICATE-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END CERTIFICATE-----
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      XXXXXXXXXXXXXXXXXXXXXXXXXXX
      -----END RSA PRIVATE KEY-----
    server: "https://host-server-ip:5443"

```

安装

```sh
# 安装
helm install karmada-agent -n karmada-system --create-namespace  karmada-0.0.3.tgz -f agent.yaml

# 卸载
helm uninstall karmada-agent -n karmada-system
```



测试

worker集群

```sh
kubectl get pods -n karmada-system

NAME                             READY   STATUS    RESTARTS   AGE
karmada-agent-86864d7d8b-kfmk9   1/1     Running   0          20s
```

host集群

```sh
kubectl get cluster --kubeconfig karmada-config

NAME      VERSION        MODE   READY   AGE
member1   v1.19.5+k3s2   Pull   True    4m39s
```

