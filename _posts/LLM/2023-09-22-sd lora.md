---
layout:     post
rewards: false
title:   stable diffusion 应用
categories:
    - AI
tags:
   - 大模型




---





# stable-diffusion-webui

https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.2.1

- [RuntimeError: Detected that PyTorch and torchvision were compiled with different CUDA versions](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/9341#issuecomment-1495224456)
- [Stable Diffusion WebUI部署过程踩坑记录](https://blog.csdn.net/qq_41234663/article/details/129783216)
- [中文插件](https://github.com/dtlnor/stable-diffusion-webui-localization-zh_CN)



```
source venv/bin/activate
./webui.sh --listen --port 7862 --xformers --enable-insecure-extension-access --skip-install --api
```



# lora 训练

参考技巧

- [AI绘画Stable Diffusion LoRA模型训练教程](https://zhuanlan.zhihu.com/p/630368440)
- [如何从零开始训练专属 LoRA 模型](https://www.uisdc.com/lora-model)

大致步骤

- 底模选择
- 图片预处理
- lora训练
- 对比lora效果



## 底模选择

底模，尽量选祖宗级别的模型练出来的LoRA会更通用。如果在融合模型上训练可能会**仅仅在你训练的底模上生成图片拥有不错的效果** 但是失去了通用性。可以自己抉择

什么是**祖宗级别的模型**？

sd1.5 2.0、novelai 原版泄露模型。也就是非融合模型。融合模型比如 anything 系列融合了一大堆，orangemix系列融合了 anything 和 basil 更灵车了等等。在他们上面训练的会迁移性更差一些。



## 图片预处理

**找素材，切素材，生成对应的关键词** 就三步

1. 不少于 15 张的高质量图片，一般可以准备 20-50 张图
2. 图片主体内容清晰可辨、特征明显，图片构图简单，避免其它杂乱元素
3. 如果是人物照，尽可能以脸部特写为主（多角度、多表情），再放几张全身像（不同姿势、不同服装）
4. 减少重复或相似度高的图片。
5. 选择高清，或者大尺寸，特大尺寸。点进去之后看看有没有套图。

具体的筛选标准：

- 脸部有遮挡的不要（比如麦克风、手指、杂物等）
- 背景太复杂的不要（比如一堆字的广告板，或夜市太乱的背景）
- 分辨率太低的不要（例如希望画 `512x512`，则**训练至少需要用 2 倍分辨率** `1024x1024`），
- 光影比较特殊的不要（比如暗光，背光等）
- 不像本人特征的不要（比如大部分训练集都是长发，那么短发显脸大的不要，大笑毁形象的不要）
- 化妆太浓重的、美颜太严重的不要



直接用sd，就可以完成**切素材，生成对应的关键词**，（人物可以选择**自动焦点裁切**）

<img src="https://cdn.jsdelivr.net/gh/631068264/img/202309221106810.png" alt="image-20230922110622768" style="zoom:33%;" />

生产**图片.png-关键词tag.txt**的训练集，例如

![image-20230922111523254](https://cdn.jsdelivr.net/gh/631068264/img/202309221115310.png)

**预处理生成 tags 打标文件**后，就需要对文件中的标签再进行优化，一般有两种优化方法：

- 方法一：保留全部标签

  就是对这些标签不做删标处理, 直接用于训练。一般在训练画风，或想省事快速训练人物模型时使用。

  - 优点：不用处理 tags 省时省力，过拟合的出现情况低。
  - 缺点：风格变化大，需要输入大量 tag 来调用、训练时需要把 epoch 训练轮次调高，导致训练时间变长。

- 方法二：删除部分特征标签

  比如训练某个特定角色，要保留蓝眼睛作为其自带特征，那么就要将 blue eyes 标签删除，以防止将基础模型中的 blue eyes 引导到训练的 LoRA 上。简单来说删除标签即将特征与 LoRA 做绑定，保留的话画面可调范围就大。

  **一般需要删掉的标签：如人物特征 long hair，blue eyes 这类。**

  **不需要删掉的标签：如人物动作 stand，run 这类，人物表情 smile，open mouth 这类，背景 simple background，white background 这类，画幅位置等 full body，upper body，close up 这类。**

  - 优点：调用方便，更精准还原特征。
  - 缺点：容易导致过拟合，泛化性降低。

**什么是过拟合**：过拟合会导致画面细节丢失、画面模糊、画面发灰、边缘不齐、无法做出指定动作、在一些大模型上表现不佳等情况。

**批量打标**：有时要优化等标签会比较多，可以尝试使用批量打标工具

## 炼丹炉

使用秋葉aaaki参考视频 

- [【AI绘画】模型训练器发布！简单又不失专业的LoRA模型训练一键包](https://www.bilibili.com/video/BV1AL411q7Ub/?spm_id_from=333.999.0.0&vd_source=d591262dc9ce1bba22682d1cba1ca930)

使用项目https://github.com/Akegarasu/lora-scripts

```sh
conda create -n sdlora python=3.10.9 -y

conda activate sdlora


pip install -r requirements.txt -i  https://mirrors.aliyun.com/pypi/simple/ -U

pip install xformers
```



```sh
 ./run_gui.sh --host 0.0.0.0 --port 7860 --tensorboard-host 0.0.0.0 --tensorboard-port 7861
```



根据页面提示去填参数就好，预估效果。

- 根据页面提示填写，主要修改的参数  http://xxx.xx:7860/  lora 训练页面 ，参数详解http://xxxx:7860/lora/params.html

  ![image-20231008105154975](https://cdn.jsdelivr.net/gh/631068264/img/202310081104599.png)

  ![image-20231008105223789](https://cdn.jsdelivr.net/gh/631068264/img/202310081104432.png)

- 看控制台报错信息，训练进度。
- tensorboard 的loss  http://xxx.xxx:7861/  tensorboard页面 
- **启用UI的训练预览图设置**  ， 在输出目录的sample目录

训练好后在output目录生成对应名字的lora文件，**.safetensors**后缀，然后复制到**stable-diffusion-webui/models/Lora**

底模放到**stable-diffusion-webui/models/Stable-diffusion**，就可以给sd使用，看不到相关的模型或者lora，可以按一下刷新。

## 对比Lora之间效果

**对比多个lora效果**，找出效果比较好的lora**版本和权重**。

![如何从零开始训练专属 LoRA 模型？4600字总结送给你！](https://cdn.jsdelivr.net/gh/631068264/img/202309221251112.jpg)



在 Stable Diffusion WebUI 页面最底部的脚本栏中调用 XYZ plot 脚本，设置模型对比参数。

划重点：其中 X 轴类型和 Y 轴类型都选择**「提示词搜索/替换」Prompt S/R**。

- X 轴值输入：NUM,000001,000002,000003,000004,000005，对应模型序号

- Y 轴值输入：STRENGTH,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1，对应模型权重值

![image-20230922125338778](https://cdn.jsdelivr.net/gh/631068264/img/202309221253812.png)

通过对比生成结果，选出表现最佳的模型和权重值。

![如何从零开始训练专属 LoRA 模型？4600字总结送给你！](https://cdn.jsdelivr.net/gh/631068264/img/202309251736371.jpg)



# sdapi

[官方wiki api](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API)

[Stable Diffusion 操作界面及基础参数介绍](https://zhuanlan.zhihu.com/p/629910090)

[[linux-sd-webui]之txt2img](https://blog.csdn.net/u012193416/article/details/130490169)

加上`--api`的参数，访问the `/docs` endpoint，其他参数值的获取参考[官方wiki api](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API)

```python
    size = 512
    payload = {
        'prompt': "xxx",
        "negative_prompt": "xxxx",
        "sampler_index": 'DPM++ 2M Karras',
        "cfg_scale": 15,  # 如果CFG Scale越大，输出将更符合输入提示和/或输入图像，但会失真。另一方面，CFG Scale 值越低，越有可能偏离提示或输入图像，但质量也越好。
        "steps": 30,
        "seed": -1,
        'override_settings': {
            'sd_model_checkpoint': 'darkSushi25D25D_v20.safetensors [2d0010aca5]', # 模型名
            'CLIP_stop_at_last_layers': 2,
        },
        'enable_hr': True,
        'denoising_strength': 0.7,  # 该参数的范围从0到1，其中0根本不添加噪声，你将获得添加的确切图像，1完全用噪声替换图像，
        'hr_scale': 2,
        'hr_upscaler': 'Latent',
    # 'hr_resize_x':768,
    # 'hr_resize_y':768,



        'height': size, # 图片越大ai就越倾向于往里面塞入更多的东西，绝大多数模型都是在512*512分辨率下训练，当输出尺寸比较大比如说1024*1024的时候，ai就会尝试在图中塞入两到三张图片的内容量，于是会出现各种肢体拼接，不受词条控制的多人，多角度等情况，增加词条可以部分缓解，但是更关键的还是控制好画幅，先画中小图，再放大为大图。
        'width': size,

        'batch_size': 4,  # 每批数量是显卡一次所生成的图片数量，速度要比调高批次快一点，但是调的太高可能会导致显存不足导致生成失败，而生成批次不会导致显存不足，只要时间足够会一直生成直到全部输出完毕。
        'n_iter': 1,
    }

    response = requests.post(url=f'{url}/sdapi/v1/txt2img', json=payload)

    r = response.json()


    if 'error' in r.keys():
        print(r)
        print(f"{r['error']}:{r['errors']}")
    else:
        print(len(r['images']))
        for i, imgb64 in enumerate(r['images']):
            imgdata = base64.b64decode(imgb64)
            with open(f"test/{i}.png", 'wb') as f:
                f.write(imgdata)
```



# Diffusion Model

[Denoising Diffusion Probabilistic Models （DDPM）](https://arxiv.org/abs/2006.11239)

## Reverse Process

生成带噪音的特定尺寸的图片，然后一步步**过滤噪音**（Denoise）, 到生成清晰图片。靠近最后一步的是1

![image-20230923235647938](https://cdn.jsdelivr.net/gh/631068264/img/202309232356992.png)

**本来图片就已经在噪音的图里面，现在只是需要过滤掉噪音。**

Denoise Model 的input (图片和现在噪音的程度)，使用同一个model进行过滤。

![image-20230924000153190](https://cdn.jsdelivr.net/gh/631068264/img/202309240001235.png)





## Noise Predicter

它就是**预测这张图片里面噪音应该长什么样子**，再把它输出的噪音去减去这个要被Denoise的图片，然后就产生Denoise以后的结果

![image-20230924001812852](https://cdn.jsdelivr.net/gh/631068264/img/202309240018891.png)

为什么不输入是要被Denoise的图片输出就直接是Denoise的结果？

- 如果你今天你的Denoise Model可以产生一只带噪音的猫，那为什么不直接画猫
- 产生一个带噪音的猫 和 产生一张图片里面的噪音难度不一样，直接产生前者比较难



训练Noise Predicter过程，使用**Forward Process(Diffusion Process)。**

![image-20230924003954366](https://cdn.jsdelivr.net/gh/631068264/img/202309240039403.png)

**主动加随机噪音往前推导，生成训练集。**

## Text to image

**训练时候Noise Predicter**加上文字说明，[Laion 比较大的一个训练集](https://laion.ai/blog/laion-5b/)

![image-20230924003724097](https://cdn.jsdelivr.net/gh/631068264/img/202309240037145.png)

![image-20230924004729177](https://cdn.jsdelivr.net/gh/631068264/img/202309240047241.png)



# Stable Diffusion

一般由三部分组成，encoder，图片生成model，decoder，分开三块训练。

![image-20230924005334622](https://cdn.jsdelivr.net/gh/631068264/img/202309240053671.png)

**encoder对结果影响大，图片生成model的规模影响小，  偏向右下效果好**

![image-20230924081257448](https://cdn.jsdelivr.net/gh/631068264/img/202309240812556.png)

## FID 对比图片相似程度

[GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500)

训练一个图片分类model(CNN+softmax)，计算原图和生产的图的高斯分布距离，FID越小，越相近。

![image-20230924082422478](https://cdn.jsdelivr.net/gh/631068264/img/202309240824532.png)

## CLIP

[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

通过image-text对的训练集，**衡量文字和图片是否真的有对应关系。**

![image-20230924082952823](https://cdn.jsdelivr.net/gh/631068264/img/202309240829875.png)

## Decoder 训练

图片生成model 的output分成两种

- 最终目标的小图 （用大图缩小，生成小-大图训练集）

  ![image-20230924083923772](https://cdn.jsdelivr.net/gh/631068264/img/202309240839825.png)

- 和最终目标相关的非人类识别的中间产物，拿最终的输入和输出图片对比，训练出Auto encoder后，直接拿decoder用

  ![image-20230924083944198](https://cdn.jsdelivr.net/gh/631068264/img/202309240839251.png)

## 图片生成model 训练

模仿[DDPM](#Diffusion Model)

![image-20230924084731129](https://cdn.jsdelivr.net/gh/631068264/img/202309240847184.png)

## 最终效果

随机生成噪音+ 文字encoder , 多次Denoise降噪，然后Decoder。**生图过程：中间产物每次Decode**

![image-20230924084941034](https://cdn.jsdelivr.net/gh/631068264/img/202309240849088.png)

