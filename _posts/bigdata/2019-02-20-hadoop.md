---
layout:     post
rewards: false
title:      Hadoop
categories:
    - big data
tags:
    - big data
---

# HDFS

Hadoop Distributed File System (HDFS)

旨在存储大量信息，通常为**PB**。

**块结构文件系统**来完成的。单个文件被拆分为固定大小的块，这些块存储在集群上。由多个块组成的文件通常不会将所有块存储在同一台机器上。
为了确保可靠性，块在集群中存在副本。复制因子默认是3，每个块在集群中存在3份。

- NameNote 保存文件系统的元数据
- DataNode 存储组成文件的块

NameNode和DataNode进程可以在一台机器上运行，但HDFS集群通常由运行NameNode进程的专用服务器和可能有数千台运行DataNode进程的计算机组成。

# NameNode
metadata 包括 文件名，文件权限以及每个文件的每个块的位置。保存在**内存**。
NameNode还跟踪块的**复制因子**，确保机器故障不会导致数据丢失。

由于NameNode是单点故障，因此可以使用辅助NameNode生成主NameNode内存结构的快照，从而降低NameNode失败时数据丢失的风险。
当DataNode失败时，NameNode将复制丢失的块以确保每个块满足最小**复制因子**。

- [snakebite](https://github.com/spotify/snakebite)
- [pyarrow](https://arrow.apache.org/docs/python/)

# YARN

https://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html

## 局限性
经典 MapReduce 的最严重的限制主要关系到可伸缩性、资源利用和对与 MapReduce 不同的工作负载的支持。在 MapReduce 框架中，作业执行受两种类型的进程控制：

一个称为 JobTracker 的主要进程，它协调在集群上运行的所有作业，分配要在 TaskTracker 上运行的 map 和 reduce 任务。
许多称为 TaskTracker 的下级进程，它们运行分配的任务并定期向 JobTracker 报告进度。


- 单个 JobTracker 导致的可伸缩性瓶颈
- Hadoop 设计为仅运行 MapReduce 作业。随着替代性的编程模型（比如 Apache Giraph 所提供的图形处理）的到来，
除 MapReduce 外，越来越需要为可通过高效的、公平的方式在同一个集群上运行并共享资源的其他编程模型提供支持。

## 改变
我们减少了单个 JobTracker 的职责，将部分职责委派给 TaskTracker，因为集群中有许多 TaskTracker。
在新设计中，这个概念通过将 JobTracker 的双重职责（集群资源管理和任务协调）分开为两种不同类型的进程来反映。

不再拥有单个 JobTracker，一种新方法引入了一个集群管理器，它惟一的职责就是跟踪集群中的活动节点和可用资源，并将它们分配给任务。
对于提交给集群的每个作业，会启动一个专用的、短暂的 JobTracker 来控制该作业中的任务的执行。有趣的是，短暂的 JobTracker 由在从属节点上运行的 TaskTracker 启动。
因此，作业的生命周期的协调工作分散在集群中所有可用的机器上。得益于这种行为，更多工作可并行运行，可伸缩性得到了显著提高。

## yarn

- ResourceManager 代替集群管理器
- ApplicationMaster 代替一个专用且短暂的 JobTracker
- NodeManager 代替 TaskTracker
- 一个分布式应用程序代替一个 MapReduce 作业

在 YARN 架构中，一个全局 ResourceManager 以主要后台进程的形式运行，它通常在专用机器上运行，在各种竞争的应用程序之间仲裁可用的集群资源。ResourceManager 会追踪集群中有多少可用的活动节点和资源，协调用户提交的哪些应用程序应该在何时获取这些资源。ResourceManager 是惟一拥有此信息的进程，所以它可通过某种共享的、安全的、多租户的方式制定分配（或者调度）决策（例如，依据应用程序优先级、队列容量、ACLs、数据位置等）。

在用户提交一个应用程序时，一个称为 ApplicationMaster 的轻量型进程实例会启动来协调应用程序内的所有任务的执行。这包括监视任务，重新启动失败的任务，推测性地运行缓慢的任务，以及计算应用程序计数器值的总和。这些职责以前分配给所有作业的单个 JobTracker。ApplicationMaster 和属于它的应用程序的任务，在受 NodeManager 控制的资源容器中运行。

NodeManager 是 TaskTracker 的一种更加普通和高效的版本。没有固定数量的 map 和 reduce slots，NodeManager 拥有许多动态创建的资源容器。容器的大小取决于它所包含的资源量，比如内存、CPU、磁盘和网络 IO。目前，仅支持内存和 CPU (YARN-3)。未来可使用 cgroups 来控制磁盘和网络 IO。一个节点上的容器数量，由配置参数与专用于从属后台进程和操作系统的资源以外的节点资源总量（比如总 CPU 数和总内存）共同决定。

有趣的是，ApplicationMaster 可在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。您还可以实现一个自定义的 ApplicationMaster 来运行特定的任务，进而发明出一种全新的分布式应用程序框架，改变大数据世界的格局。您可以查阅 Apache Twill，它旨在简化 YARN 之上的分布式应用程序的编写。


### 一个可运行任何分布式应用程序的集群

ResourceManager、NodeManager 和容器都不关心应用程序或任务的类型。所有特定于应用程序框架的代码都转移到它的 ApplicationMaster，
以便任何分布式框架都可以受 YARN 支持 — 只要有人为它实现了相应的 ApplicationMaster。

# MapReduce

## map
处理一堆键值对，`mapper`会顺序单独处理每个键值对，产生若干个output键值对

## shuffle sort  
相当于根据key聚类

`mapper`运行完，mapper的输出传送到reducers的过程叫shuffle，每一个分类器确保来自同一个key的所有value都发送同一个reducer。
默认根据mapper’s output key的hash值去分类。

到达reducers前，会对key和value进行排序。具有相同key值的数据聚合在一起。

## reduce
唯一key包含不唯一的值，reducer为每个唯一key合并它们的value,产生若干键值对

![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0e5g94fgmj31ia0mewhi.jpg)

- [mrjob](https://github.com/Yelp/mrjob)

# Pig
[pig online doc](https://pig.apache.org/docs/latest/index.html)
- 高级数据流语言 Pig Latin
- 运行Pig Latin程序的执行环境

Pig能够让你专心于数据及业务本身，而不是纠结于数据的格式转换以及MapReduce程序的编写。

## Execution Modes

### local
only requires a single machine. Pig will run on the local host and access the local filesystem.
`pig -x local ...`

### MapReduce
`pig ...`

## Interactive Mode
Pig can be run interactively in the Grunt shell. 
```
pig -x local
...
grunt>
```

## Batch Mode
use pig script
`pig -x local id.pig`

## pig 语法
Each statement is an operator that takes a relation as an input, performs a transformation on that relation,
and produces a relation as an out‐ put. Statements can span multiple lines,`;`结尾。

- A LOAD statement that reads the data from the filesystem
- One or more statements to transform the data
- A DUMP or STORE statement to view or store the results

### load
USING default keyword `\t` 
AS default not named and type bytearray
```pig
LOAD 'data' [USING function] [AS schema];
```

```pig
A = LOAD 'students' AS (name:chararray, age:int);
DUMP A; 

(john,21,3.89) 
(sally,19,2.56) 
(alice,22,3.76) 
(doug,19,1.98) 
(susan,26,3.25)
```

### Transforming Data
条件关系 and or not

#### FILTER
处理列
```
A = LOAD 'students' AS (name:chararray, age:int, gpa:float);

DUMP A; 
(john,21,3.89)
(sally,19,2.56) 
(alice,22,3.76)
(doug,19,1.98)
(susan,26,3.25)

R = FILTER A BY age>=20;
DUMP R; 

(john,21,3.89) 
(alice,22,3.76) 
(susan,26,3.25)
```
![](https://ws3.sinaimg.cn/large/006tKfTcgy1g0egcywet8j30zu09m3yr.jpg)

#### FOREACH
处理行 有点像select

```
R = FOREACH A GENERATE *;
```
![](https://ws2.sinaimg.cn/large/006tKfTcgy1g0eghch92lj30ek0bidg5.jpg)

![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0egi7gevyj31hw0imaav.jpg)

#### GROUP
```
B = GROUP A BY age;
```
![](https://ws4.sinaimg.cn/large/006tKfTcgy1g0egkxb7cpj31h60jswfo.jpg)

![](https://ws2.sinaimg.cn/large/006tKfTcgy1g0ehjnt6a6j31hg0jujsk.jpg)

#### STORE

```
STORE alias INTO 'directory' [USING function];
```

```
A = LOAD 'students' AS (name:chararray, age:int, gpa:float);
STORE A INTO 'output' USING PigStore('|');
CAT output;
```
![](https://ws2.sinaimg.cn/large/006tKfTcgy1g0ehsqlwbwj318b0u0wg2.jpg)


## UDF

[pig_util](https://github.com/apache/pig/tree/trunk/src/python/streaming)
```python
from pig.pig_util import outputSchema


@outputSchema('word:chararray')
def reverse(word):
    """
    Return the reverse text of the provided word """
    return word[::-1]


@outputSchema('length:int')
def num_chars(word):
    """
    Return the length of the provided word """
    return len(word)
```
```
REGISTER 'my_udf.py' USING streaming_python AS string_udf;
term_length = FOREACH unique_terms GENERATE word, string_udf.num_chars(word) as length;
```

# Spark

集群计算框架,相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，内存内运算，比Hadoop快100倍。
其基础的程序抽象则称为弹性分布式数据集（RDDs），是一个可以并行操作、有容错机制的数据集合。

Spark应用程序作为集群上的独立进程集运行，由SparkContext 主程序中的对象`driver program`主导

```
brew install scala
brew install apache-spark
```
[spark](https://github.com/apache/spark)
[pyspark](http://spark.apache.org/docs/latest/api/python/index.html)

```python
from pyspark import SparkContext

PATH = 'hdfs://localhost:9000'


def main():
    with SparkContext(appName='sparkwordcount') as sc:
        input_file = sc.textFile(PATH + '/user/wyx/input/input.txt')
        counts = input_file.flatMap(lambda line: line.split()) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a + b)
        counts.saveAsTextFile(PATH + '/user/wyx/output')


if __name__ == '__main__':
    main()
```
## Interactive
master 参数

- local
Run Spark with one worker thread.
- local[n]
Run Spark with n worker threads.
- spark://HOST:PORT
Connect to a Spark standalone cluster.
- mesos://HOST:PORT
Connect to a Mesos cluster.

```
pyspark --master local[4]
```
## run
run spark script
```
spark-submit --master local word_count.py
```

```
sc = SparkContext(master='local[4]')
```

## 弹性分布式数据集（RDD）
Resilient Distributed Datasets (RDDs) 是Spark基本编程对象，不可变数据集，分布式存储在机器，可以并行操作。

> The RDD.glom() method returns a list of all of the elements within each partition, 
and the RDD.collect() method brings all the elements to the driver node. 

### get RDD from collect
`sc.parallelize`
![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0far6sw53j314a0u0gnq.jpg)

### get file
`sc.textFile`
![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0fatrgcq7j318u0u0di1.jpg)

## transformations and actions

Transformations create new datasets from existing ones, and actions run a computation on the dataset and return results to the driver program.

不会马上计算，Spark会记住对于base dataset的所有转换。当有action时才会计算出结果返回到`the driver program`
默认情况下，每次action都会计算，即使是相同的transform。`.persist()` save result in memory when it first computed

- [RDD API](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)

```python
def t_map():
    data = [1, 2, 3, 4, 5]
    with SparkContext() as sc:
        rdd = sc.parallelize(data)
        gl = rdd.map(lambda x: x * 2)
        col = gl.collect()
        print(col)
```
- When use Broadcast variable
> 只有在跨多个阶段的任务需要相同数据,数据重要，不更改的值。仅在需要时加载，数据仅发送到包含需要它的执行程序的节点。
- [In spark, how does broadcast work?](https://stackoverflow.com/questions/40685469/in-spark-how-does-broadcast-work)

# Hive

Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上。

- [Hive Home](https://cwiki.apache.org/confluence/display/Hive/Home#Home-UserDocumentation)
- [Hive Tutorial](https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-HiveTutorial)

通过HSQL访问在**Hadoop**上的文件或者**HBase**上的数据，实现extract/transform/load(ETL)和数据分析，etc。
**Tez**，**Spark**或 **MapReduce**执行引擎,支持UDF
![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0gqiv06sdj30go096glu.jpg)

metastore 保存了于Hive到HDFS映射。Hive使用HQL操作HDFS的数据