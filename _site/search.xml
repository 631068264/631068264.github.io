<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[安全术语解析]]></title>
      <url>/blog/2019/10/23/sec_info</url>
      <content type="text"><![CDATA[APT来源 知乎 知道创宇 云安全  APT是黑客以窃取核心资料为目的，针对客户所发动的网络攻击和侵袭行为，是一种蓄谋已久的“恶意商业间谍威胁”。这种行为往往经过长期的经营与策划，并具备高度的隐蔽性。APT的攻击手法，在于隐匿自己，针对特定对象，长期、有计划性和组织性地窃取数据，这种发生在数字空间的偷窃资料、搜集情报的行为，就是一种“网络间谍”的行为。APT攻击是一个集合了多种常见攻击方式的综合攻击。综合多种攻击途径来尝试突破网络防御，通常是通过Web或电子邮件传递，利用应用程序或操作系统的漏洞，利用传统的网络保护机制无法提供统一的防御。除了使用多种途径，高级定向攻击还采用多个阶段穿透一个网络，然后提取有价值的信息，这使得它的攻击更不容易被发现。扫描探测在APT攻击中，攻击者会花几个月甚至更长的时间对”目标”网络进行踩点，针对性地进行信息收集，目标网络环境探测，线上服务器分布情况，应用程序的弱点分析，了解业务状况，员工信息等等。工具投送在多数情况下，攻击者会向目标公司的员工发送邮件，诱骗其打开恶意附件，或单击一个经过伪造的恶意URL，希望利用常见软件(如Java或微软的办公软件)的0day漏洞，投送其恶意代码。一旦到位，恶意软件可能会复制自己，用微妙的改变使每个实例都看起来不一样，并伪装自己，以躲避扫描。有些会关闭防病毒扫描引擎，经过清理后重新安装，或潜伏数天或数周。恶意代码也能被携带在笔记本电脑、USB设备里，或者通过基于云的文件共享来感染一台主机，并在连接到网络时横向传播。漏洞利用利用漏洞，达到攻击的目的。攻击者通过投送恶意代码，并利用目标企业使用的软件中的漏洞执行自身。而如果漏洞利用成功的话，你的系统将受到感染。普通用户系统忘记打补丁是很常见的，所以他们很容易受到已知和未知的漏洞利用攻击。一般来说，通过使用零日攻击和社会工程技术，即使最新的主机也可以被感染，特别是当这个系统脱离企业网络后。木马植入随着漏洞利用的成功，更多的恶意软件的可执行文件——击键记录器、木马后门、密码破解和文件采集程序被下载和安装。这意味着，犯罪分子现在已经建成了进入系统的长期控制机制。远程控制一旦恶意软件安装，攻击者就已经从组织防御内部建立了一个控制点。攻击者最常安装的就是远程控制工具。这些远程控制工具是以反向连接模式建立的，其目的就是允许从外部控制员工电脑或服务器，即这些工具从位于中心的命令和控制服务器接受命令，然后执行命令，而不是远程得到命令。这种连接方法使其更难以检测，因为员工的机器是主动与命令和控制服务器通信而不是相反。横向渗透一般来说，攻击者首先突破的员工个人电脑并不是攻击者感兴趣的，它感兴趣的是组织内部其它包含重要资产的服务器，因此，攻击者将以员工个人电脑为跳板，在系统内部进行横向渗透，以攻陷更多的pc和服务器。攻击者采取的横向渗透方法包括口令窃听和漏洞攻击等。目标行动将敏感数据从被攻击的网络非法传输到由攻击者控制的外部系统。在发现有价值的数据后，APT攻击者往往要将数据收集到一个文档中，然后压缩并加密该文档。此操作可以使其隐藏内容，防止遭受深度的数据包检查和DLP技术的检测和阻止。然后将数据从受害系统偷运出去到由攻击者控制的外部。大多数公司都没有针对这些恶意传输和目的地分析出站流量。那些使用工具监控出站传输的组织也只是寻找”已知的”恶意地址和受到严格监管的数据。TTPcsdn chenyulancn战术Tactics、技术Techniques、过程Procedures  战术 — 协调使用并有序部署军队。  技术 — 用于执行战斗行动、履行职责或任务的非规定方式或方法。  过程 — 规定如何执行特定任务的标准和具体步骤。MITRE ATT&amp;CK提供了易于使用的计算机安全相关的TTP分类。旁路串联  旁路模式一般是指通过交换机等网络设备的端口镜像功能来实现监控  串联模式一般是通过网关或者网桥的模式来进行监控对原始传递的数据包不会造成延时，不会对网速造成任何影响。监控设备一旦故障或者停止运行，不会影响现有网络的正常原因。旁路模式采用发送RST包的方式来断开TCP连接]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> 安全 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Storm]]></title>
      <url>/blog/2019/10/20/storm</url>
      <content type="text"><![CDATA[Storm 集群Storm集群上有两种节点：主节点和工作节点。 Storm run topologies 每秒可以访问数万条消息  主节点运行一个名为Nimbus的守护程序。Nimbus负责在集群中分发代码，向计算机分配任务以及监视故障。  每个工作程序节点都运行一个称为“ Supervisor”的守护程序。主管侦听分配给其机器的工作，并根据Nimbus分配给它的工作，根据需要启动和停止工作进程。  每个工作进程执行一个拓扑子集；一个正在运行的拓扑由分布在许多计算机上的许多工作进程组成。Nimbus守护程序和Supervisor守护程序是快速故障且无状态的。所有状态都保存在Zookeeper或本地磁盘中，使Storm集群非常稳定。Topologies  A topology is a graph of computation. Each node in a topology containsprocessing logic, and links between nodes indicate how data should bepassed around between nodes.一个拓扑就是一个计算图谱。拓扑中的每一个节点包含着处理逻辑，以及表明数据如何在节点间传递的很多链接。可以使用任何编程语言来创建和提交拓扑。streamA stream is an unbounded sequence oftuples。Storm提供以分布式的、可靠的方式提供流的转换。Storm提供处理流转换的基本原语是spouts和bolts。  spouts 流的源 接收原始数据(tuples) 作为流发射它们  boltsSpouts将数据传递到Bolts和Bolts过程，并产生新的输出流。Bolts可以执行过滤，聚合等操作Spouts和Bolts连接在一起，形成拓扑结构。实时应用程序逻辑在Storm拓扑中指定。简单地说，拓扑是有向图，其中顶点是计算，边缘是数据流。其中每个节点都是Spouts或Bolts。图中的边缘指示哪些Bolts正在订阅哪些流。当Spouts或Bolts向流中发送元组时，它将元组发送给订阅该流的每个Bolts。拓扑中节点之间的链接指示元组应如何传递。Storm拓扑中的每个节点都并行执行。在拓扑中，您​​可以指定每个节点要多少并行度，然后Storm将在集群中产生该数量的线程来执行。拓扑将永远运行，或者直到您杀死它为止。Storm将自动重新分配所有失败的任务。此外，Storm保证即使机器宕机和消息丢失也不会丢失数据。]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[snmp]]></title>
      <url>/blog/2019/09/24/snmp</url>
      <content type="text"><![CDATA[whatSNMP是英文Simple Network ManagementProtocol的缩写，中文意思是简单网络管理协议。SNMP是一种简单网络管理协议，它属于TCP/IP五层协议中的应用层协议。SNMP主要用于网络设备的管理(服务器、工作站、路由器、交换机及HUBS等)。它由一组网络管理的标准组成，包含一个应用层协议（application layer protocol）、数据库模式（database schema），和一组数据对象install in Ubuntuapt installapt-get install snmpd snmp snmp-mibs-downloader  snmpd：snmp服务端软件  snmp：snmp客户端软件  snmp-mibs-downloader：用来下载更新本地mib库的软件安装过程中，程序会帮我们自动下载mib库，并保存在/usr/share/mibs目录中download-mibs 手动下载SNMPv2 config配置目录cd /etc/snmp重启服务service snmpd restart# 遍历所有mibsnmpwalk -v 2c -c public localhostsnmpd.conf 服务端获取更多的节点信息# 注释view  systemonly  included  .1.3.6.1.2.1.1view  systemonly  included  .1.3.6.1.2.1.25.1# 新增view  systemonly  included  .1修改v2密码默认密码为public# 改rocommunity public  default    -V systemonlyrocommunity6 public  default  -V systemonlyrocommunity passwdYouWant  default    -V systemonlyrocommunity6 passwdYouWant  default  -V systemonly允许远程默认端口是161# 改agentAddress  udp:127.0.0.1:161agentAddress udp:161,udp6:[::1]:161查看效果netstat -antup | grep 161udp        0      0 0.0.0.0:161            0.0.0.0:*                          11656/snmpd   udp6      0      0 ::1:161                :::*                                11656/snmpd客户端snmp.conf获取相关节点信息更直观oid -On 显示oid# 注释mibs :service snmp restartSNMPv3SNMPv3在SNMP的基础之上增强了安全性以及远程配置功能。最初，SNMP最大的缺点就是安全性弱。SNMP的第一与第二个版本中，身份验证仅仅是在管理员与代理间传送一个明文的密码而已。目前每一个SNMPv3的信息都包含了被编码成8进制的安全参数。这些安全参数的具体意义由所选用的安全模型决定。SNMPv3提供了重要的安全特性：  保密性 —— 加密数据包以防止未经授权的源监听。  完整性 —— 数据的完整性特性确保数据在传输的时候没有被干扰，并且包含了可选的数据响应保护机制。  身份验证 —— 检查数据是否来自一个合法的源。vim /etc/default/snmpd改SNMPDOPTS='-Lsd -Lf /dev/null -u snmp -g snmp -I -smux,mteTrigger,mteTriggerConf -p /var/run/snmpd.pid'SNMPDOPTS='-Lsd -Lf /dev/null -u snmp -I -smux -p /var/run/snmpd.pid -c /etc/snmp/snmpd.conf'SNMPv3有多种不同的用途。noauth —— 没有授权，加密以及任何安全保护！auth —— 需要身份认证，但是不对通过网络发送的数据进行加密。priv —— 最可靠模式。需要身份认证而且数据会被加密。vim /etc/snmp/snmpd.conf#createUser user1createUser user2 MD5 user2passwordcreateUser user3 MD5 user3password DES user3encryption#rouser user1 noauth 1.3.6.1.2.1.1rouser user2 auth 1.3.6.1.2.1rwuser user3 priv 1.3.6.1.2.1MIB OID  MIB set  常用OID管理信息库MIB 有点像分级目录的结构。不同厂商的设备会有自己的mibOID(对象标识符），是SNMP代理提供的具有唯一标识的键值。MIB（管理信息基）提供数字化OID到可读文本的映射使用实例import (	"errors"	"math/big"	"regexp"	"strings"	"time"	g "github.com/soniah/gosnmp")func parseIpTable(name string) (ipTable IpTableT) {	name = strings.Replace(name, ipCidrRouteDest, "", -1)	ipReg := `(\.[0-9]{1,3}){4}`	reg := regexp.MustCompile(ipReg)	res := reg.FindAllString(name, -1)	ipTable = IpTableT{		Dest:    strings.TrimLeft(res[0], "."),		Mask:    strings.TrimLeft(res[1], "."),		Gateway: strings.TrimLeft(res[2], "."),	}	return}func snmp2Client(target string, port uint16, password string) *g.GoSNMP {	g.Default.Target = target	g.Default.Port = port	g.Default.Community = password	g.Default.Timeout = timeOut	g.Default.Retries = retries	return g.Default}func getMsgFlags(parameters *g.UsmSecurityParameters) g.SnmpV3MsgFlags {	if parameters.AuthenticationProtocol == g.NoAuth {		return g.NoAuthNoPriv	} else if parameters.AuthenticationProtocol != g.NoAuth &amp;&amp; parameters.PrivacyProtocol == g.NoPriv {		return g.AuthNoPriv	}	return g.AuthPriv}func userSec(user string, authProto uint8, authPasswd string, privProto uint8, privPasswd string) *g.UsmSecurityParameters {	return &amp;g.UsmSecurityParameters{		UserName:                 user,		AuthenticationProtocol:   authProtocol[authProto],		AuthenticationPassphrase: authPasswd,		PrivacyProtocol:          privProtocol[privProto],		PrivacyPassphrase:        privPasswd,	}}func snmp3Client(target string, port uint16, parameters *g.UsmSecurityParameters) *g.GoSNMP {	client := &amp;g.GoSNMP{		Target:             target,		Port:               port,		Version:            g.Version3,		Timeout:            timeOut,		SecurityModel:      g.UserSecurityModel,		MsgFlags:           getMsgFlags(parameters),		SecurityParameters: parameters,	}	return client}func snmpGet(client *g.GoSNMP) (SnmpResultT, error) {	var result = SnmpResultT{		Alive: false,	}	err := client.Connect()	if err != nil {		log.Errorf("connect error %s:%s %s", client.Target, client.Port, err)		return result, ConnectError	}	defer client.Conn.Close()	res, err := client.Get(oid)	if err != nil {		log.Errorf("get error %s:%s %s", client.Target, client.Port, err)		return result, GetError	}	result.Alive = true	for _, variable := range res.Variables {		if variable.Name == systemRunTime {			result.SystemRunTime = g.ToBigInt(variable.Value)		} else if variable.Name == ipSystemStatsOutRequestsIpv4 {			result.Out = g.ToBigInt(variable.Value)		} else if variable.Name == ipSystemStatsInDeliversIpv4 {			result.In = g.ToBigInt(variable.Value)		}	}	return result, nil}func snmpWalk(client *g.GoSNMP) ([]IpTableT, error) {	err := client.Connect()	if err != nil {		log.Errorf("connect error %s:%s %s", client.Target, client.Port, err)		return nil, ConnectError	}	defer client.Conn.Close()	results, err := client.BulkWalkAll(ipCidrRouteDest)	if err != nil {		log.Errorf("walk error %s:%s %s", client.Target, client.Port, err)		return nil, GetError	}	var res []IpTableT	for _, r := range results {		res = append(res, parseIpTable(r.Name))	}	return res, nil}func Snmp2(target string, port uint16, password string) (SnmpResultT, error) {	client := snmp2Client(target, port, password)	getResult, err := snmpGet(client)	if err != nil {		return getResult, err	}	ipRoute, err := snmpWalk(client)	if err != nil {		return getResult, err	}	getResult.IpRoute = ipRoute	return getResult, nil}func Snmp3(target string, port uint16, user string, authProto uint8, authPasswd string, privProto uint8, privPasswd string) (SnmpResultT, error) {	userParam := userSec(user, authProto, authPasswd, privProto, privPasswd)	client := snmp3Client(target, port, userParam)	getResult, err := snmpGet(client)	if err != nil {		return getResult, err	}	ipRoute, err := snmpWalk(client)	if err != nil {		return getResult, err	}	getResult.IpRoute = ipRoute	return getResult, nil}func Snmp(s *model.SnmpDevice) (SnmpResultT, error) {	if s.Proto == V2 {		return Snmp2(s.Ip, s.Port, s.Community)	}	return Snmp3(s.Ip, s.Port, s.UserName, s.AuthProto, s.AuthPassword, s.PrivateProto, s.PrivatePassword)}]]></content>
      <categories>
        
          <category> network </category>
        
      </categories>
      <tags>
        
          <tag> network </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[git 子模块]]></title>
      <url>/blog/2019/09/21/git-submodule</url>
      <content type="text"><![CDATA[已存在子项目引入到not exist dirgit submodule add repo_url tar_dir子项目加入到exist dirgit rm -r --cached tar_dirgit submodule add repo_url tar_dir第一次clone含子模块的项目clone main project 后子项目只有空目录git submodule update --init本地更新子项目  master分支git submodule update --remote [子模块名]]]></content>
      <categories>
        
          <category> git </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MySQL Server has gone away]]></title>
      <url>/blog/2019/08/29/mysql-gone-away</url>
      <content type="text"><![CDATA[Mysql 曾经挂过查看mysql的运行时长mysql -uroot -p -e "show global status like 'uptime';"+---------------+-------+| Variable_name | Value |+---------------+-------+| Uptime        | 68928 |+---------------+-------+1 row in set (0.04 sec)连接超时长连接 长连接很久没有新的请求发起，达到了server端的timeout时长会被server强行关闭。 此后再通过这个connection发起查询的时候，就会报错server hasgone away。mysql -uroot -p -e "show global variables like '%timeout';"+---------------------+----------------+| variable_name       | variable_value |+---------------------+----------------+| INTERACTIVE_TIMEOUT | 28800          || WAIT_TIMEOUT        | 28800          |+---------------------+----------------+interactive_timeout和wait_timeout的区别  interactive_timeout针对交互式连接，wait_timeout针对非交互式连接  mysql客户端连接数据库是交互式连接，通过jdbc连接数据库是非交互式连接。  连接启动的时候，根据连接的类型，来确认会话变量wait_timeout的值是继承于全局变量wait_timeout，还是interactive_timeout。  如果是交互式连接，则继承全局变量interactive_timeout的值，如果是非交互式连接，则继承全局变量wait_timeout的值验证代码中检查是否超时，是否需要重连   pool_recycle&lt; WAIT_TIMEOUTsqlalchemy  create_engine(pool_recycle)结果集超大通常发生在查询字段包含数据量较大 &gt; max_allowed_packetshow global variables like 'max_allowed_packet';+--------------------+---------+| Variable_name      | Value   |+--------------------+---------+| max_allowed_packet | 1048576 |+--------------------+---------+1 row in set (0.00 sec)]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[auto increment]]></title>
      <url>/blog/2019/08/12/auto-increment</url>
      <content type="text"><![CDATA[遇到mysql关于auto increment的重启坑正常情况定义表CREATE TABLE `user` (  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,  `user_name` varchar(10) NOT NULL,  PRIMARY KEY (`id`),) ENGINE=InnoDB AUTO_INCREMENT=101 DEFAULT CHARSET=utf8;直接insert into的话id会是101诡异情况定义了AUTO_INCREMENT但没有插入数据，重启mysql后，在插入居然是id=1关于AUTO_INCREMENT的原理官方 InnoDB AUTO_INCREMENT计数器初始化为InnoDB表某列指定AUTO_INCREMENT，在内存表对象中auto-incrementcounter（自动增量计数器），为该列赋予新值。MySQL 5.7及更早版本中，自动增量计数器仅存储在内存中，而不存储在磁盘上。所以服务重启时，InnoDB会执行类似SELECT MAX(ai_col) FROM table_name FORUPDATE;的语句来重置auto-increment counterMySQL 8.0中，此行为已更改。当maximum auto-increment countervalue每次改变时，都会写入redolog，并保存到每个检查点上的引擎专用系统表中。保证重启后最大值不变。在MySQL 5.7及更早版本中，服务器重新启动会取消AUTO_INCREMENT = Ntable选项的效果，该选项可以在CREATE TABLEor ALTER TABLE语句中用于设置初始计数器值或分别更改现有计数器值。在MySQL 8.0中，服务器重新启动不会取消AUTO_INCREMENT = N表选项的效果 。如果将自动递增计数器初始化为特定值，或者将自动递增计数器值更改为更大的值，则新值将在服务器重新启动时保持不变。]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[zookeeper]]></title>
      <url>/blog/2019/08/11/zookeeper</url>
      <content type="text"><![CDATA[what文件系统+监听通知机制ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能  顺序一致性 - 客户端的更新将按发送顺序应用。  原子性 - 更新成功或失败。没有部分结果。  单系统映像 - 无论服务器连接到哪个服务器，客户端都将看到相同的服务视图。  可靠性 - 一旦应用了更新，它将从那时起持续到客户端覆盖更新。  及时性 - 系统的客户视图保证在特定时间范围内是最新的。使用简单  create : creates a node at a location in the tree  delete : deletes a node  exists : tests if a node exists at a location  get data : reads the data from a node  set data : writes data to a node  get children : retrieves a list of children of a node  sync : waits for data to be propagatedznode文件目录 znode(目录节点) 节点有两种 持久的，临时的通过一个 Leader 选举过程来选定一台称为 Leader 的机器，Leader既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 都只能提供读服务。集群ZAB（ZooKeeper Atomic Broadcast 原子广播）实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。崩溃恢复当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。消息广播集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[go 锁 chan]]></title>
      <url>/blog/2019/08/10/go-lock</url>
      <content type="text"><![CDATA[sync.Mutex互斥锁 同一时间只能有一个线程进入当你使用mutex时，确保mutex和其保护的变量没有被导出 import "sync"type Info struct {	mu sync.Mutex	// ... other fields, e.g.: Str string}func Update(info *Info) {	info.mu.Lock()    // critical section:    info.Str = // new value    // end critical section    info.mu.Unlock()}sync.RWMutex读写锁针对读写操作的互斥锁，它可以分别针对读操作和写操作进行锁定和解锁操作。读写锁遵循的访问控制规则与互斥锁有所不同。使用情况多读单写 多个只读操作并行执行，但写操作会完全互斥。changoroutine 轻量级线程协程是轻量的，比线程更轻。它们痕迹非常不明显（使用少量的内存和资源）：使用 4K 的栈内存就可以在堆中创建它们。因为创建非常廉价，必要的时候可以轻松创建并运行大量的协程（在同一个地址空间中 100,000 个连续的协程）。并且它们对栈进行了分割，从而动态的增加（或缩减）内存的使用；栈的管理是自动的，但不是由垃圾回收器管理的，而是在协程退出后自动释放。goroutine 与其他协程不一样  Go 协程意味着并行（或者可以以并行的方式部署），协程一般来说不是这样的  Go 协程通过通道来通信；协程通过让出和恢复操作来通信chan 数据共享// 发送和接收操作在另一端准备好之前都会阻塞 同步且无缓冲a := make(chan time.Duration)ch &lt;- v    // 将 v 发送至信道 ch。v := &lt;-ch  // 从 ch 接收值并赋予 v。buffer始化一个带缓冲的信道仅当信道的缓冲区填满后，向其发送数据时才会阻塞。当缓冲区为空时，接受方会阻塞。通道可以同时容纳的元素个数 缓冲容量和类型无关selectselect 会阻塞到某个分支可以继续执行为止，这时就会执行该分支。当多个分支都准备好时会随机选择一个执行。当 select 中的其它分支都没有准备好时，default 分支就会执行。为了在尝试发送或者接收时不发生阻塞，可使用 default 分支CheckValidDuration = make(chan time.Duration)go func() {		CheckValidDuration &lt;- GetInitTimeDuration()	}()func checkExpire() {	timeDuration := GetInitTimeDuration()	for {		ok := license.Lcache.Valid()		if !ok {			log4go.Error("sensor license expire time")		}		select {		case &lt;-time.After(timeDuration):		case x := &lt;-license.CheckValidDuration:			timeDuration = x			log4go.Info("update timeDuration : %d", timeDuration)		}	}}]]></content>
      <categories>
        
          <category> go </category>
        
      </categories>
      <tags>
        
          <tag> go </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[grpc]]></title>
      <url>/blog/2019/08/06/grpc</url>
      <content type="text"><![CDATA[what  grpc  protobufgprc 是一种通信协议 ，使用protocol buffers作为结构数据序列化机制，通信协议格式分成client server  client  就像本地对象一样，可以远程调用不同机器server上的方法，轻松地创建分布式应用程序和服务  server  服务器实现此接口并运行gRPC服务器来处理客户端调用installbrew install protobufgo get -u github.com/golang/protobuf/protoc-gen-gohow  定义*.proto消息结构体，服务  protoc --go_out=plugins=grpc:. *.proto 生成对应语言的协议接口 ( clientserver接口 &amp;&amp; 消息的请求响应序列化 )通过 protocol buffer 的编译器 protoc以及一个特殊的 gRPC Go 插件来完成]]></content>
      <categories>
        
          <category> go </category>
        
      </categories>
      <tags>
        
          <tag> go </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[go rule]]></title>
      <url>/blog/2019/07/27/go-rule</url>
      <content type="text"><![CDATA[变量Go 语言变量名由字母、数字、下划线组成，其中首个字母不能为数字。var (    a int    b bool    str string)这种因式分解关键字的写法一般用于声明全局变量，一般在func 外定义。当一个变量被var声明之后，系统自动赋予它该类型的零值：int 为 0float 为 0.0bool 为 falsestring 为空字符串””指针为 nil记住，这些变量在 Go 中都是经过初始化的。声明赋值声明var s string声明赋值 := 结构不能在函数外使用i := 100                  // an intvar boiling float64 = 100 // a float64var i, j, k int                 // int, int, intvar b, f, s = true, 2.3, "four" // bool, float64, string作用域的坑声明赋值func main() {      x := 1    fmt.Println(x)     // prints 1    {        fmt.Println(x) // prints 1        x := 2         // 两个x不一样了        fmt.Println(x) // prints 2    }    fmt.Println(x)     // prints 1 (不是2)}表达式new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址，返回的指针类型为*T。p := new(int)   // p, *int 类型, 指向匿名的 int 变量fmt.Println(*p) // "0"*p = 2          // 设置 int 匿名变量的值为 2fmt.Println(*p) // "2"x := 1p := &amp;x         // p, of type *int, points to xfmt.Println(*p) // "1"*p = 2          // equivalent to x = 2fmt.Println(x)  // "2"生命周期变量的生命周期指的是在程序运行期间变量有效存在的时间间隔。对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。而相比之下，局部变量的声明周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。常量显式类型定义： const b string = "abc"隐式类型定义： const b = "abc"枚举 从0开始  每遇到一次 const 关键字，iota 就重置为 0const (    a = iota    b = iota    c = iota)数组数组长度也是数组类型的一部分，所以[5]int和[10]int是属于不同类型的。a := [2]int{1, 2}b := [...]int{1, 2}c := [2]int{1, 3}fmt.Println(a == b, a == c, b == c) // "true false false"d := [3]int{1, 2}fmt.Println(a == d) // compile error: cannot compare [2]int == [3]inttype Currency intconst (    USD Currency = iota // 美元    EUR                 // 欧元    GBP                 // 英镑    RMB                 // 人民币)//表示数组的长度是根据初始化值的个数来计算  通过索引赋值symbol := [...]string{USD: "$", EUR: "€", GBP: "￡", RMB: "￥"}fmt.Println(RMB, symbol[RMB]) // "3 ￥"把一个大数组传递给函数会消耗很多内存。有两种方法可以避免这种现象：  传递数组的指针  使用数组的切片切片一直觉得这是定义数组另一种方式‍切片是引用，本身就是一个指针。所以它们不需要使用额外的内存并且比使用数组更有效率，所以在Go 代码中切片比数组更常用多个切片如果表示同一个数组的片段，它们可以共享数据；因此一个切片和相关数组的其他切片是共享存储的一个slice由三个部分构成：指针、长度和容量。// 类似type IntSlice struct {    ptr      *int    len, cap int}var x = []int{2, 3, 5, 7, 11}var runes []runex := []int{}var slice1 []type = make([]type, len,[cap])s1 := []int{1, 2, 3}fmt.Println(len(s1), cap(s1), s1) // 输出 3 3 [1 2 3]s2 := s1[1:]fmt.Println(len(s2), cap(s2), s2) // 输出 2 2 [2 3]for i := range s2 {    s2[i] += 20}// s2的修改会影响到数组数据，s1输出新数据fmt.Println(s1) // 输出 [1 22 23]fmt.Println(s2) // 输出 [22 23]s2 = append(s2, 4) // append  导致了slice 扩容for i := range s2 {    s2[i] += 10}// s1 的数据现在是陈旧的老数据，而s2是新数据，他们的底层数组已经不是同一个了。fmt.Println(s1) // 输出[1 22 23]fmt.Println(s2) // 输出[32 33 14]append 操作  slice底层数组是否有足够的容量来保存新添加的元素。如果有足够空间的话，直接扩展slice  没有足够的增长空间的话，会先分配一个足够大的slice用于保存新的结果，先将输入的x复制到新的空间，然后添加y元素和数组不同的是，slice之间不能比较，因此我们不能使用==操作符来判断两个slice是否含有全部相等元素slice唯一合法的比较操作是和nil比较mapmap 是引用类型未初始化的 map 的值是 nilvar map1 map[string]intmap3 := map[string]string{}var map1 = make(map[keytype]valuetype)key可以用 == 或者 != 操作符比较的类型数组、切片和结构体不能作为 key (含有数组切片的结构体不能作为 key，只包含内建类型的struct 是可以作为 key 的)value 可以是任意类型的；通过使用空接口类型，我们可以存储任意值  通过 key 在 map中寻找值是很快的，比线性查找快得多，但是仍然比从数组和切片的索引中直接读取要慢 100倍；所以如果你很在乎性能的话还是建议用切片来解决问题。structtype identifier struct {    field1 type1    field2 type2    ...}new 函数给一个新的结构体变量分配内存，它返回指向已分配内存的指针：var t *T =new(T) 表达式 new(Type) 和 &amp;Type{} 是等价的。匿名成员匿名成员的数据类型必须是命名的类型或指向一个命名的类型的指针type Base struct {	basename string}type Derive struct { // 内嵌 匿名成员	Base	adf int}type Derive1 struct { // 内嵌， 这种内嵌与上面内嵌有差异	*Base	adf int}type Derive2 struct { // 聚合	base Base	adf  int}func main() {    // 匿名嵌入的特性，我们可以直接访问叶子属性而不需要给出完整的路径	a := Derive{}	getType(a.basename)	b := Derive1{}	getType(b.basename)	// 必须显式访问这些叶子成员	c := Derive2{}	getType(c.base.basename)}但是构造时还是要写清楚w = Wheel{Circle{Point{8, 8}, 5}, 20}w = Wheel{    Circle: Circle{        Point:  Point{X: 8, Y: 8},        Radius: 5,    },    Spokes: 20,}实际上，外层的结构体不仅仅是获得了匿名成员类型的所有成员，而且也获得了该类型导出的全部的方法。错误处理自定义错误var ErrNil = errors.New("redigo: nil returned")func Sqrt(f float64) (z float64, err error) {	if f &lt; 0 {		return 0, ErrNil	}	return 0,nil}func main() {		_,err := Sqrt(-1)	if err != nil{		fmt.Print(err.Error())	}}控制结构if            &amp;&amp;、             或 !      if condition {	return x}return yif condition {	// do something	} else {	// do something	}if condition1 {	// do something	} else if condition2 {	// do something else	} else {	// catch-all or default}if value, ok := readData(); ok {…}if err := file.Chmod(0664); err != nil {	fmt.Println(err)	return err}switchfunc main() {	var num1 int = 100	switch num1 {	case 98, 99:		fmt.Println("It's equal to 98")	case 100: 		fmt.Println("It's equal to 100")	default:		fmt.Println("It's not equal to 98 or 100")	}}func main() {	var num1 int = 7	switch {	    case num1 &lt; 0:		    fmt.Println("Number is negative")	    case num1 &gt; 0 &amp;&amp; num1 &lt; 10:		    fmt.Println("Number is between 0 and 10")	    default:		    fmt.Println("Number is 10 or greater")	}}forfor i:=0; i&lt;5; i++ {	for j:=0; j&lt;10; j++ {		println(j)	}}func main() {	var i int = 5	for i &gt;= 0 {		i = i - 1		fmt.Printf("The variable i is now: %d\n", i)	}}函数new make 区别new 和 make 均是用于分配内存：new 用于值类型和用户定义的类型，如自定义结构，  make 用于内置引用类型（切片、map 和通道）。它们的用法就像是函数，但是将类型作为参数：new(type)、make(type)。  new(T) 分配类型 T 的零值并返回其地址，也就是指向类型 T 的指针。它也可以被用于基本类型：v := new(int)。make(T) 返回类型 T 的初始化之后的值，因此它比 new 进行更多的工作。 new()是一个函数，不要忘记它的括号。二者都是内存的分配（堆上），但是make只用于slice、map以及channel的初始化（非零值）；而new用于类型的内存分配，并且内存置为零。函数作为参数package mainimport (    "fmt")func main() {    callback(1, Add)}func Add(a, b int) {    fmt.Printf("The sum of %d and %d is: %d\n", a, b, a+b)}func callback(y int, f func(int, int)) {    f(y, 2) // 实际上是 Add(1, 2)}闭包package mainimport (    "fmt")func addNumber(x int) func(int) {    fmt.Printf("x: %d, addr of x:%p\n", x, &amp;x)    return func(y int) {        k := x + y        x = k        y = k        fmt.Printf("x: %d, addr of x:%p\n", x, &amp;x)        fmt.Printf("y: %d, addr of y:%p\n", y, &amp;y)    }}func main() {    addNum := addNumber(5)    addNum(1)    addNum(1)    addNum(1)    fmt.Println("---------------------")    addNum1 := addNumber(5)    addNum1(1)    addNum1(1)    addNum1(1)}可变参数参数列表的最后一个参数类型之前加上省略符号...，这表示该函数会接收任意数量的该类型参数func sum(vals...int) int {    total := 0    for _, val := range vals {        total += val    }    return total}fmt.Println(sum())           // "0"fmt.Println(sum(3))          // "3"fmt.Println(sum(1, 2, 3, 4)) // "10"values := []int{1, 2, 3, 4}fmt.Println(sum(values...)) // "10"测试测试程序必须属于被测试的包 文件名 *_test.go  不会被普通的 Go 编译器编译，所以当放应用部署到生产环境时它们不会被部署；只有 Gotest 会编译所有的程序：普通程序和测试程序必须导入 testing 包，并写一些名字以 TestZzz 打头的全局函数import (	"fmt"	"testing")func n() { fmt.Println(a) }func m() {	a := "O"	fmt.Println(a)}func TestA(t *testing.T) {	n()	m()	n()}func TestB(t *testing.T) {	n()	m()	n()}]]></content>
      <categories>
        
          <category> go </category>
        
      </categories>
      <tags>
        
          <tag> go </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[go pkg]]></title>
      <url>/blog/2019/07/27/go-pkg</url>
      <content type="text"><![CDATA[what每个 Go 文件都属于且仅属于一个包。一个包可以由许多以 .go 为扩展名的源文件组成 通常目录名package main表示一个可独立执行的程序，每个 Go 应用程序都包含一个名为 main 的包。package main包下可以有多个文件，但所有文件中只能有一个main()方法，main()方法代表程序入口。导入import (    "crypto/rand"    mrand "math/rand" // alternative name mrand avoids conflict)import (    _ "github.com/revel/modules/testrunner/app"    _ "guild_website/app")_操作其实是引入该包，而不直接使用包里面的函数，而是调用了该包里面的init函数。本包在本包范围内 定义或声明 0个或多个常量（const）、变量（var）和类型（type），这些对象的作用域都是全局的导出首字母大写才能在包外用项目结构GOPATH/    src/        gopl.io/            .git/            ch1/                helloworld/                    main.go                dup/                    main.go                ...        golang.org/x/net/            .git/            html/                parse.go                node.go                ...    bin/        helloworld        dup    pkg/        darwin_amd64/        ...  $GOPATH/src 存储源代码  $GOPATH/src/工程名  pkg子目录用于保存编译后的包的目标文件  bin子目录用于保存编译后的可执行程序  GOROOT用来指定Go的安装目录 自带的标准库包的位置 目录结构和GOPATH类似内部包internal包的内容只能被同一个父目录的包导入  net/http/internal/chunked内部包只能被net/http/httputil或net/http包导入，但是不能被net/url包导入。不过net/url包却可以导入net/http/httputil包。vendor在执行 go build 或 go run 命令时，会按照以下顺序去查找包：  当前包下的 vendor 目录  向上级目录查找，直到找到 src 下的 vendor 目录  在 GOROOT 目录下查找  在 GOPATH 下面查找依赖包cd $GOPATH/src/工程名/vendorvendor 参数程序启动流程  程序的初始化和执行都起始于main包  main包还导入了其它的包，那么就会在编译时将它们依次导入。有时一个包会被多个包同时导入，那么它只会被导入一次  当一个包被导入时，如果该包还导入了其它的包，那么会先将其它包导入进来，然后再对这些包中的包级常量和变量进行初始化  执行init函数（如果有的话）  等所有被导入的包都加载完毕了，就会开始对main包中的包级常量和变量进行初始化  执行main包中的init函数（如果存在的话），最后执行main函数。关于init  init函数是用于程序执行前做包的初始化的函数，比如初始化包里的变量等  每个包可以拥有多个init函数  包的每个源文件也可以拥有多个init函数  同一个包中多个init函数的执行顺序Go语言没有明确的定义(说明)  不同包的init函数按照包导入的依赖关系决定该初始化函数的执行顺序  init函数不能被其他函数调用，而是在main函数执行之前，自动被调用命令getgo get命令获取的代码是真实的本地存储仓库，而不仅仅只是复制源文件go get -u命令只是简单地保证每个包是最新版本go get github.com/golang/lint/golint做精确的版本依赖管理,使用vendor的目录用于存储依赖包的固定版本的源代码，对本地依赖的包的版本更新也是谨慎和持续可控的build命令编译命令行参数指定的每个包 检测包是可以正确编译 不会重新编译没有发生变化的包，这可以使后续构建更快捷包的名字是main，go build将调用链接器在当前目录创建一个可执行程序；以导入路径的最后一段作为可执行程序的名字cd 到main.go目录go buildgo build -i命令将安装每个目标所依赖的包。go run命令实际上是结合了构建和运行的两个步骤install和go build命令很相似，但是它会保存每个包的编译成果，而不是将它们都丢弃。被编译的包会被保存到$GOPATH/pkg目录下，目录路径和src目录路径对应，可执行程序被保存到$GOPATH/bin目录。]]></content>
      <categories>
        
          <category> go </category>
        
      </categories>
      <tags>
        
          <tag> go </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概念]]></title>
      <url>/blog/2019/06/28/rockdb-base</url>
      <content type="text"><![CDATA[what is rocksdb基于google leveldb 持久化kv存储，Keys and values are arbitrary bytearrays. 任意大小字节流。(value size must be smaller than 4GB) Log Structured Database Engine for storage，  专为希望在本地或远程存储系统上存储多达数TB数据的应用程序服务器而设计。  优化用于在快速存储 - 闪存设备或内存中存储中小尺寸键值  它适用于具有多个内核的处理器基本结构logfile，memtable, sstfilememtable 是一个内存数据结构，新写入的数据被插入到 memtable 中，并可选地写入日志文件。日志文件是存储上顺序写入的文件。当 memtable 填满时，它被 flush 到存储上的 sstfile ，然后可以被安全地删除。sstfile 中的数据顺序存放，以方便按 key 进行查找。datebase数据库名对应于文件系统的目录的名称，数据库的所有内容都存储该目录下RocksDB Options可以随时更改 can be changed dynamically while DB isrunning, automatically keeps options used in the database in OPTIONS-xxxx files under the DB directory.create rocksdb 坑import rocksdb_CONFIG = CONFIG['databases']['rocksdb']_DB_LOG = {    'db_log_dir': _CONFIG['log_dir'],    'max_log_file_size': _CONFIG['max_log_size'],    'keep_log_file_num': _CONFIG['max_log_num'],}def create_rocksdb_client(db_name: str, read_only: bool = True) -&gt; rocksdb.DB:    # 不能放在外面 [fix] Exception: Options object is already used by another DB    _READONLY_OPTIONS = rocksdb.Options(        create_if_missing=True,        **_DB_LOG,    )    _WRITABLE_OPTIONS = rocksdb.Options(        create_if_missing=True,        max_background_compactions=4,        max_background_flushes=4,        allow_mmap_writes=True,        compression=rocksdb.CompressionType.lz4_compression,        **_DB_LOG,    )    if read_only:        return rocksdb.DB(os.path.join(_CONFIG['data_dir'], db_name), _READONLY_OPTIONS, read_only=read_only)    return rocksdb.DB(os.path.join(_CONFIG['data_dir'], db_name), _WRITABLE_OPTIONS, read_only=read_only)  只有 read_only=True rocksdb 可以多开 不然就会报IO Error Lock: No locks available  Options object is already used by another DB一个程序里面一个option貌似只能给一个db  设置了read_only=True，create_if_missing=True就没什么鬼用。原子操作batch = rocksdb.WriteBatch()batch.put(b"key", b"v1")batch.delete(b"key")batch.put(b"key", b"v2")batch.put(b"key", b"v3")db.write(batch)WriteBatch 有序执行写入默认情况下，每次写入rocksdb都是异步的：它在将进程写入操作系统后返回。从操作系统内存到底层持久存储的转移是异步发生的。sync可以为特定写入打开该标志，以使写入操作不会返回，直到正在写入的数据一直被推送到持久存储。对于非同步写入 RocksDB only buffers WAL write in OS buffer or internalbuffer 它们通常比同步写入快得多。非同步写入的缺点是机器崩溃可能导致最后几次更新丢失。]]></content>
      <categories>
        
          <category> rocksdb </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[flake8 + pycharm]]></title>
      <url>/blog/2019/06/12/flake8</url>
      <content type="text"><![CDATA[配置:pip install flake8当前使用的Interpreter$PyInterpreterDirectory$/pythonArguments:-mflake8--show-source--statistics--max-line-length=130--exclude.env,tests--ignoreE501,W503$ProjectFileDir$exclude 忽略目录 ignore 忽略的错误 $ProjectFileDir$ 当前项目所在目录使用]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Ubuntu 下docker 部署]]></title>
      <url>/blog/2019/06/12/deploy-docker</url>
      <content type="text"><![CDATA[安装apt-get install -y docker.io nginx docker-compose systemctl start dockersystemctl enable docker# 开机启动systemctl enable nginxsystemctl start nginx systemctl stop nginx systemctl restart nginx配置启动 servicecd etc/systemd/system/[Unit]Description={xxxxx}After=network.targetAfter=docker.service[Service]Type=simpleUser=rootExecStart={where is docker-compose} upWorkingDirectory={project}Restart=on-failureRestartSec=15StandardOutput=syslogStandardError=syslog[Install]WantedBy=multi-user.target# Service 状态systemctl status endoscope# 启动失败 查看logjournalctl -u xxxx.service -f# 更新xxx.servicesystemctl daemon-reload]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker remote debug]]></title>
      <url>/blog/2019/06/07/docker_debug</url>
      <content type="text"><![CDATA[Pycharm local debug dockerBuild, Execution, DeploymentPython Interpreter        基本弄好这个就可以下断点docker logupdate project重构 修改的部分docker-compose -f .debug.docker-compose.yml up --build -dcompose configcompose services host nameversion: '3'services:  endoscope-web:    build:      context: .      dockerfile: .debug.Dockerfile    depends_on:      - mysql      - redis    ports:      - 9000:9000    privileged: true    volumes:      - ./.etc/localtime:/etc/localtime      - ./conf:/endoscope/conf    restart: on-failure  mysql:    image: mysql:5.7    environment:      MYSQL_ROOT_PASSWORD: ""      MYSQL_USER: ""      MYSQL_PASSWORD: ""      MYSQL_DATABASE: ""    volumes:      - ./.etc/localtime:/etc/localtime      - ./sql/create.sql:/docker-entrypoint-initdb.d/create.sql      - mysql_data:/var/lib/mysql    ports:      - 3306:3306    restart: on-failure  redis:    image: redis    command: redis-server --appendonly yes    volumes:      - redis_data:/data    ports:      - 6379:6379    restart: on-failure]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[sqlalchemy 坑]]></title>
      <url>/blog/2019/05/25/sqlalchemy_basic</url>
      <content type="text"><![CDATA[model 必须要主键在我看来ORM底层就好好的拼字符串sql搞什么奇葩设定 How do I map a table that has no primary key大多数ORM要求对象定义某种主键，因为内存中的对象必须对应于数据库表中唯一可识别的行; 至少，这允许对象可以作为UPDATE和DELETE语句的目标，这些语句将仅影响该对象的行而不影响其他行。但是，主键的重要性远不止于此。在SQLAlchemy中，所有ORM映射对象始终Session 使用称为身份映射的模式在其特定数据库行中唯一链接，该模式是SQLAlchemy使用的工作单元系统的核心，也是最关键的模式。 ORM使用的常见（而不是那么常见）模式。insertCREATE TABLE `user` (  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,  `name` varchar(120) NOT NULL,  `age` int(10) unsigned NOT NULL DEFAULT '0',  `ts` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  PRIMARY KEY (`id`))modelclass User(BaseModel):    __tablename__ = 'user'    id = Column(INTEGER(unsigned=True), primary_key=True)    name = Column(VARCHAR(120))    age = Column(INTEGER(unsigned=True))    ts = Column(TIMESTAMP())add一股傻逼气息扑面而来 Cannot insert NULL value incolumn, but I have a default value specifiedsession.add(User(name='faf'))ERRORsqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (1048, "Column 'age' cannot be null")执行的是这个sql 坑爹啊INSERT INTO user (name, age, ts) VALUES (%s, %s, %s)('faf', None, None)speed upsqlalchemyのinsert高速化session.execute(User.__table__.insert(), {'name': 'affa'})# 批量insertsession.bulk_save_objects([User(name='bulko') for i in range(0, 5)])# 推荐写法data = [{'name': 'bulk', 'age': i} for i in range(0, 5)]session.execute(User.__table__.insert(), data)INSERT INTO user (name) VALUES (%s)('affa',)INSERT INTO user (name, age) VALUES (%s, %s)(('bulk', 0), ('bulk', 1), ('bulk', 2), ('bulk', 3), ('bulk', 4))on_duplicate_key_updateonly for mysql insert-on-duplicate-key-update-upsertupdate or insert by unique keyfrom sqlalchemy.dialects.mysql import insert, Insertdef upsert(model: declarative_base, data: typing.Union[typing.Dict, typing.List[typing.Dict]],           update_field: typing.List) -&gt; Insert:    """    on_duplicate_key_update for mysql    """    # https://docs.sqlalchemy.org/en/13/dialects/mysql.html#insert-on-duplicate-key-update-upsert    stmt = insert(model).values(data)    d = {f: getattr(stmt.inserted, f) for f in update_field}    return stmt.on_duplicate_key_update(**d)data = [dict(name=f'bulko{i}', age=i) for i in range(0, 10)]stmt = upsert(User, data, update_field=list(data[0].keys()))session.execute(stmt)session.commit()INSERT INTO user (name, age) VALUES (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s), (%s, %s) ON DUPLICATE KEY UPDATE name = VALUES(name), age = VALUES(age)('bulko0', 0, 'bulko1', 1, 'bulko2', 2, 'bulko3', 3, 'bulko4', 4, 'bulko5', 5, 'bulko6', 6, 'bulko7', 7, 'bulko8', 8, 'bulko9', 9)要想再复杂点马上就跪了i = 11data = dict(name=f'bulko{i}', age=i)stmt = insert(User).values(data)stmt = stmt.on_duplicate_key_update({    'age': 'age + VALUES(age)',})session.execute(stmt)session.commit()ERRORsqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (1366, "Incorrect integer value: 'age + VALUES(age)' for column 'age' at row 1")[SQL: INSERT INTO user (name, age) VALUES (%s, %s) ON DUPLICATE KEY UPDATE age = %s][parameters: ('bulko11', 11, 'age + VALUES(age)')]然后只能 先select 在update 超傻逼for d in data:    inner_machine_id = d['inner_machine_id']    risk_machine = RiskInnerMachine.get_by_inner_machine_id(db, inner_machine_id)    if risk_machine:        RiskInnerMachine.update_by_inner_machine_id(db, inner_machine_id, {            RiskInnerMachine.is_risk: risk_machine.is_risk if risk_machine.is_risk == 1 else d['is_risk'],            RiskInnerMachine.threat_access: risk_machine.threat_access + d['threat_access'],            RiskInnerMachine.risk_file: risk_machine.risk_file + d['risk_file'],            RiskInnerMachine.attack_event: risk_machine.attack_event + d['attack_event'],            RiskInnerMachine.access_rule: risk_machine.access_rule + d['access_rule'],            RiskInnerMachine.update_time: d['update_time'],        })    else:        d['fall_type'] = 0        d['ignore_time'] = 0        db.add(RiskInnerMachine(**d))使用 compilesCustom SQL Constructs and Compilation ExtensionSQLAlchemy ON DUPLICATE KEY UPDATEfrom sqlalchemy.ext.compiler import compiles# 这个import很关键from sqlalchemy.sql.expression import Insert@compiles(Insert, 'mysql')def on_duplicate_key_update(insert, compiler, **kw):    def _gen_fv_dict(fv):        sql = []        if isinstance(fv, dict):            for f, v in fv.items():                sql.append(f' {f} = {v} ')        elif isinstance(fv, list):            for f in fv:                sql.append(f' {f} = VALUES({f}) ')        return ','.join(sql)    s = compiler.visit_insert(insert, **kw)    if 'on_duplicate_key_update' in insert.kwargs:        return s + ' ON DUPLICATE KEY UPDATE ' + _gen_fv_dict(insert.kwargs['on_duplicate_key_update'])    return s效果拔群，但是会拖慢正常的insert,拖慢很多没用compiles插1000个，用0.138sec ，用了0.624sec 1000个0.540一个。。。。test codeclass User(BaseModel):    __tablename__ = 'user'    id = Column(INTEGER(unsigned=True), primary_key=True)    name = Column(VARCHAR(120))    age = Column(INTEGER(unsigned=True))    data = [{'name': f'bulk{i}', 'age': i} for i in range(0, 1000)]def test1():    # 0.138 0.624  0.540    with mysql() as db:        db.execute(User.__table__.insert(), data[0])        db.commit()def test2():    # 0.253 0.731    with mysql() as db:        db.execute(insert(User).values(data))        db.commit()def test3():    with mysql() as db:        db.add_all(data)        db.commit()import profileprofile.run('test1()')# profile.run('test2()')# profile.run('test3()')all() &amp; first() &amp; scalar()只要一个object直接first() 多个用all() 数字用# SELECT user.name AS user_name, user.age AS user_age FROM user          return &lt;class 'list'&gt;:[....]user = session.query(User.name, User.age).all()# SELECT user.name AS user_name, user.age AS user_age FROM user LIMIT 1  return  ('bulko0', 0)user = session.query(User.name, User.age).first()# SELECT user.name AS user_name, user.age AS user_age FROM user LIMIT 1  return  &lt;class 'list'&gt;: [('bulko0', 0)]user = session.query(User.name, User.age).limit(1).all()# SELECT user.name AS user_name, user.age AS user_age FROM user LIMIT 1  return  ('bulko0', 0)user = session.query(User.name, User.age).limit(1).first()# SELECT user.name AS user_name, user.age AS user_age FROM user LIMIT 1  return  'bulko0'user = session.query(User.name, User.age).limit(1).scalar()找不到目标user_id = 100# []user = session.query(User.name, User.age).filter(User.id == user_id).all()# Noneuser = session.query(User.name, User.age).filter(User.id == user_id).first()user = session.query(User.name, User.age).filter(User.id == user_id).limit(1).all()user = session.query(User.name, User.age).filter(User.id == user_id).limit(1).first()user = session.query(User.name, User.age).filter(User.id == user_id).limit(1).scalar()# 0user = session.query(func.count('*')).filter(User.id == user_id).scalar()# (0,)user = session.query(func.count('*')).filter(User.id == user_id).first()常用gistsqlalchemy object to list of dictdef sqlalchemy2dict(result: typing.List[_LW]) -&gt; Results:    return [r._asdict() for r in result]聚合# countselect = session.query(func.count(    distinct(cls.id)).label('count'))count = get_wheres(select).scalar()# queryselect = session.query(    func.group_concat(distinct(cls.aa)).label('aa'),    func.any_value(cls.id).label('id'),    func.sum(cls.count).label('count'))query = get_wheres(select).group_by(cls.signature_id).offset(    (page - 1) * page_size).limit(page_size).all()return count, sqlutil.sqlalchemy2dict(query)query = session.query(            # sum -&gt; decimal to int            func.sum(cls.count).op('div')(1).label('count'),            func.Hour(func.FROM_UNIXTIME(cls.create_time)).label('hour'),        ).filter(            (start_time &lt;= cls.create_time) &amp; (cls.create_time &lt;= end_time)        ).group_by(            'sensorid', 'hour'  # 自定义label        ).order_by('hour')        return query.all()and or 拼接or_list = []for ip_range in ip_range_list:    min_ip, max_ip = int(min(ip_range)), int(max(ip_range))    or_list.append(and_(min_ip &lt;= xx.ip4, xx.ip4 &lt;= max_ip))query = query.filter(or_(*or_list))joindef get_wheres(query: Query) -&gt; Query:    query = query.join(xx, cls.id == xx.id)    query = query.filter(xx.ip == ip)    return query# 列表select = session.query(cls.id, (cls.c &gt; cls.a).label('ignore'))query = get_wheres(select).limit(1)c = query.first()if c is None:    return False, Falsereturn True, c.ignore子查询 .subquery()sub = session.query(func.any_value(stat_column).label('name')) \    .join(xx, xx.id == cls.id) \    .filter((start_ts &lt;= xx.date) &amp; (xx.date &lt;= end_ts)) \    .group_by(cls.md5).subquery()query = session.query('name', func.count('*').label('count')) \    .select_from(sub) \    .group_by('name') \    .order_by(desc('count'))模糊查询可以用 mysql concat query =query.filter(func.concat(*columns).like(f'%{keyword}%')) 将字段连起来查询但是会有造成以下情况 kw 关键字 f 字段kw = ab f1 = a f2=b 是匹配的 ,当然f1f2中间可以塞其他特殊字符作为连接符，但是很难保证kw输入什么。关于转义在mysql中，反斜杠在字符串中是转义字符，在进行语法解析时会进行一次转义，所以当我们在insert字符时，insert \\在数据库中最终只会存储\。而在mysql的like语法中，like后边的字符串除了会在语法解析时转义一次外，还会在正则匹配时进行第二次的转义。因此如果期望最终匹配到\，就要反转义两次，也即由\到\\再到\\\\。def search_keyword(query: Query, keyword: str, columns: Columns) -&gt; Query:    if keyword:        # 路经关键字查询反斜杠转义        keyword = keyword.replace('\\', '\\\\')        if isinstance(columns, typing.Sequence):            or_list = []            for c in columns:                or_list.append(c.like(f'%{keyword}%'))            query = query.filter(or_(*or_list))        else:            query = query.filter(columns.like(f'%{keyword}%'))    return query动态filterclass FILTER_OP(object):    EQ = "="    NE = "!="    GT = "&gt;"    LT = "&lt;"    LIKE = "like"    NOT_LIKE = "not like"    # https://docs.sqlalchemy.org/en/13/orm/internals.html#sqlalchemy.orm.properties.ColumnProperty.Comparator    NAME_DICT = {        EQ: 'eq',        NE: 'ne',        GT: 'gt',        LT: 'lt',        LIKE: 'like',        NOT_LIKE: 'notlike',    }    ALL = list(NAME_DICT.keys())'''raw_filter jsonschema 结构    'filter': {            'type': 'array',            'items': {                'type': 'object',                'required': ['key', 'op', 'value'],                "properties": {                    "key": {"type": "string", "enum": 条件字段},                    "op": {"type": "string", "enum": const.FILTER_OP.ALL},                    "value": {},                }            }        },'''def sql_filter(query: Query, model: declarative_base, raw_filter: typing.List[typing.Dict]) -&gt; Query:    """    动态filter and 连接    const.FILTER_OP 控制操作    """    if raw_filter:        for raw in raw_filter:            key, op, value = raw['key'], const.FILTER_OP.NAME_DICT[raw['op']], raw['value']            column = getattr(model, key)            attr = list(filter(lambda e: hasattr(column, e % op), ['%s', '%s_', '__%s__']))[0] % op            query = query.filter(getattr(column, attr)(value))    return query时间间隔统计query = session.query(            func.count('*').label('count'),            cls.timestamp.op('div')(duration).label('time')        ).filter(            (start_ts &lt;= cls.timestamp) &amp; (cls.timestamp &lt;= end_ts)        ).group_by('time').order_by('time')        result = []        record = query.all()        for r in record:            result.append({                'timestamp': r.time * duration,                'count': r.count,            })        return result                def fill_timestamp_count(start_ts, end_ts: int, interval: int, raw_result: Results) -&gt; Results:    """补全缺失的时间分布"""    if not raw_result:        return []    begin = start_ts // interval * interval    end = end_ts // interval * interval    raw_result = {r['timestamp']: r for r in raw_result}    result = {t: {'timestamp': t, 'count': 0} for t in range(begin, end + interval, interval)}    for r in raw_result:        result[r] = raw_result[r]    return sorted(result.values(), key=itemgetter('timestamp'))]]></content>
      <categories>
        
          <category> sqlalchemy </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ip 子网]]></title>
      <url>/blog/2019/05/19/ip-%E5%AD%90%E7%BD%91</url>
      <content type="text"><![CDATA[IP段  A类IP段　 0.0.0.0 到127.255.255.255  B类IP段　128.0.0.0 到191.255.255.255  C类IP段　192.0.0.0 到223.255.255.255默认分配的子网掩码每段只有255或0  Ａ类的默认子网掩码　255.0.0.0　　　　　一个子网最多可以容纳1677万多台电脑  Ｂ类的默认子网掩码　255.255.0.0　　　　一个子网最多可以容纳6万台电脑  Ｃ类的默认子网掩码　255.255.255.0　　　一个子网最多可以容纳254台电脑  子网掩码 &amp; IP地址=网络标识  网络标识一样，那么属于同一网段ip/num/num 是子网掩码表示方法 表示1的个数192.168.0.0/24 =&gt; 11000000.10101000.00000000.00000000/24ip有13位 num 取值范围[13,32]  总个数 2**(32-num)net = 12network = ip_range(f'192.168.0.0/{net}')ip2bin = lambda ip: '.'.join([bin(int(x) + 256)[3:] for x in ip.split('.')])print(ip2bin('192.168.0.0'))&gt;&gt;&gt; 11000000.10101000.00000000.00000000e = list(network)print(str(min(e)), str(max(e)))&gt;&gt;&gt; 192.168.0.0 192.175.255.255n = 2**(32-net)l = len(e)assert n == l]]></content>
      <categories>
        
          <category> network </category>
        
      </categories>
      <tags>
        
          <tag> network </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[gist]]></title>
      <url>/blog/2019/05/19/gist</url>
      <content type="text"><![CDATA[decoratordef func_time(log=None, tag=None):    def deco(old_handler):        @wraps(old_handler)        def new_handler(*args, **kwargs):            if not config.debug:                return old_handler(*args, **kwargs)            start = time.time()            result = old_handler(*args, **kwargs)            end = time.time()            msg = "Total time running [%s]: %s seconds" % (                old_handler.__name__ if tag is None else tag, str(end - start))            if log:                log(msg)            else:                print(msg)            return result        return new_handler    return decodecorator in class methodclass retry(object):    def __init__(self, times: int):        self.times = times    def __call__(self, func):        def new_handler(*args, **kwargs):            if self.times == -1:                while True:                    try:                        is_ok = func(*args, **kwargs)                        if is_ok:                            break                    except Exception:                        pass            elif self.times &gt; 0:                for i in range(self.times):                    try:                        is_ok = func(*args, **kwargs)                        if is_ok:                            break                    except Exception:                        pass        new_handler.__doc__ = func.__doc__        return new_handlerclass Test(object):    def _decorator(foo):        def magic(self):            print("start magic")            foo(self)            print("end magic")        return magic    @retry(times=3)    def bar(self):        print("normal call")class Test(object):    def retry(times=3):        """        times == -1 forever        :return:        """        def deco(func):            def new_handler(*args, **kwargs):                retry_time = 1                if times == -1:                    while True:                        try:                            is_ok = func(*args, **kwargs)                            if is_ok:                                break                            print(f'retry[{retry_time}]:{Duration.get_time(args[2]).to_str()}')                            retry_time +=1                        except Exception:                            pass                elif times &gt; 0:                    for i in range(times):                        try:                            is_ok = func(*args, **kwargs)                            if is_ok:                                break                            print(f'retry[{retry_time}]:{Duration.get_time(args[2]).to_str()}')                            retry_time += 1                        except Exception:                            pass            return new_handler        return deco    @retry(times=-1)    def download_img(self, station_num: int, time: float, rewrite=False):        path = f'afreecatv/{station_num}/{Duration.get_time(time).to_str()}.jpg'        if not rewrite and os.path.exists(path):            return True        param = self.get_thumbnail_param(station_num, time)        is_ok = util.down_img(self.THUMBNAIL_URL, param, path)        if not is_ok:            print(path)        return is_okwith@contextmanagerdef with_func_time(tag, log=None, is_log=False):    if config.debug or is_log:        start = time.time()    try:        yield    finally:        if config.debug or is_log:            end = time.time()            msg = "Total time running %s: %s seconds" % (tag, str(end - start))            if log:                log(msg)                return            print(msg)@contextmanagerdef transaction(conn):    """    Automatic handle transaction COMMIT/ROLLBACK. You MUST call trans.finish(),    if you want to COMMIT; Otherwise(not call or exception occurs), ROLLBACK.    &gt;&gt;&gt; with transaction(conn) as trans:    &gt;&gt;&gt;     do something...    &gt;&gt;&gt;     if xxxxx:    &gt;&gt;&gt;         # if you don't want to commit, you just not call trans.finish().    &gt;&gt;&gt;         return error_page("xxxxxx")    &gt;&gt;&gt;     # if you want to commit, you call:    &gt;&gt;&gt;     trans.finish()    @param conn: database connection    """    trans = Job()    conn.begin()    try:        yield trans    except:        conn.rollback()        raise    if trans.is_finished():        conn.commit()    else:        conn.rollback()]]></content>
      <categories>
        
          <category> 奇技淫巧 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Swarm]]></title>
      <url>/blog/2019/05/11/docker_swarm</url>
      <content type="text"><![CDATA[概述由多个Docker主机组成，这些主机以群集模式运行，一个主机可以充当managers，workers或者同时扮演两个角色。manager node 指派task to worker nodes,负责集群管理，工作调度。worker nodes执行被委派的task, 默认情况下manager node也会作为worker，可配置仅执行manage task。worker node会汇报工作状态到manager node.task 包含指定的image and 有执行命令的container 是原子的一旦分配 不能转移到另一个nodeSwarm模式有一个内部DNS组件，可以自动为swarm中的每个服务分配一个DNS条目。群集管理器使用内部负载平衡来根据服务的DNS名称在群集内的服务之间分发请求。集群中的每个节点都强制执行TLS相互身份验证和加密，以保护自身与所有其他节点之间的通信。您可以选择使用自签名根证书或自定义根CA的证书。        setadd managerdocker-machine ls     /   docker-machine ip &lt;MACHINE-NAME&gt;指定manager# 使用名为的机器manager1。如果您使用Docker Machine，则可以使用以下命令通过SSH连接到它：docker-machine ssh manager1# 创建新的swarm  得到token 让work joindocker swarm init --advertise-addr &lt;MANAGER-IP&gt;节点信息docker node lsadd work在 work node 也run sshdocker swarm join \  --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \  192.168.99.100:2377  # 在manage node 上获得命令docker swarm join-token workerset taskdocker service create --replicas 1 --name helloworld alpine ping docker.com  该docker service create命令创建服务。  该–name标志命名该服​​务helloworld。  该–replicas标志指定1个正在运行的实例的所需状态。  参数alpine ping docker.com将服务定义为执行命令的Alpine Linux容器ping docker.com。docker service ls]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Docker Compose]]></title>
      <url>/blog/2019/05/11/docker_compose</url>
      <content type="text"><![CDATA[概述Compose是一个用于定义和运行多容器Docker应用程序的工具。使用Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，您可以从配置中创建并启动所有服务。使用Compose基本上是一个三步过程：  定义您的应用程序环境，Dockerfile以便可以在任何地方进行复制。  定义构成应用程序的服务，docker-compose.yml 以便它们可以在隔离的环境中一起运行。  Run docker-compose up和Compose启动并运行整个应用程序。特征  单个主机上的多个隔离环境  创建容器时保留卷数据  仅重新创建已更改的容器  变量和在环境之间移动合成writeDockerfileFROM python:3.4-alpineCOPY . /codeWORKDIR /codeRUN pip install -r requirements.txtCMD ["python", "app.py"]docker-compose.ymlversion: '3'services:  web:    build: .    ports:     - "5000:5000"  redis:    image: "redis:alpine"或者 允许您动态修改代码，而无需重建映像。version: '3'services:  web:    build: .    ports:     - "5000:5000"    volumes:     - .:/code  redis:    image: "redis:alpine"Compose文件定义了两个服务：web和redis。从项目目录中，键入docker-compose up [-d 后台模式]docker-compose.yml 配置参考cmd查看docker-compose ps停止docker-compose stopdocker-compose --help增量更新服务stop remove build startdocker-compose -f .debug.docker-compose.yml up --builddocker-compose exec &lt;service-name&gt; bashdocker-compose logs -f &lt;service-name&gt;]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker 常用]]></title>
      <url>/blog/2019/05/11/docker_common_cmd</url>
      <content type="text"><![CDATA[builddocker build -t &lt;仓库名 URL等&gt;:&lt;new_标签&gt; &lt;上下文路径/URL&gt; -f &lt;Dockerfile path&gt;使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1docker build -t runoob/ubuntu:v1 .manage所有的容器 IDdocker ps -aq停止所有的容器docker stop $(docker ps -aq)删除所有的容器docker rm $(docker ps -aq)删除所有的镜像docker rmi $(docker images -q)删除stop containerdocker container prune删除dangling image docker image prunerun/exec  t:在新容器内指定一个伪终端或终端  i:对容器内的标准输入  d:后台run  p:端口绑定查看docker ps -adocker logs -f [container ID or NAMES]docker statsdocker info交互docker exec -it &lt;container-name&gt; bash]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概念]]></title>
      <url>/blog/2019/05/05/clickhouse-base</url>
      <content type="text"><![CDATA[what is OLAP  大多数是读请求  数据总是以相当大的批(&gt; 1000 rows)进行写入  不修改已添加的数据  每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列  宽表，即每个表包含着大量的列  较少的查询(通常每台服务器每秒数百个查询或更少)  对于简单查询，允许延迟大约50毫秒  列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节)  处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）  事务不是必须的  对数据一致性要求低  每一个查询除了一个大表外都很小  查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中why clickhouse            what is clickhouseClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)        对于存储而言，列式数据库总是将同一列的数据存储在一起，不同列的数据也总是分开存储。优点  每秒几亿行的吞吐能力  数据可以持续不断高效的写入到表中，并且写入的过程中不会存在任何加锁的行为。  为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理。  支持SQL  多核心并行处理 多服务器分布式处理  异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据缺点  仅能用于批量删除或修改数据 每次写入不少于1000行的批量写入，或每秒不超过一个写入请求  没有完整的事务支持  稀疏索引使得ClickHouse不适合通过其键检索单行的点查询。IO  针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。  由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了I/O的体积。  由于I/O的降低，这将帮助更多的数据被系统缓存。解压缩的速度主要取决于未压缩数据的大小。MergeTree表由按主键排序的数据片段 组成。当数据被插入到表中时，会分成数据片段并按主键的字典序排序。例如，主键是 (CounterID, Date) 时，片段中数据按 CounterID 排序，具有相同 CounterID 的部分按 Date 排序。不同分区的数据会被分成不同的片段，ClickHouse 在后台合并数据片段以便更高效存储。不会合并来自不同分区的数据片段。这个合并机制并不保证相同主键的所有行都会合并到同一个数据片段中。ClickHouse 会为每个数据片段创建一个索引文件，索引文件包含每个索引行（标记）的主键值。索引行号定义为 n * index_granularity 。最大的 n 等于总行数除以 index_granularity 的值的整数部分。对于每列，跟主键相同的索引行处也会写入标记。这些标记让你可以直接找到数据所在的列。你可以只用一单一大表并不断地一块块往里面加入数据 – MergeTree 引擎的就是为了这样的场景。ClickHouse 不要求主键唯一。所以，你可以插入多条具有相同主键的行。                视图普通视图与物化视图CREATE MATERIALIZED VIEW kafka_attack_event_consumer TO attack_event	AS SELECT toDate(toDateTime(timestamp)) AS date, *	FROM kafka_attack_event;      普通视图不存储任何数据，只是执行从另一个表中的读取。换句话说，普通视图只是保存了视图的查询，当从视图中查询时，此查询被作为子查询用于替换FROM子句。        物化视图存储的数据是由相应的SELECT查询转换得来的。  Kafka 引擎CREATE TABLE queue (    timestamp UInt64,    level String,    message String  ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow');CREATE TABLE daily (day Date,level String,total UInt64) ENGINE = SummingMergeTree(day, (day, level), 8192);CREATE MATERIALIZED VIEW consumer TO dailyAS SELECT toDate(toDateTime(timestamp)) AS day, level, count() as totalFROM queue GROUP BY day, level;SELECT level, sum(total) FROM daily GROUP BY level;  使用引擎创建一个 Kafka 消费者并作为一条数据流。  创建一个结构表。  创建物化视图，改视图会在后台转换引擎中的数据并将其放入之前创建的表中。当 MATERIALIZED VIEW 添加至引擎，它将会在后台收集数据。可以持续不断地从 Kafka 收集数据并通过 SELECT 将数据转换为所需要的格式。配置]]></content>
      <categories>
        
          <category> clickhouse </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[js 反爬]]></title>
      <url>/blog/2019/04/26/js</url>
      <content type="text"><![CDATA[sign 计算js debug xhrexecjsimport execjsnode = execjs.get()def js_file(path):    o = open(path).read()    return node.compile(o)    def js_sha(param):    js_path = '../js/sha1.min.js'    ctx = js_file(js_path)    # ctx.eval("sha1 = require('js-sha1')")    return ctx.call('sha1', param)def t(param):    cmd = """    Math.floor(({} - 99) / 99)    """.format(param)    return node.eval(cmd)    def page(page_num):    sess = requests.session()    sess.headers = headers = {        # 'User-Agent': random.choice(config.HEADER.USER_AGENT),        'User-Agent': ua.random, }    # sess.cookies = cookies    resp = sess.get(url=URL.format(page_num), cookies=cookies)    soup = BeautifulSoup(resp.text, "lxml")    ele = soup.find('div', class_='container', p=True)    p = ele['p']    ts = t(ele['t'])    # sign = sha('Xr0Z-javascript-obfuscation-1{}'.format(ts).encode())    # p = page_num    # ts = int(time.time())    sign = sha('Xr0Z-javascript-obfuscation-1{}'.format(ts).encode())    param = {        'page': p,        't': ts,        'sign': sign,    }    # sess.headers.update({    #     # 'Referer': URL.format(page_num),    #     'Referer': 'http://glidedsky.com/level/web/crawler-javascript-obfuscation-1',    #     'X-Requested-With': 'XMLHttpRequest',    # })    resp = sess.get(XHR_URL, params=param)    # print(resp.headers['Referer'])    print(resp.json())]]></content>
      <categories>
        
          <category> spider </category>
        
      </categories>
      <tags>
        
          <tag> spider </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[字体反爬]]></title>
      <url>/blog/2019/04/26/font</url>
      <content type="text"><![CDATA[在没有什么css的影响下，html上显示 != 常人页面上显示的，应该是字体反爬。获取自定义字体库的url/文件font-family这个关键字，利用正则匹配出来，下载下来。字体文件使用 Font Creator 查看解析字体小解fonttools的xmlfonttoolsfrom fontTools.ttLib import TTFontfont = TTFont('/path/to/font.ttf')# 保存xmlfont.saveXML('test.xml')font name  xml  GlyphOrder.GlyphID['name]就是html里面显示的样子 =&gt; font name  code  font.getGlyphOrder() 按序获取，按照需要切片font info  xml  glyf.TTGlyph.contour  font name轮廓坐标应该是独一无二 与 font name一一对应  code  font['glyf'][gly_key].coordinates解析原理明确一些关系 两个字体库 base 已知所有映射关系的字体库 求 新字体库的映射关系  base字体库(base Font library  bfl) 页面显示字符(F) =&gt; base font name(fb) =&gt; base坐标(coor)  由于字体库通常是动态的我们会获得 new字体库(nbl) 页面显示字符(F) =&gt; new font name(fn) =&gt; base坐标(coor)基本思路  利用font name轮廓坐标一致，通过旧映射bfl获取新映射nbl的关系 目标是 fn =&gt; F  通过页面明确F =&gt; fb的映射关系 (这是最蛋疼的一步，这步错了后面就完了，当字体库庞大各种花里胡哨，可能很难找齐对应关系)  解析bfl获取 fb =&gt; coor 反推 coor =&gt; F的关系A  解析nfl获取 fn=&gt;coor映射关系B  通过 关系A、B 得到  fn =&gt; coor =&gt; F 的转换  解析出页面字体def get_base_flag(name):    # html real    base_dict = {        'two': '1',        'three': '5',        'one': '0',        'nine': '6',        'seven': '2',        'eight': '7',        'six': '8',        'five': '3',        'zero': '4',        'four': '9',    }    font_base = get_font(name)    base_gly = font_base.getGlyphOrder()[1:-1]    base_flag = {}    for gly_key in base_gly:        flags = font_base['glyf'][gly_key].coordinates        flags = ''.join(map(str, list(flags)))        base_flag[flags] = {'html': gly_key, 'real': base_dict[gly_key]}    return base_flagdef get_flag(name, base_flag):    font = get_font(name)    gly_list = font.getGlyphOrder()[1:-1]    flag = {}    for gly_key in gly_list:        flags = font['glyf'][gly_key].coordinates        flags = ''.join(map(str, list(flags)))        flag[gly_key] = base_flag[flags]['real']    return flag]]></content>
      <categories>
        
          <category> spider </category>
        
      </categories>
      <tags>
        
          <tag> spider </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[css反爬]]></title>
      <url>/blog/2019/04/26/css</url>
      <content type="text"><![CDATA[普通css        不断变化的class属性带着诡异的样式，找到css样式的地方，按照class聚合起来，分析其中的规律(位移，显示与否)，再解析def get_css(ele_txt):    raw_css = map(lambda x: x.strip(), ele_txt.split('\n'))    css_dict = defaultdict(dict)    for r in raw_css:        if r:            class_, key, value = re.search(r'\.(\w*).*?{\s*?(.*?):(.*?)\s*}', r).groups()            css_dict[class_.strip()][key.strip()] = value.strip()    return css_dictdef parse_css(i, r, css_dict, dis_dict):    class_ = r['class'][0]    value = r.get_text(strip=True)    if class_ in css_dict:        # content        if not value and 'content' in css_dict[class_]:            return True, 'append', int(css_dict[class_]['content'].replace('"', ''))        # ignore        if 'opacity' in css_dict[class_]:            return True, 'ignore', None        # 位移        if 'left' in css_dict[class_]:            displacement = int(re.search(r'(.*?)em', css_dict[class_]['left']).group(1))            dis_dict[i + displacement] = value            return True, 'index', None        # 不动        else:            dis_dict[i] = value            return True, 'index', None    return False, None, None最难是找到css样式的地方，可以是js生成的(加密)，放在css里面，放在html里面]]></content>
      <categories>
        
          <category> spider </category>
        
      </categories>
      <tags>
        
          <tag> spider </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[验证码反爬]]></title>
      <url>/blog/2019/04/26/captcha</url>
      <content type="text"><![CDATA[主要类型  数字字母混合型 CNN  滑块验证码  opencv 懂的话比第一种更容易破解  中文标记文字位置 难  12306常用手段  ocr  打码平台  深度学习CNN opencvopencv 破解腾讯滑块验证码使用Chrome + seleniumframe 切换wait.until(EC.presence_of_element_located((By.ID, "tcaptcha_iframe")))driver.switch_to.frame('tcaptcha_iframe')# 刷新验证码wait.until(EC.presence_of_element_located((By.CLASS_NAME, "tc-action-icon"))).click()# 解决验证码get_captcha(page)# 返回到默认default_contentdriver.switch_to.default_content()验证码结构验证码由两部分组成block(滑块),bg(背景)        def get_captcha(page):    time.sleep(1)        # save block,bg img    bg = wait.until(EC.presence_of_element_located((By.ID, 'slideBg')))    down_img(bg.get_attribute('src'), IMG_PATH.format(page))    block = wait.until(EC.presence_of_element_located((By.ID, 'slideBlock')))    slider_name = '{}_temple'.format(page)    down_img(block.get_attribute('src'), IMG_PATH.format(slider_name))        # get gap 左上角x坐标    gap_top_left = get_gap(page, bg.size, block.size)        # block 与 bg 的起始距离    xoffset = abs(bg.location['x'] - block.location['x']) + 1        # 拖滑块到gap    drag_slider(gap_top_left, xoffset, block.size)识别gapdownload的img和页面显示的img size不同， 必须resize，不然寻找的坐标不准，方便后面拖动滑块def get_img(page, bg_size, block_size, is_debug):    """get resize img"""    bg = cvutil.load_img(IMG_PATH.format(page))    bg = cvutil.resize(bg, width=bg_size['width'], height=bg_size['height'])    block_name = '{}_temple'.format(page)    tem = cvutil.load_img(IMG_PATH.format(block_name))    tem = cvutil.resize(tem, width=block_size['width'], height=block_size['height'])    if is_debug:        cvutil.plt_color_img(bg)        cvutil.plt_color_img(tem)    return bg, temdef search_tem(bg, tem, is_debug):    """    opencv 模板匹配        :param bg:    :param tem:    :param is_debug:    :return:    """    bg_gray = cvutil.gray(bg)    tem = cvutil.gray(tem)    cvutil.plt_gray_img(tem)    w, h = tem.shape[1::-1]    tem = np.abs(255 - tem)    if is_debug:        cvutil.plt_gray_img(tem)    res = cv.matchTemplate(bg_gray, tem, cv.TM_CCOEFF_NORMED)    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)    top_left = max_loc    bottom_right = (top_left[0] + w, top_left[1] + h)    if is_debug:        cv.rectangle(bg, top_left, bottom_right, 255, 2)        cvutil.plt_color_img(bg)    return top_left[0]def get_gap(page, bg_size, block_size, is_debug=False):    print(page, bg_size, block_size)    bg, tem = get_img(page, bg_size, block_size, is_debug)    return search_tem(bg, tem, is_debug)get_gap(773, {'height': 195, 'width': 341}, {'height': 68, 'width': 68})滑块拖动def drag_slider(gap_x, xoffset, block_size):    slider = wait.until(EC.presence_of_element_located((By.ID, 'tcaptcha_drag_thumb')))    w = slider.size['width']    distance = gap_x - xoffset    ActionChains(driver).click_and_hold(slider).perform()    total_x = 0    while True:        # x = random.randint(xoffset + block_size['width'], gap_x + block_size['width'])        x = random.randint(10, 25)        y = random.randint(-1, 1)        total_x += x        if total_x &gt; distance:            x = x - (total_x - distance)            total_x = distance        # print('{}/{} [{}]'.format(x, total_x, distance))        action = ActionChains(driver)        # action.move_by_offset(xoffset=x, yoffset=y).perform()        action.move_to_element_with_offset(slider, xoffset=x + w / 2, yoffset=y).perform()        time.sleep(random.uniform(0.1, 1))        if total_x == distance:            break    # print('stop {}/{} [{}]'.format(x, total_x, distance))    ActionChains(driver).release(slider).perform()效果成功率还是挺高的]]></content>
      <categories>
        
          <category> spider </category>
        
      </categories>
      <tags>
        
          <tag> spider </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mongo 概念]]></title>
      <url>/blog/2019/04/17/mongo-base</url>
      <content type="text"><![CDATA[概念集合集合就是 MongoDB 文档组(类似于table) 集合存在于数据库中，集合没有固定的结构，通常情况下我们插入集合的数据都会有一定的关联性。{"site":"www.baidu.com"}{"site":"www.google.com","name":"Google"}{"site":"www.runoob.com","name":"菜鸟教程","num":5}capped collectionsMongoDB 的操作日志文件固定大小的collection,按照插入顺序来保存（提高增添数据的效率）,必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。元数据数据库的信息是存储在集合中。dbname.system.*文档(Document)文档是一组键值 MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型  文档中的键/值对是有序的。  文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。  MongoDB区分类型和大小写。  MongoDB的文档不能有重复的键。  文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符数据类型ObjectId 类似唯一主键，可以很快的去生成和排序，MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象连接mongodb://username:password@hostname/dbname操作database数据库不存在，则创建数据库，否则切换到指定数据库use DATABASE_NAME查看所有数据库show dbs删除数据库db.dropDatabase()集合删除db.[collection].drop()查看所有集合show tables / show collections建立db.createCollection(name, options)select条件比较            操作      格式      范例      RDBMS中的类似语句                  等于      {&lt;key&gt;:&lt;value&gt;}      db.col.find({“by”:”菜鸟教程”}).pretty()      where by = ‘菜鸟教程’              小于      {&lt;key&gt;:{$lt:&lt;value&gt;}}      db.col.find({“likes”:{$lt:50}}).pretty()      where likes &lt; 50              小于或等于      {&lt;key&gt;:{$lte:&lt;value&gt;}}      db.col.find({“likes”:{$lte:50}}).pretty()      where likes &lt;= 50              大于      {&lt;key&gt;:{$gt:&lt;value&gt;}}      db.col.find({“likes”:{$gt:50}}).pretty()      where likes &gt; 50      and每个键(key)以逗号隔开，即常规 SQL 的 AND 条件。db.col.find({key1:value1, key2:value2}).pretty()50&lt;qty&lt;80db.posts.find( {  qty: { $gt: 50 ,$lt: 80}} )or&gt;db.col.find(   {      $or: [         {key1: value1}, {key2:value2}      ]   }).pretty()模糊查询 title 包含”教”字的文档：db.col.find({title:/教/})查询 title 字段以”教”字开头的文档：db.col.find({title:/^教/})查询 titl e字段以”教”字结尾的文档：db.col.find({title:/教$/})索引db.collection.createIndex(keys, options)1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1db.col.createIndex({"title":1,"description":-1})查看集合索引db.col.getIndexes()查看集合索引大小db.col.totalIndexSize()删除集合所有索引db.col.dropIndexes()删除集合指定索引db.col.dropIndex("索引名称")pymongoconnectimport pymongofrom bson import ObjectId, SONmongo_config = {    'host': 'localhost',    'port': 27017,}db = pymongo.MongoClient(**mongo_config)# dblist = client.list_database_names()# print(dblist)site = db.my.siteinventory = db.my.inventoryinsertd = {"name": "RUNOOB", "alexa": "10000", "url": "https://www.runoob.com"}res = site.insert_one(d)print(res.inserted_id)data_list = [    {"name": "Taobao", "alexa": "100", "url": "https://www.taobao.com"},    {"name": "QQ", "alexa": "101", "url": "https://www.qq.com"},    {"name": "Facebook", "alexa": "10", "url": "https://www.facebook.com"},    {"name": "知乎", "alexa": "103", "url": "https://www.zhihu.com"},    {"name": "Github", "alexa": "109", "url": "https://www.github.com"}]res = site.insert_many(data_list)print(res.inserted_ids)updatequery = {'alexa': '10880'}value = {"$set": {"alexa": "12345"}}value = {'$inc': {'age': 3}}unvalue = {"$unset": {"name": "12345"}}# res = db.update_one(query,value)# res = db.update_many(query, value, upsert=True)res = site.update_one(query, unvalue)print(res)site.replace_one({'_id': ObjectId('5cb735b4efdf11198ebb92ac')},                 {"name": "Taobao12323", "url": "https://www.taobao.com"}, )deletesite.delete_one({'_id': ObjectId('5cb73399efdf11190e096729')})site.delete_many({'_id': ObjectId('5cb73399efdf11190e096729')})bulksite.delete_one({'_id': ObjectId('5cb73399efdf11190e096729')})site.delete_many({'_id': ObjectId('5cb73399efdf11190e096729')})select# 查询  文档中的第一条数据print(site.find_one())# 查询集合中所有数据for s in site.find():    print(s)# whereoutput(site.find_one({'_id': ObjectId('5cb73399efdf11190e096721')}))# 嵌入doccursor = inventory.find(    {"size": SON(        [("h", 14), ("w", 21), ("uom", "cm")]    )})inventory.find({"size.uom": "in"})# andoutput(site.find_one({'name': 'RUNOOB', 'alexa': '100003', }))query = {"name": {"$lt": "H"}}mydoc = site.find(query)for x in mydoc:    print(x)# 字段限制 or select xx .limit(1).skip(2) =&gt; limit(2,1)query = {"$or": [{'alexa': '101'}, {'alexa': '12345'}]}output(site.find(query, {'_id': 0}).limit(1).skip(2))# order by  1 为升序排列，而 -1 是用于降序排列。for x in site.find().sort([    ('alexa', pymongo.DESCENDING), ('name', pymongo.ASCENDING)]):    print(x)# inquery = {'name': {'$in': ['Facebook', 'Taobao']}}for x in site.find(query):    print(x)# array# 严格 有 顺序一样inventory.find({"tags": ["red", "blank"]})# 包含这两个元素 不考虑数组中的顺序或其他元素inventory.find({"tags": {"$all": ["red", "blank"]}})# 包含inventory.find({"tags": "red"})# 数组位置2cursor = db.inventory.find({"dim_cm.1": {"$gt": 25}})# 长度cursor = db.inventory.find({"tags": {"$size": 3}})# 数组嵌入cursor = db.inventory.find({'instock.0.qty': {"$lte": 20}})cursor = db.inventory.find({"instock.qty": {"$gt": 10, "$lte": 20}})cursor = inventory.find(    {"size": SON(        [("h", 14), ("w", 21), ("uom", "cm")]    )})inventory.find_one({"size.uom": "in"})output(inventory.find({"size.uom": "cm"}))# 聚合query = [    # {'$match': {'a': 1}},    {        '$group': {            '_id': "$name",            'a': {'$sum': 1},            'b': {'$max': '$alexa'},        }    },    {'$sort': {'a': pymongo.ASCENDING}},    {'$sort': {'b': pymongo.DESCENDING}},    {'$limit': 20},    {'$match': {'a': 1}},]# select _id, sum(*) as a, max(alexa) as b  from xx group by name as _id having a=1 order by a asce, b descfor x in site.aggregate(query):    print(x)        ]]></content>
      <categories>
        
          <category> mongo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[窗口函数 分析函数]]></title>
      <url>/blog/2019/04/12/windows-function</url>
      <content type="text"><![CDATA[doc概念MySQL中的使用窗口函数的时候，是不允许使用*的，必须显式指定每一个字段。8.0后才有Window Function，其他数据库貌似一早就有窗口函数有点像聚合函数，an aggregate operation groups query rows into a single result row, a window function produces a result for each query row普通聚合函数mysql&gt; SELECT SUM(profit) AS total_profit       FROM sales;+--------------+| total_profit |+--------------+|         7535 |+--------------+mysql&gt; SELECT country, SUM(profit) AS country_profit       FROM sales       GROUP BY country       ORDER BY country;+---------+----------------+| country | country_profit |+---------+----------------+| Finland |           1610 || India   |           1350 || USA     |           4575 |+---------+----------------+窗口函数mysql&gt; SELECT         year, country, product, profit,         SUM(profit) OVER() AS total_profit, -- OVER子句为空，它将整个查询行集视为单个分区。因此，窗函数产生全局和，但对每一行都这样做。         SUM(profit) OVER(PARTITION BY country) AS country_profit -- 按国家/地区划分行，每个分区（每个国家/地区）生成一个总和。该函数为每个分区行生成此总和。       FROM sales       ORDER BY country, year, product, profit;+------+---------+------------+--------+--------------+----------------+| year | country | product    | profit | total_profit | country_profit |+------+---------+------------+--------+--------------+----------------+| 2000 | Finland | Computer   |   1500 |         7535 |           1610 || 2000 | Finland | Phone      |    100 |         7535 |           1610 || 2001 | Finland | Phone      |     10 |         7535 |           1610 || 2000 | India   | Calculator |     75 |         7535 |           1350 || 2000 | India   | Calculator |     75 |         7535 |           1350 || 2000 | India   | Computer   |   1200 |         7535 |           1350 || 2000 | USA     | Calculator |     75 |         7535 |           4575 || 2000 | USA     | Computer   |   1500 |         7535 |           4575 || 2001 | USA     | Calculator |     50 |         7535 |           4575 || 2001 | USA     | Computer   |   1200 |         7535 |           4575 || 2001 | USA     | Computer   |   1500 |         7535 |           4575 || 2001 | USA     | TV         |    100 |         7535 |           4575 || 2001 | USA     | TV         |    150 |         7535 |           4575 |+------+---------+------------+--------+--------------+----------------+执行方式  Window functions are permitted only in the select list and ORDER BY clause. Query result rows are determined from the FROM clause, after WHERE, GROUP BY, and HAVING processing,and windowing execution occurs before ORDER BY, LIMIT, and SELECT DISTINCT.窗口函数一般的聚合函数可以作为窗口函数        非聚合窗口函数mysql&gt; SELECT         year, country, product, profit,         ROW_NUMBER() OVER(PARTITION BY country) AS row_num1,         ROW_NUMBER() OVER(PARTITION BY country ORDER BY year, product) AS row_num2       FROM sales;+------+---------+------------+--------+----------+----------+| year | country | product    | profit | row_num1 | row_num2 |+------+---------+------------+--------+----------+----------+| 2000 | Finland | Computer   |   1500 |        2 |        1 || 2000 | Finland | Phone      |    100 |        1 |        2 || 2001 | Finland | Phone      |     10 |        3 |        3 || 2000 | India   | Calculator |     75 |        2 |        1 || 2000 | India   | Calculator |     75 |        3 |        2 || 2000 | India   | Computer   |   1200 |        1 |        3 || 2000 | USA     | Calculator |     75 |        5 |        1 || 2000 | USA     | Computer   |   1500 |        4 |        2 || 2001 | USA     | Calculator |     50 |        2 |        3 || 2001 | USA     | Computer   |   1500 |        3 |        4 || 2001 | USA     | Computer   |   1200 |        7 |        5 || 2001 | USA     | TV         |    150 |        1 |        6 || 2001 | USA     | TV         |    100 |        6 |        7 |+------+---------+------------+--------+----------+----------+语法over_clause:    {OVER (window_spec) | OVER window_name}OVER (window_spec)window_spec:    [window_name] [partition_clause] [order_clause] [frame_clause]  partition_clause  PARTITION BY子句指示如何将查询行分区，执行窗口函数的结果是基于该分区的所有行，没有PARTITION BY代表所有行是一个单独分区。  frame_clause  frame当前分区的子集 帧是根据当前行确定的，这使得帧能够在分区内移动，具体取决于其分区内当前行的位置 =&gt; 就是移动窗口mysql&gt; SELECT         time, subject, val,         SUM(val) OVER (PARTITION BY subject ORDER BY time                        ROWS UNBOUNDED PRECEDING) -- 该分区内 sum(当前值,sum(前一个值))           AS running_total,         AVG(val) OVER (PARTITION BY subject ORDER BY time                        ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) -- 该分区内 avg(后一个值,当前值,前一个值)           AS running_average       FROM observations;+----------+---------+------+---------------+-----------------+| time     | subject | val  | running_total | running_average |+----------+---------+------+---------------+-----------------+| 07:00:00 | st113   |   10 |            10 |          9.5000 || 07:15:00 | st113   |    9 |            19 |         14.6667 || 07:30:00 | st113   |   25 |            44 |         18.0000 || 07:45:00 | st113   |   20 |            64 |         22.5000 || 07:00:00 | xh458   |    0 |             0 |          5.0000 || 07:15:00 | xh458   |   10 |            10 |          5.0000 || 07:30:00 | xh458   |    5 |            15 |         15.0000 || 07:45:00 | xh458   |   30 |            45 |         20.0000 || 08:00:00 | xh458   |   25 |            70 |         27.5000 |+----------+---------+------+---------------+-----------------+只有聚合函数和FIRST_VALUE() LAST_VALUE() NTH_VALUE()可以使用，其他的用了会被ignore，use the entire partition even if a frame is specifiedmysql&gt; SELECT         time, subject, val,         FIRST_VALUE(val)  OVER w AS 'first',         LAST_VALUE(val)   OVER w AS 'last',         NTH_VALUE(val, 2) OVER w AS 'second',         NTH_VALUE(val, 4) OVER w AS 'fourth'       FROM observations       WINDOW w AS (PARTITION BY subject ORDER BY time                    ROWS UNBOUNDED PRECEDING);+----------+---------+------+-------+------+--------+--------+| time     | subject | val  | first | last | second | fourth |+----------+---------+------+-------+------+--------+--------+| 07:00:00 | st113   |   10 |    10 |   10 |   NULL |   NULL || 07:15:00 | st113   |    9 |    10 |    9 |      9 |   NULL || 07:30:00 | st113   |   25 |    10 |   25 |      9 |   NULL || 07:45:00 | st113   |   20 |    10 |   20 |      9 |     20 || 07:00:00 | xh458   |    0 |     0 |    0 |   NULL |   NULL || 07:15:00 | xh458   |   10 |     0 |   10 |     10 |   NULL || 07:30:00 | xh458   |    5 |     0 |    5 |     10 |   NULL || 07:45:00 | xh458   |   30 |     0 |   30 |     10 |     30 || 08:00:00 | xh458   |   25 |     0 |   25 |     10 |     30 |+----------+---------+------+-------+------+--------+--------+OVER window_nameWINDOW window_name AS (window_spec)    [, window_name AS (window_spec)] ...    按名称引用窗口，可以更简单地编写查询SELECT  val,  ROW_NUMBER() OVER w AS 'row_number',  RANK()       OVER w AS 'rank',  DENSE_RANK() OVER w AS 'dense_rank'FROM numbersWINDOW w AS (ORDER BY val);子句中使用以不同方式修改窗口SELECT  DISTINCT year, country,  FIRST_VALUE(year) OVER (w ORDER BY year ASC) AS first,  FIRST_VALUE(year) OVER (w ORDER BY year DESC) AS lastFROM salesWINDOW w AS (PARTITION BY country);CTECTE有两种用法，非递归的CTE和递归的CTE。非递归的CTE可以用来增加代码的可读性，增加逻辑的结构化表达。WITH cte as (select row_number()over(partition by course order by score desc) as row_num, 		id,		name,		course,		score 	from tb )select id,name,course,score from cte where row_num=1;]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[performance]]></title>
      <url>/blog/2019/04/11/performance</url>
      <content type="text"><![CDATA[时间import cv2 as cv# 时钟周期数e1 = cv.getTickCount()# your code executione2 = cv.getTickCount()# 时间 = 时钟周期数/时钟周期的频率time = (e2 - e1)/ cv.getTickFrequency()print(time)优化Many of the OpenCV functions are optimized using SSE2, AVX etc. OpenCV runs the optimized code if it is enabled.cv.useOptimized() to check if it is enabled/disabled and cv.setUseOptimized() to enable/disable通常，OpenCV函数比Numpy函数更快。因此，对于相同的操作，OpenCV功能是首选。但是，可能有例外，尤其是当Numpy使用视图而不是副本时。  尽量避免在Python中使用循环，尤其是双循环/三循环等。它们本质上很慢。  将算法/代码矢量化到最大可能范围，因为Numpy和OpenCV针对向量运算进行了优化。  利用缓存一致性。  除非需要，否则永远不要复制数组。尝试使用视图。阵列复制是一项昂贵的操作。  即使在完成所有这些操作之后，如果您的代码仍然很慢，或者使用大型循环是不可避免的，请使用其他库（如Cython）来加快速度。numpy 视图 副本视图数据的一个别称或引用，对视图进行修改，它会影响到原始数据，物理内存在同一位置。  numpy 的切片操作返回原数据的视图  调用 ndarray 的 view() 函数产生一个视图副本一个数据的完整的拷贝，如果我们对副本进行修改，它不会影响到原始数据，物理内存不在同一位置。  Python deepCopy()函数  调用 ndarray 的 copy() 函数产生一个副本。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> opencv </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[img]]></title>
      <url>/blog/2019/04/11/img</url>
      <content type="text"><![CDATA[小知识一个BGR图像 0~255ROI更改图像的特定区域 Region Of Interest ROI 感兴趣区域mask 掩膜0 黑 255 白与目标图像做mask操作 目标图像扣走mask中黑色轮廓部分，保留白色区域 =&gt; 保留ROI,其他区域为0通常mask之前  先转成灰度图像  二值化、反二值化作用  对自己做mask 可以抠图  提取ROI  特征提取hsvBGR和HSV的转换使用 cv.COLOR_BGR2HSV  H表示色彩/色度，取值范围 [0，179]  S表示饱和度，取值范围 [0，255]  V表示亮度，取值范围 [0，255]图像矩图像矩一般都是原点矩  一阶矩和零阶矩，计算形状的质心  二阶矩，计算形状的方向V（i，j）表示图像在(i,j)点上的灰度值。二阶矩，计算形状的方向基本read write show获取图片 路径不对，imread get Noneimport cv2 as cvfrom matplotlib import pyplot as pltdef show(*args):    if args:        for img in args:            cv.imshow('', img)            # wait 毫秒 0是永远            cv.waitKey(0)            cv.destroyAllWindows()def plt_color_img(img):    """    彩色图片使用opencv加载是使用BGR模式，但是使用Matplotlib库是用RGB模式    :param img:    :return:    """    b, g, r = cv.split(img)    img2 = cv.merge([r, g, b])    plt.imshow(img2)    plt.xticks([])    plt.yticks([])    plt.show()def plt_gray_img(img):    plt.imshow(img, cmap='gray', interpolation='bicubic')    plt.xticks([])    plt.yticks([])    plt.show()def load_img(path, mode=cv.IMREAD_COLOR):    """    cv.IMREAD_COLOR：加载彩色图像。任何图像的透明度都将被忽略。这是默认标志。    cv.IMREAD_GRAYSCALE：以灰度模式加载图像    cv.IMREAD_UNCHANGED：加载图像，包括alpha通道    整数1,0或-1    """    return cv.imread(path, mode)def save_img(path, img):    cv.imwrite(path, img)属性img = util.load_img('img/messi5.jpg')# 属性print(img.shape)  # 行数 高、列数 宽、[通道数]print(img.size)  # 总像素数print(img.dtype)  # 数据类型选取 ROI选取区域# 选择数组的区域 前5行和后3列px = img[50, 50]print(px)px = img[50, 50, 0]  # 获取blue值print(px)px = img[50, 50, 1]  # 获取green值print(px)px = img[50, 50, 2]  # 获取red值print(px)img[50, 50] = [255, 255, 255]print(img[50, 50])# copyball = img[280:340, 330:390]img[273:333, 100:160] = ballutil.save_img('out.jpg', img)单个选取# 单个像素访问item() itemset()print('\n', img[10, 10])sca = img.item(10, 10, 0)  # 获取bgr值print(sca)sca = img.item(10, 10, 1)print(sca)sca = img.item(10, 10, 2)print(sca)split merge# 分割和合并图像通道 split 耗时 没什么必要可以用numpy的indexb, g, r = cv.split(img)img2 = cv.merge([r, g, b])运算add addWeightedBoth images should be of same depth and type, (shape and 图片后缀)or second image can just be a scalar value."""图像运算"""x = np.uint8([250])y = np.uint8([10])"""Both images should be of same depth and type, (shape and 图片后缀)or second image can just be a scalar value.OpenCV添加是饱和操作，而Numpy添加是模运算。"""print(cv.add(x, y))  # 250+10 = 260 =&gt; 255print(x + y)  # 250+10 = 260 % 256(2^8) = 4img1 = util.load_img('img/ml.png')img2 = util.load_img('img/opencv-logo.png')print(img1.shape, img2.shape)img2 = cv.resize(img2, (img1.shape[1], img1.shape[0]))dst = cv.add(img1, img2)util.show(dst)"""图像混合 按比例混合起来，有不同的权重 修改透明度dst=α⋅img1+β⋅img2+γ"""img1 = util.load_img('img/ml.png')img2 = util.load_img('img/opencv-logo.png')print(img1.shape, img2.shape)img2 = cv.resize(img2, (img1.shape[1], img1.shape[0]))assert img1.shape == img2.shapeprint(img1.shape, img2.shape)# 有不同的权重dst = cv.addWeighted(img1, 0.7, img2, 0.3, 0)util.show(dst)位运算"""按位AND，OR，NOT和XOR运算 添加两个图像，它将改变颜色。如果我混合它，我会得到一个透明的效果"""img1 = util.load_img('img/messi5.jpg')img2 = util.load_img('img/opencv-logo-white.png')rows, cols, channels = img2.shaperoi = img1[0:rows, 0:cols]"""what is mask 掩膜0 黑 255 白与目标图像做mask操作 目标图像扣走mask中黑色轮廓部分，保留白色区域 =&gt; 保留ROI,其他区域为0通常mask之前先转成灰度图像二值化、反二值化对自己做mask 可以抠图提取ROI特征提取"""img2gray = cv.cvtColor(img2, cv.COLOR_BGR2GRAY)ret, mask = cv.threshold(img2gray, 10, 255, cv.THRESH_BINARY)mask_inv = cv.bitwise_not(mask)img1_bg = cv.bitwise_and(roi, roi, mask=mask_inv)img2_fg = cv.bitwise_and(img2, img2, mask=mask)util.save_img('img1_bg.png', img1_bg)util.save_img('img2_fg.png', img2_fg)dst = cv.add(img1_bg, img2_fg)img1[0:rows, 0:cols] = dstutil.show(mask_inv, img1_bg, mask, img2_fg)util.show(img1)# util.show(mask_inv)color灰度img = util.load_img('img/messi5.jpg')img2gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)util.show(img2gray)HSV选取颜色ROI视频cap = cv.VideoCapture(0)while (1):    # Take each frame    _, frame = cap.read()    # Convert BGR to HSV    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)    """    BGR2HSV        选取绿色    green = np.uint8([[[0,255,0 ]]])    hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV)        使用[H-10, 100,100] and [H+10, 255, 255] 做阈值上下限    """    # define range of blue color in HSV    lower_blue = np.array([110, 50, 50])    upper_blue = np.array([130, 255, 255])    # 设定取值范围    mask = cv.inRange(hsv, lower_blue, upper_blue)    # Bitwise-AND mask and original image    res = cv.bitwise_and(frame, frame, mask=mask)    cv.imshow('frame', frame)    cv.imshow('mask', mask)    cv.imshow('res', res)    k = cv.waitKey(5) &amp; 0xFF    if k == 27:        breakcv.destroyAllWindows()几何变换放大缩小"""translation, rotation, affine transformationimg.shape height width channelx 横坐标 y 纵坐标resize( (width,height),fx,fy (因子) )cv.INTER_AREA for 缩小cv.INTER_LINEAR for 放大 defaultcv.INTER_CUBIC slow"""img = util.load_img('img/messi5.jpg')height, width = img.shape[:2]print(img.shape)res = cv.resize(img, None, fx=3, fy=2, interpolation=cv.INTER_LINEAR)res = cv.resize(img, (int(0.5 * width), int(0.5 * height)), interpolation=cv.INTER_AREA)print(res.shape)util.show(res)平移"""平移 （100，50）"""img = util.load_img('img/messi5.jpg', 0)print(img.shape)rows, cols = img.shapeM = np.float32([[1, 0, 100], [0, 1, 50]])dst = cv.warpAffine(img, M, (cols, rows))print(dst.shape)util.show(dst)旋转"""旋转 90度"""img = util.load_img('img/messi5.jpg', 0)rows, cols = img.shape# cols-1 and rows-1 are the coordinate limits.# center, angle, scaleM = cv.getRotationMatrix2D(((cols - 1) / 2.0, (rows - 1) / 2.0), 90, 1)dst = cv.warpAffine(img, M, (cols, rows))print(dst.shape)util.show(dst)二值化 threshold自定义全局值img = util.load_img('img/gradient.png', 0)ret, thresh1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)ret, thresh2 = cv.threshold(img, 127, 255, cv.THRESH_BINARY_INV)ret, thresh3 = cv.threshold(img, 127, 255, cv.THRESH_TRUNC)ret, thresh4 = cv.threshold(img, 127, 255, cv.THRESH_TOZERO)ret, thresh5 = cv.threshold(img, 127, 255, cv.THRESH_TOZERO_INV)titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]for i in range(6):    plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray')    plt.title(titles[i])    plt.xticks([]), plt.yticks([])plt.show()自适应阈值"""自适应阈值照明条件算法基于其周围的小区域确定像素的阈值。因此，我们为同一图像的不同区域获得不同的阈值，这为具有不同照明的图像提供了更好的结果。cv.ADAPTIVE_THRESH_MEAN_C 阈值= 取邻近区域的平均值 - 常数Ccv.ADAPTIVE_THRESH_MEAN_C 阈值= 邻近区域的高斯加权和 - 常数CadaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst]) → dst"""img = util.load_img('img/sudoku.png', 0)util.show(img)# smoothes an image using the median filter 二值化效果更好img = cv.medianBlur(img, 5)util.show(img)ret, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)th2 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2)th3 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2)titles = ['Original Image', 'Global Thresholding (v = 127)',          'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']images = [img, th1, th2, th3]for i in range(4):    plt.subplot(2, 2, i + 1), plt.imshow(images[i], 'gray')    plt.title(titles[i])    plt.xticks([]), plt.yticks([])plt.show()OTSU适合双峰图img_rgb = util.load_img('img/coins.png')gray = util.gray(img_rgb)plt.hist(gray.ravel(), 256, [0, 256])plt.show()cv.THRESH_OTSU效果更好_, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV)_, thresh1 = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)res = np.hstack((thresh,thresh1))util.show(res)过滤 smooth 消除噪音"""图像过滤低通滤波LPF可以使图像去除噪声，高通滤波HPF可以找到图像的边缘。图像平滑内核卷积来实现图像模糊。它有助于消除噪音。实际上从图像中去除了高频内容（例如：噪声，边缘）。边缘会有点模糊。均值过滤调用blur()等效于调用将normalize=true的boxFilter().中位数 cv.medianBlur() 内核区域下所有像素的中值，并用该中值替换中心元素双边过滤cv.bilateralFilter() 降低噪音方面非常有效，同时保持边缘清晰。但与其他过滤器相比，操作速度较慢。高斯滤波器采用像素周围的邻域并找到其高斯加权平均值:return:"""img = util.load_img('img/opencv-logo-white.png')cv.blur(img, (5, 5))blur = cv.GaussianBlur(img, (5, 5), 0)img = cv.medianBlur(img, 5)cv.blur(img, (5, 5))morphological 侵蚀和膨胀形态学运算符是 侵蚀和膨胀img = util.load_img('img/j.png', 0)"""侵蚀 Erosion 如果与卷积核对应的原图像像素值都是1，那么中心元素保持原值，否则为0有助于消除小的白噪声"""kernel = np.ones((5, 5), np.uint8)erosion = cv.erode(img, kernel, iterations=1)"""扩张 Dilation 恰好与侵蚀相反 由于噪音消失了，它们不会再回来"""dilation = cv.dilate(img, kernel, iterations=1)"""开运算  先腐蚀再膨胀，一般用来去除噪声"""opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)"""闭运算  先膨胀再腐蚀，一般用来填充黑色的小像素点"""closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)util.plt_gray_img(img, closing)边界识别"""Edge DetectionCanny Edge Detection去除噪声 remove the noise in the image with a 5x5 Gaussian filter计算图像梯度 在水平与竖直方向上计算一阶导数，图像梯度方向和大小去除噪声 remove the noise in the image with a 5x5 Gaussian filter去除噪声 remove the noise in the image with a 5x5 Gaussian filter"""img = util.load_img('img/messi5.jpg', 0)edges = cv.Canny(img, 100, 200)plt.subplot(121), plt.imshow(img, cmap='gray')plt.title('Original Image'), plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(edges, cmap='gray')plt.title('Edge Image'), plt.xticks([]), plt.yticks([])plt.show()watershed使用距离变换和分水岭来分割相互接触的物体靠近对象中心的区域是前景，离对象远的区域是背景，不确定的区域是边界。      物体没有相互接触/只求前景 可用侵蚀消除了边界像素        到距离变换并应用适当的阈值  膨胀操作会将对象边界延伸到背景，确保background区域只有background  边界 = 能否确认是否是背景的区域  - 确定是前景的区域img = util.load_img('img/coins.png')gray = util.gray(img)ret, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)# noise removalkernel = np.ones((3, 3), np.uint8)opening = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations=2)# sure background areasure_bg = cv.dilate(opening, kernel, iterations=3)# Finding sure foreground areadist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5) # 计算每个像素离最近0像素的距离ret, sure_fg = cv.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)# Finding unknown regionsure_fg = np.uint8(sure_fg)unknown = cv.subtract(sure_bg, sure_fg)# Marker labelling 用0标记图像的背景 其他对象用从1开始的整数标记ret, markers = cv.connectedComponents(sure_fg)"""我们知道，如果背景标记为0，分水岭会将其视为未知区域。所以我们想用不同的整数来标记它。相反，我们将标记由未知定义的未知区域，为0。"""# Add one to all labels so that sure background is not 0, but 1markers = markers + 1# Now, mark the region of unknown with zeromarkers[unknown == 255] = 0markers = cv.watershed(img, markers)# 修改标记图像。边界区域将标记为-1img[markers == -1] = [255, 0, 0]util.show(img,is_seq=True)        GrabCut提取图像中的前景  用户在前景区域周围绘制一个矩形（前景区域应该完全在矩形内）  算法迭代地对其进行分段以获得最佳结果某些情况下，分割将不会很好。某些前景区域标记为背景，用户需要做标记前景和后景算法过程 就是监督半监督学习 根据颜色相似性 聚类分割Hough检测直线 霍夫变换极坐标img = util.load_img('img/sudoku.png')img_gray = util.gray(img)edges = cv.Canny(img_gray, 100, 200)"""cv.HoughLinesP"""lines = cv.HoughLinesP(edges, 1, np.pi / 180, 200, minLineLength=100, maxLineGap=10)for line in lines:    x1, y1, x2, y2 = line[0]    cv.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)"""cv.HoughLines"""# lines = cv.HoughLines(edges, 1, np.pi / 180, 200)# for line in lines:#     rho, theta = line[0]#     a = np.cos(theta)#     b = np.sin(theta)#     x0 = a * rho#     y0 = b * rho#     x1 = int(x0 + 1000 * (-b))#     y1 = int(y0 + 1000 * (a))#     x2 = int(x0 - 1000 * (-b))#     y2 = int(y0 - 1000 * (a))#     cv.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)""""""util.show(img)pyramid有不同分辨率的相同图像不同分辨率的图像就是图像金字塔 小分辨率的图像在顶部，大的在底部低分辨率的图像由高分辨率的图像去除连续的行和列得到img = util.load_img('img/messi5.jpg')# 分辨率减少 M×N的图像就变成了M/2×N/2的图像了，面积变为原来的四分之一lower_reso = cv.pyrDown(img)# 尺寸变大 分辨率不变high_reso = cv.pyrUp(lower_reso)util.show(img, lower_reso, high_reso)复杂例子 模糊边界A = util.load_img('img/apple.jpg')B = util.load_img('img/orange.jpg')# generate Gaussian pyramid for AG = A.copy()gpA = [G]for i in range(6):    G = cv.pyrDown(G)    gpA.append(G)# generate Gaussian pyramid for BG = B.copy()gpB = [G]for i in range(6):    G = cv.pyrDown(G)    gpB.append(G)# generate Laplacian Pyramid for AlpA = [gpA[5]]for i in range(5, 0, -1):    GE = cv.pyrUp(gpA[i])    L = cv.subtract(gpA[i - 1], GE)    lpA.append(L)util.show(*lpA)# generate Laplacian Pyramid for BlpB = [gpB[5]]for i in range(5, 0, -1):    GE = cv.pyrUp(gpB[i])    L = cv.subtract(gpB[i - 1], GE)    lpB.append(L)util.show(*lpB)# Now add left and right halves of images in each levelLS = []for la, lb in zip(lpA, lpB):    rows, cols, dpt = la.shape    ls = np.hstack((la[:, 0:int(cols / 2)], lb[:, int(cols / 2):]))    LS.append(ls)util.show(*LS)# now reconstructls_ = LS[0]for i in range(1, 6):    ls_ = cv.pyrUp(ls_)    ls_ = cv.add(ls_, LS[i])# image with direct connecting each halfreal = np.hstack((A[:, :int(cols / 2)], B[:, int(cols / 2):]))cv.imwrite('Pyramid_blending2.jpg', ls_)cv.imwrite('Direct_blending.jpg', real)        Contour轮廓概念what is contour轮廓 是连接所有连续点（沿着边界）的曲线，具有相同的颜色或强度。轮廓是形状分析和物体检测和识别的有用工具。从黑色背景中找到白色物体hierarchy 层次结构外部一个称为父项，将内部项称为子项，图像中的轮廓彼此之间存在某种关系。并且我们可以指定一个轮廓如何相互连接每个轮廓都有自己的信息 [Next，Previous，First_Child，Parent] 没有为-1  Next 表示同一层级的下一个轮廓  Previous 表示同一层级的先前轮廓  First_Child 表示其第一个子轮廓  Parent 表示其父轮廓的索引轮廓查找模式  RETR_LIST 检索所有轮廓，但不创建任何父子关系 它们都属于同一层级 First_Child Parent = -1  RETR_EXTERNAL 只返回最外边的轮廓，所有子轮廓会忽略  RETR_CCOMP 所有轮廓并将它们排列为2级层次结构 对象的外部轮廓（即其边界）放置在层次结构-1中。对象内部的轮廓（如果有的话）放在层次结构-2中 如此类推  RETR_TREE 检索所有轮廓并创建完整的族层次结构列表img = util.load_img('img/apple.jpg')imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)ret, thresh = cv.threshold(imgray, 127, 255, 0)# cv.findContours函数中有三个参数，第一个是源图像，第二个是轮廓检索模式，第三个是轮廓逼近法。它输出轮廓和层次结构contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)# 第一个参数是源图像，第二个参数是轮廓，第三个参数是轮廓索引 -1是所有轮廓 其余参数是颜色，厚度res = cv.drawContours(img, contours, -1, (0,255,0), 3)util.show(res)轮廓逼近法轮廓中会存储所有边界上的(x,y)坐标。但是不一定需要全部存储  cv.CHAIN_APPROX_NONE，则存储所有边界点  cv.CHAIN_APPROX_SIMPLE 它删除所有冗余点并压缩轮廓，从而节省内存质心 面积 周长其他特征和性质import cv2 as cvimport utilsrc = util.load_img('img/s.png', 0)ret, thresh = cv.threshold(src, 127, 255, cv.THRESH_BINARY)# cv.findContours（）函数中有三个参数，第一个是源图像，第二个是轮廓检索模式，第三个是轮廓近似方法。它输出轮廓和层次结构contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)height = src.shape[0]width = src.shape[0]out = util.blank_img(width*1.5, height, (255, 255, 255))for index, cnt in enumerate(contours):    # 画轮廓    cv.drawContours(out, contours, index, (0, 0, 255), 1)    # 质心坐标    M = cv.moments(cnt)    cx = M['m10'] / M['m00']    cy = M['m01'] / M['m00']    cv.circle(out, (int(cx), int(cy)), 2, (255, 0, 0), -1)    # 轮廓面积 M['m00']    area = cv.contourArea(cnt)    # 轮廓周长   轮廓是否闭合True    perimeter = cv.arcLength(cnt, True)    print('({},{}) 面积={} 周长={}'.format(cx, cy, area, perimeter))util.show(out)        approx 轮廓近似用更少点组成的轮廓img = util.load_img('img/t.png',0)height = img.shape[0]width = img.shape[0]out = util.blank_img(width * 1.5, height, (255, 255, 255))contours = get_cnt(img)# 蓝色a = cv.drawContours(out, contours, -1, (255, 0, 0), 5)# factor  越小越接近轮廓 用的点越多factor = 0.01epsilon = factor * cv.arcLength(contours[0], True)approx = cv.approxPolyDP(contours[0], epsilon, True)# 红色cv.drawContours(out, [approx], -1, (0, 0, 255), 5)util.show(a,out)Convex Hull类似于轮廓近似凸包: 在多维空间中有一群散佈各处的点 ，凸包 是包覆这群点的所有外壳当中，表面积暨容积最小的一个外壳，而最小的外壳一定是凸的。凸的定义: 图形内任意两点的连线不会经过图形外部凸性缺陷：求凸包img = util.load_img('img/horse.png', 0)convex = util.load_img('img/horse1.png',0)height = img.shape[0]width = img.shape[0]out = util.blank_img(width * 1.5, height, (255, 255, 255))# 求轮廓contours = get_cnt(img)convex_cnt = get_cnt(convex)[0]# 求凸包cv.drawContours(out, contours, -1, (255, 0, 0), 5)cnt = contours[0]hull = cv.convexHull(cnt)# 检测一个曲线是不是凸的print(cv.isContourConvex(cnt))cv.drawContours(out, [hull], -1, (0, 0, 255), 5)util.show(out)边界矩形 边界标注Bounding Rectangleimg = util.load_img('img/rect.png', 0)contours = get_cnt(img)height = img.shape[0]width = img.shape[0]out = util.blank_img(width * 1.5, height, (255, 255, 255))# out = imgfor index, cnt in enumerate(contours):    x, y, w, h = cv.boundingRect(cnt)    # 原型轮廓 红    cv.drawContours(out, contours, index, (0, 0, 255), 3)    # 直边矩形 绿    cv.rectangle(out, (x, y), (x + w, y + h), (0, 255, 0), 2)    # 最小面积绘制边界矩形 蓝    rect = cv.minAreaRect(cnt)    box = cv.boxPoints(rect)    box_ = np.int0(box)    cv.drawContours(out, [box_], 0, util.rgb2bgr((30, 144, 255)), 2)    # 最小封闭圈 橙    (x, y), radius = cv.minEnclosingCircle(cnt)    center, radius = (int(x), int(y)), int(radius)    cv.circle(out, center, radius, util.rgb2bgr((255, 140, 0)), 2)    # 拟合椭圆 蓝    ellipse = cv.fitEllipse(cnt)    cv.ellipse(out, ellipse, util.rgb2bgr((135, 206, 250)), 2)    print(index)    # 线 黑    # rows, cols = out.shape[:2]    # [vx, vy, x, y] = cv.fitLine(cnt, cv.DIST_L2, 0, 0.01, 0.01)    # lefty = int((-x * vy / vx) + y)    # righty = int(((cols - x) * vy / vx) + y)    # print((cols - 1, righty), (0, lefty))    # cv.line(out, (cols - 1, righty), (0, lefty), (0, 0, 0), 2)    # util.show(out)util.show(out)其他凸性缺陷img = util.load_img('img/star.png', 0)contours = get_cnt(img)cnt = contours[0]# 凸包时传递returnPoints = False，以便找到凸起缺陷hull = cv.convexHull(cnt, returnPoints=False)defects = cv.convexityDefects(cnt, hull)for i in range(defects.shape[0]):    # 起点，终点，最远点，到最远点的近似距离    s, e, f, d = defects[i, 0]    start = tuple(cnt[s][0])    end = tuple(cnt[e][0])    far = tuple(cnt[f][0])    cv.line(img, start, end, [0, 255, 0], 2)    cv.circle(img, far, 5, [0, 0, 255], -1)util.show(img)点多边形距离（50,50） 点与轮廓之间的最短距离 当点在轮廓外时返回负值，当点在内部时返回正值，如果点在轮廓上则返回零dist = cv.pointPolygonTest(cnt,(50,50),True)匹配形状越相似越小 一样 =0.0ret = cv.matchShapes(cnt1,cnt2,1,0.0)]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> opencv </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[draw]]></title>
      <url>/blog/2019/04/11/draw</url>
      <content type="text"><![CDATA[drawdef draw():    # 画图窗口大小设置为512*512 通常参数，可选1,3,4    img = np.zeros((512, 512, 4), np.uint8)    # 画直线，参数为绘图窗口，起点，终点，颜色，粗细，连通性（可选4联通或8联通）    img = cv.line(img, (100, 0), (511, 511), (255, 0, 0), 5, 8)    # 画矩形，参数为窗口，左上角坐标，右下角坐标，颜色，粗度    img = cv.rectangle(img, (384, 0), (510, 128), (0, 255, 0), 3)    # 画圆函数，参数为窗口，圆心，半径，颜色，粗度（如果粗度为-1标示实心）,连通性    img = cv.circle(img, (447, 63), 50, (0, 0, 255), 1, 8)    # 画椭圆函数，参数为窗口，椭圆中心，椭圆长轴短轴，椭圆逆时针旋转角度，椭圆起绘制起始角度，椭圆绘制结束角度，颜色，粗度，连通性    img = cv.ellipse(img, (256, 256), (100, 50), 100, 36, 360, (0, 255, 255), 1, 8)    # 画多边形    pts = np.array([[10, 5], [20, 30], [70, 20], [50, 10]], np.int32)    # 转换成三维数组，表示每一维里有一个点坐标    pts = pts.reshape((-1, 1, 2))    # 画多边形函数，参数为窗口，坐标，是否封闭，颜色    img = cv.polylines(img, [pts], True, (0, 255, 255))    # 字 参数为窗口，字符，字体位(左下角),字体类型(查询cv2.putText()函数的文档来查看字体),字体大小,常规参数，类似颜色，粗细，线条类型等等    font = cv.FONT_HERSHEY_SIMPLEX    cv.putText(img, 'OpenCV', (10, 500), font, 4, (255, 255, 255), 2, cv.LINE_AA)    util.show(img)鼠标 draw监听鼠标事件 cv.EVENT_* cv.setMouseCallback('image', draw_circle)def mouse_draw():    """双击画圆"""    def draw_circle(event, x, y, flags, param):        if event == cv.EVENT_LBUTTONDBLCLK:            cv.circle(img, (x, y), 100, (255, 0, 0), -1)    def draw_circle(event, x, y, flags, param):        global ix, iy, drawing, mode        # print(ix, iy, drawing, mode)        if event == cv.EVENT_LBUTTONDOWN:            drawing = True            ix, iy = x, y        elif event == cv.EVENT_MOUSEMOVE:            if drawing:                if mode:                    cv.rectangle(img, (ix, iy), (x, y), (0, 255, 0), -1)                else:                    cv.circle(img, (x, y), 5, (0, 0, 255), -1)        elif event == cv.EVENT_LBUTTONUP:            drawing = False            if mode:                cv.rectangle(img, (ix, iy), (x, y), (0, 255, 0), -1)            else:                cv.circle(img, (x, y), 5, (0, 0, 255), -1)    img = np.zeros((512, 512, 3), np.uint8)    cv.namedWindow('image')    cv.setMouseCallback('image', draw_circle)    while True:        cv.imshow('image', img)        if cv.waitKey(20) &amp; 0xFF == 27:            break    cv.destroyAllWindows()UIdef bar_grb():    def nothing(x):        pass    # Create a black image, a window    img = np.zeros((300, 512, 3), np.uint8)    cv.namedWindow('image')    # create trackbars for color change    cv.createTrackbar('R', 'image', 0, 255, nothing)    cv.createTrackbar('G', 'image', 0, 255, nothing)    cv.createTrackbar('B', 'image', 0, 255, nothing)    # create switch for ON/OFF functionality    switch = '0 : OFF \n1 : ON'    cv.createTrackbar(switch, 'image', 0, 1, nothing)    while True:        cv.imshow('image', img)        k = cv.waitKey(1) &amp; 0xFF        if k == 27:            break        # get current positions of four trackbars        r = cv.getTrackbarPos('R', 'image')        g = cv.getTrackbarPos('G', 'image')        b = cv.getTrackbarPos('B', 'image')        s = cv.getTrackbarPos(switch, 'image')        if s == 0:            img[:] = 0        else:            img[:] = [b, g, r]    cv.destroyAllWindows()plot直方图 histogram直方图可以对图像灰度分布有一个整体了解，x轴上是灰度值（0到255）,y轴是图片中该灰度值的像素点的数目。"""# cv.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])"""# hist = cv.calcHist([img], [0], None, [256], [0, 256])## hist, bins = np.histogram(img.ravel(), 256, [0, 256])# mask = np.zeros(img.shape[:2], np.uint8)# mask[100:300, 100:400] = 255# masked_img = cv.bitwise_and(img, img, mask=mask)# grayimg = util.load_img('img/home.jpg',0)# plt.hist(img.ravel(), 256, [0, 256])# plt.show()mask = np.zeros(img.shape[:2], np.uint8)mask[100:300, 100:400] = 255masked_img = cv.bitwise_and(img, img, mask=mask)hist_full = cv.calcHist([img], [0], None, [256], [0, 256])hist_mask = cv.calcHist([img], [0], mask, [256], [0, 256])plt.subplot(221), plt.imshow(img, 'gray')plt.subplot(222), plt.imshow(mask, 'gray')plt.subplot(223), plt.imshow(masked_img, 'gray')plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)plt.xlim([0, 256])plt.show()# colorimg = util.load_img('img/home.jpg')color = ('b', 'g', 'r')for i, col in enumerate(color):    #    histr = cv.calcHist([img], [i], None, [256], [0, 256])    plt.plot(histr, color=col)    plt.xlim([0, 256])plt.show()直方图均衡增加图像整体的对比度，局部对比度较低的区域获得较高的对比度，缺点它可能会增加背景噪音的对比度 ，同时减少可用信号。img = util.load_img('img/contrast.png',0)equ = cv.equalizeHist(img)res = np.hstack((img, equ))  # stacking images side-by-sideutil.show(res)img = util.load_img('img/tsukuba.png', 0)# 直方图均衡equ = cv.equalizeHist(img)"""自适应直方图均衡解决 增加背景噪音的对比度 增加局部对比度图像被分成称为tile（8x8）的小块.每个小块进行直方图均衡, 加入对比度限制防止噪音被放大超过限制，像素剪切并均匀分布到其他区间，均衡后，为了去除图块边框中的瑕疵，应用双线性插值。"""clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))cl1 = clahe.apply(img)res = np.hstack((img, equ,cl1))util.show(res)直方图反向投影用于图像分割或查找图像中感兴趣的对象，创建的图像与我们的输入图像具有相同大小（但是单个通道）的图像，输出的图像我们感兴趣的部分像素值较高（更白）最好使用颜色直方图，物体的颜色信息比灰度图像更容易被分割和识别。再将颜色直方图投影到tar查找目标,找到输入图像中每一个像素点的像素值在直方图中对应的概率得到一个概率图像，最后设置适当的阈值对概率图像进行二值化roi = util.load_img('img/roi.png')roi_hsv = util.hsv(roi)tar = util.load_img('img/tar.png')tar_hsv = util.hsv(tar)# 计算目标直方图 颜色直方图优于灰度直方图roihist = cv.calcHist([roi_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])# 对象直方图进行normalize cv.calcBackProject返回概率图像cv.normalize(roihist, roihist, 0, 255, cv.NORM_MINMAX)dst = cv.calcBackProject([tar_hsv], [0, 1], roihist, [0, 180, 0, 256], 1)# 与disc kernel卷积disc = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5, 5))cv.filter2D(dst, -1, disc, dst)# threshold and binary ANDret, thresh = cv.threshold(dst, 50, 255, 0)# 使用merge变成通道图像thresh = cv.merge((thresh, thresh, thresh))# 蒙板res = cv.bitwise_and(tar, thresh)res = np.hstack((tar, thresh, res))util.show(res)模板匹配较大图像中搜索和查找模板图像位置的方法与2D卷积一样 模板图像在输入图像上滑动（类似窗口），在每一个位置对模板图像和输入图像的窗口区域进行匹配。 与直方图的反向投影类似。输入图像大小是W×H，模板大小是w×h，输出结果的大小(W-w+1,H-h+1)。得到此结果后可以使用函数cv2.minMaxLoc()来找到其中的最小值和最大值的位置。第一个值为矩形左上角的位置，(w,h)是模板矩形的宽度和高度。矩形就是模板区域。匹配一个roi = util.load_img('img/roi.png', 0)w, h = roi.shape[::-1]tar = util.load_img('img/tar.png', 0)methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',           'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']for meth in methods:    img = roi.copy()    method = eval(meth)    res = cv.matchTemplate(img, tar, method)    # 只匹配一个对象    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:        # 最小值会给出最佳匹配        top_left = min_loc    else:        top_left = max_loc    bottom_right = (top_left[0] + w, top_left[1] + h)    cv.rectangle(img, top_left, bottom_right, 255, 2)    plt.subplot(121), plt.imshow(res, cmap='gray')    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])    plt.subplot(122), plt.imshow(img, cmap='gray')    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])    plt.suptitle(meth)    plt.show()匹配多个img_rgb = util.load_img('img/mario.png')img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)template = cv.imread('img/mario_coin.png', 0)w, h = template.shape[::-1]res = cv.matchTemplate(img_gray, template, cv.TM_CCOEFF_NORMED)print(res.shape)threshold = 0.8# 注意 行列 高宽loc = np.where(res &gt;= threshold)for pt in zip(*loc[::-1]):    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0, 0, 255), 2)util.show(img_rgb)]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> opencv </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[cap video]]></title>
      <url>/blog/2019/04/11/cap</url>
      <content type="text"><![CDATA[摄像头cap = cv.VideoCapture(0)if not cap.isOpened():    print("Cannot open camera")    exit()"""cap.get/set(proid) proid是0到18的数字每个数组表示一个视频的属性https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get"""# print('width {:.2f}'.format(cap.get(3)))# print('height {:.2f}'.format(cap.get(4)))# 不要把set写到循环里，会闪瞎cap.set(3, 320)cap.set(4, 240)while True:    # Capture frame-by-frame    ret, frame = cap.read()    if ret:        # Our operations on the frame come here        frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)        # Display the resulting frame        cv.imshow('frame', frame)        if cv.waitKey(1) &amp; 0xFF == ord('q'):            break# When everything done, release the capturecap.release()cv.destroyAllWindows()videocap = cv.VideoCapture(path)while cap.isOpened():    # Capture frame-by-frame    ret, frame = cap.read()    if ret:        # Our operations on the frame come here        frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)        # Display the resulting frame        cv.imshow('frame', frame)        if cv.waitKey(1) &amp; 0xFF == ord('q'):            break    else:        print("Can't receive frame (stream end?). Exiting ...")        break# When everything done, release the capturecap.release()cv.destroyAllWindows()cap savedef cap_save():    cap = cv.VideoCapture(0)    # Define the codec and create VideoWriter object    width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH) + 0.5)    height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT) + 0.5)    size = (width, height)    # http://www.ntta.szm.com/Tutors/FourCC.htm#QT4CC    fourcc = cv.VideoWriter_fourcc(*'XVID')    out = cv.VideoWriter('output.avi', fourcc, 20.0, size, )    while cap.isOpened():        ret, frame = cap.read()        if not ret:            print("Can't receive frame (stream end?). Exiting ...")            break        # frame = cv.flip(frame, 0)        # write the flipped frame        out.write(frame)        cv.imshow('frame', frame)        if cv.waitKey(1) == ord('q'):            break    # Release everything if job is finished    cap.release()    out.release()    cv.destroyAllWindows()]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> opencv </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[设计模式 for python]]></title>
      <url>/blog/2019/04/05/design_pattern</url>
      <content type="text"><![CDATA[一直觉得设计模式是很高大上的东西，但其实有时自己用了也不知道，面试时被问得一愣一愣的，看一下书才发现，自己用过这么多。当然说不出==没用过╮(╯_╰)╭23种设计模式，分为三类：创造性 (creational)，结构性(structural)和行为性(behavioral)。creationalcreational 当不方便使用__init__创建对象时，提供更好的方法Factory,Builder,Prototype,SingletonstructuralAdapter,Decorator,Bridge,Facade,flyweight, model-view-controller (MVC), and proxy.behavioralChain of Responsibility,Command, Observer,State，Interpreter, Strategy, Mememto, Iterator, and TemplateFactory简化对象创建过程,将创建对象的代码与使用它的代码分离(client 端不知对象是怎样创建的)来降低维护应用程序的复杂性。factory method输入一些参数到一个单独函数得到想要的对象  集中了对象(逻辑上相似的对象)创建，使得跟踪对象变得更加容易，可以创建多个工厂方法，  优化应用性能和内存。工厂方法只有在绝对必要时才能通过创建新对象来提高性能和内存使用率。当我们使用直接类实例化创建对象时，每次创建新对象时都会分配额外的内存import jsonimport xml.etree.ElementTree as etreeclass JSONConnector:    def __init__(self, filepath):        self.data = dict()        with open(filepath, mode='r', encoding='utf8') as f:            self.data = json.load(f)    @property    def parsed_data(self):        return self.dataclass XMLConnector:    def __init__(self, filepath):        self.tree = etree.parse(filepath)    @property    def parsed_data(self):        return self.treedef connection_factory(filepath):    """ 工厂方法 """    if filepath.endswith('json'):        connector = JSONConnector    elif filepath.endswith('xml'):        connector = XMLConnector    else:        raise ValueError('Cannot connect to {}'.format(filepath))    return connector(filepath)abstract factory抽象工厂是工厂方法的（逻辑）组  When use abstract factory not factory method          从更简单的工厂方法开始，当工厂方法越来越多的时候      class Frog:    def __init__(self, name):        self.name = name    def __str__(self):        return self.name    def interact_with(self, obstacle):        """ 不同类型玩家遇到的障碍不同 """        print('{} the Frog encounters {} and {}!'.format(            self, obstacle, obstacle.action()))class Bug:    def __str__(self):        return 'a bug'    def action(self):        return 'eats it'class FrogWorld:    def __init__(self, name):        print(self)        self.player_name = name    def __str__(self):        return '\n\n\t----Frog World -----'    def make_character(self):        return Frog(self.player_name)    def make_obstacle(self):        return Bug()class Wizard:    def __init__(self, name):        self.name = name    def __str__(self):        return self.name    def interact_with(self, obstacle):        print('{} the Wizard battles against {} and {}!'.format(            self, obstacle, obstacle.action()))class Ork:    def __str__(self):        return 'an evil ork'    def action(self):        return 'kill it'class WizardWorld:    def __init__(self, name):        print(self)        self.player_name = name    def __str__(self):        return '\n\n\t------ Wizard World -------'    def make_character(self):        return Wizard(self.player_name)    def make_obstacle(self):        return Ork()class GameEnvironment:    """ 抽象工厂，根据不同的玩家类型创建不同的角色和障碍 (游戏环境)    这里可以根据年龄判断，成年人返回『巫师』游戏，小孩返回『青蛙过河』游戏"""    def __init__(self, factory):        self.hero = factory.make_character()        self.obstacle = factory.make_obstacle()    def play(self):        self.hero.interact_with(self.obstacle)                def main():    name = input("Hello. What's your name? ")    valid_input = False    while not valid_input:       valid_input, age = validate_age(name)    game = FrogWorld if age &lt; 18 else WizardWorld    environment = GameEnvironment(game(name))    environment.play()Builder我们想要创建一个由多个部分组成的对象，并且需要一步一步地完成组合。除非完全创建所有部件，否则该对象不完整。  factory diff          构建器模式在多个步骤中创建对象,当工厂模式立即返回创建的对象时，在构建器模式中，客户端代码显式要求导向器在需要时返回最终对象      # Directorclass Director(object):    def __init__(self):        self.builder = None     def construct_building(self):        self.builder.new_building()        self.builder.build_floor()        self.builder.build_size()     def get_building(self):        return self.builder.building  # Abstract Builderclass Builder(object):    def __init__(self):        self.building = None     def new_building(self):        self.building = Building()  # Concrete Builderclass BuilderHouse(Builder):    def build_floor(self):        self.building.floor = 'One'     def build_size(self):        self.building.size = 'Big'  class BuilderFlat(Builder):    def build_floor(self):        self.building.floor = 'More than One'     def build_size(self):        self.building.size = 'Small'  # Productclass Building(object):    def __init__(self):        self.floor = None        self.size = None     def __repr__(self):        return 'Floor: %s | Size: %s' % (self.floor, self.size)  # Clientif __name__ == "__main__":    director = Director()    director.builder = BuilderHouse()    director.construct_building()    building = director.get_building()    print(building)    director.builder = BuilderFlat()    director.construct_building()    building = director.get_building()    print(building)Prototype在原有的对象基础上创建对象 copy出两个独立的副本copy.deepcopySingleton需要创建一个且只有一个对象时，它很有用。维护一个全局状态等metaclass  A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.class SingletonType(type):       _instances = {}       def __call__(cls, *args, **kwargs):           if cls not in cls._instances:               cls._instances[cls] = super(SingletonType,               cls).__call__(*args, **kwargs)           return cls._instances[cls]Adapter解决接口不兼容,在不修改原有类代码的情况下实现新的功能class Adapter:    def __init__(self, obj, adapted_methods):        """ 不使用继承，使用__dict__属性实现适配器模式 """        self.obj = obj        self.__dict__.update(adapted_methods)    def __str__(self):        return str(self.obj)Decorator动态地给一个对象添加一些额外的职责对象添加新功能方式： 直接给对象所属的类添加方法，继承，组合，接口（Adapter适配器）class foo(object):    def f1(self):        print("original f1")     def f2(self):        print("original f2")  class foo_decorator(object):    def __init__(self, decoratee):        self._decoratee = decoratee     def f1(self):        print("decorated f1")        self._decoratee.f1()     def __getattr__(self, name):        return getattr(self._decoratee, name) u = foo()v = foo_decorator(u)v.f1()v.f2()Bridge多个对象共享实现将抽象部分与它的实现部分分离，使它们都可以独立地变化。class ResourceContentFetcher(metaclass=abc.ABCMeta):       """       Define the interface for implementation classes that fetch content.       """       @abc.abstractmethod       def fetch(path):            pass            class File(ResourceContentFetcher):    def fetch(path):          xxxFacade外观设计模式有助于我们隐藏系统的内部复杂性，并通过简化的界面仅向客户端公开必要的内容引入facade 将这个子系统与客户以及其他的子系统分离，可以提高子系统的独立性和可移植性。使用facade模式定义子系统中每层的入口点。如果子系统之间是相互依赖的，你可以让它们仅通过facade进行通讯，从而简化了它们之间的依赖关系。class A:    def run():        passclass B:    def run():        pass#  Facadeclass Facade:    def __init__(self):         self.run_list=[A(),B()]    def run(self):        [r.run() for r in self.run_list]Facade().run()Flyweight对象创建带来的性能和内存占用问题,实现对象复用从而改善资源使用,对象之间引入数据共享来最小化内存使用并提高性能的技术。flyweight是一个共享对象，它包含与状态无关的，不可变的（也称为内部）数据。可变（也称为外在）数据不应该是flyweight的一部分，因为这是无法共享的信息，因为它对每个对象不同。如果flyweight需要外部数据，则应由客户端代码明确提供。import weakrefclass Card(object):    """The object pool. Has builtin reference counting"""    _CardPool = weakref.WeakValueDictionary()    """Flyweight implementation. If the object exists in the    pool just return it (instead of creating a new one)"""    def __new__(cls, value, suit):        obj = Card._CardPool.get(value + suit, None)        if not obj:            obj = object.__new__(cls)            Card._CardPool[value + suit] = obj            obj.value, obj.suit = value, suit        return obj    # def __init__(self, value, suit):    #     self.value, self.suit = value, suit    def __repr__(self):        return "&lt;Card: %s%s&gt;" % (self.value, self.suit)if __name__ == '__main__':    # comment __new__ and uncomment __init__ to see the difference    c1 = Card('9', 'h')    c2 = Card('9', 'h')    print(c1, c2)    print(c1 == c2)    print(id(c1), id(c2))MVC关注点分离（SoC）原理。 SoC原则背后的想法是将应用程序拆分为不同的部分，其中每个部分都涉及一个单独的问题。模型，控制器，视图 数据访问层，业务逻辑层，表示层模型是核心组成部分，包含并管理应用程序的（业务）逻辑，数据，状态和规则。视图是模型的直观表示。视图仅显示数据;它没有处理它。控制器是模型和视图之间的链接/粘合剂。模型和视图之间的所有通信都通过控制器进行。不修改模型的情况下使用多个视图，为了实现模型与视图之间的分离，每个视图通常都需要自己的控制器。如果模型直接与特定视图通信，我们将无法使用多个视图Proxy代理 访问实际对象之前执行重要操作  remote proxy 访问远程对象就像本地访问一样  virtual proxy 延迟初始化来推迟创建计算昂贵的对象，直到实际需要它为止,带来显着的性能改进  protection/protective proxy 用于控制对敏感对象的访问  smart(reference) proxy 在访问对象时执行额外操作。此类操作的示例是引用计数和线程安全检查。protection/protective proxyimport timeclass SalesManager:    def work(self):        print("Sales Manager working...")    def talk(self):        print("Sales Manager ready to talk")class Proxy:    def __init__(self):        self.busy = 'No'        self.sales = None    def work(self):        print("Proxy checking for Sales Manager availability")        if self.busy == 'No':            self.sales = SalesManager()            time.sleep(2)            self.sales.talk()        else:            time.sleep(2)            print("Sales Manager is busy")if __name__ == '__main__':    p = Proxy()    p.work()    p.busy = 'Yes'    p.work()virtual proxyclass LazyProperty:    """ 用描述符实现延迟加载的属性 """    def __init__(self, method):        self.method = method        self.method_name = method.__name__    def __get__(self, obj, cls):        if not obj:            return None        value = self.method(obj)        print('value {}'.format(value))        setattr(obj, self.method_name, value)        return valueclass Test:    def __init__(self):        self.x = 'foo'        self.y = 'bar'        self._resource = None    @LazyProperty    def resource(self):    # 构造函数里没有初始化，第一次访问才会被调用        print('initializing self._resource which is: {}'.format(self._resource))        self._resource = tuple(range(5))    # 模拟一个耗时计算        return self._resourcet = Test()print(t.x)print(t.y)# 访问LazyProperty, resource里的print语句只执行一次，实现了延迟加载和一次执行print(t.resource)print(t.resource)Chain of Responsibility使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。在责任连模式中，我们把消息发送给一系列对象的首个节点，对象可以选择处理消息或者向下一个对象传递,只有对消息感兴趣的节点处理。用来解耦发送者和接收者。如果所有请求都可以由单个处理元素处理，则责任链模式不是非常有用，除非我们真的不知道哪个元素。这种模式的价值在于它提供的解耦。客户端和所有处理元素之间没有多对多的关系（对于处理元素和所有其他处理元素之间的关系也是如此），客户端只需要知道如何与开始进行通信(链头)。处理请求class Handler:    def successor(self, successor):        self.successor = successorclass ConcreteHandler1(Handler):    def handle(self, request):        if 0 &lt; request &lt;= 10:            print("in handler1")        else:            self.successor.handle(request)class ConcreteHandler2(Handler):    def handle(self, request):        if 10 &lt; request &lt;= 20:            print("in handler2")        else:            self.successor.handle(request)class ConcreteHandler3(Handler):    def handle(self, request):        if 20 &lt; request &lt;= 30:            print("in handler3")        else:            print('end of chain, no handler for {}'.format(request))class Client:    def __init__(self):        h1 = ConcreteHandler1()        h2 = ConcreteHandler2()        h3 = ConcreteHandler3()        h1.successor(h2)        h2.successor(h3)        requests = [2, 5, 14, 22, 18, 3, 35, 27, 20]        for request in requests:            h1.handle(request)if __name__ == "__main__":    client = Client()eventclass Event:    def __init__(self, name):        self.name = name    def __str__(self):        return self.nameclass Widget:    """Docstring for Widget. """    def __init__(self, parent=None):        self.parent = parent    def handle(self, event):        handler = 'handle_{}'.format(event)        if hasattr(self, handler):            method = getattr(self, handler)            method(event)        elif self.parent:            self.parent.handle(event)        elif hasattr(self, 'handle_default'):            self.handle_default(event)class MainWindow(Widget):    def handle_close(self, event):        print('MainWindow: {}'.format(event))    def handle_default(self, event):        print('MainWindow: Default {}'.format(event))class SendDialog(Widget):    def handle_paint(self, event):        print('SendDialog: {}'.format(event))class MsgText(Widget):    def handle_down(self, event):        print('MsgText: {}'.format(event))def main():    mw = MainWindow()    sd = SendDialog(mw)    # parent是mw    msg = MsgText(sd)    for e in ('down', 'paint', 'unhandled', 'close'):        evt = Event(e)        print('\nSending event -{}- to MainWindow'.format(evt))        mw.handle(evt)        print('Sending event -{}- to SendDialog'.format(evt))        sd.handle(evt)        print('Sending event -{}- to MsgText'.format(evt))        msg.handle(evt)if __name__ == "__main__":    main()Command把一个操作封装成一个对象。命令模式可以控制命令的执行时间和过程，在不同的时刻指定、排列和执行请求，复杂的命令还可以分组支持取消操作。Command的Excute 操作可在实施操作前将状态存储起来，在取消操作时这个状态用来消除该操作的影响。import os class MoveFileCommand(object):    def __init__(self, src, dest):        self.src = src        self.dest = dest     def execute(self):        self()     def __call__(self):        print('renaming {} to {}'.format(self.src, self.dest))        os.rename(self.src, self.dest)     def undo(self):        print('renaming {} to {}'.format(self.dest, self.src))        os.rename(self.dest, self.src)  if __name__ == "__main__":    command_stack = []     # commands are just pushed into the command stack    command_stack.append(MoveFileCommand('foo.txt', 'bar.txt'))    command_stack.append(MoveFileCommand('bar.txt', 'baz.txt'))     # they can be executed later on    for cmd in command_stack:        cmd.execute()     # and can also be undone at will    for cmd in reversed(command_stack):        cmd.undo()Observer一对多的依赖关系,当一个对象的状态发生改变时, 所有依赖于它的对象都得到通知并被自动更新。e.g 事件驱动class Publisher:    def __init__(self):        self.observers = []    def add(self, observer):        if observer not in self.observers:            self.observers.append(observer)        else:            print('Failed to add : {}').format(observer)    def remove(self, observer):        try:            self.observers.remove(observer)        except ValueError:            print('Failed to remove : {}').format(observer)    def notify(self):        [o.notify_by(self) for o in self.observers]class DefaultFormatter(Publisher):    def __init__(self, name):        super().__init__()        self.name = name        self._data = 0    def __str__(self):        return "{}: '{}' has data = {}".format(            type(self).__name__, self.name, self._data)    @property    def data(self):        return self._data    @data.setter    def data(self, new_value):        try:            self._data = int(new_value)        except ValueError as e:            print('Error: {}'.format(e))        else:            self.notify()    # data 在被合法赋值以后会执行notifyclass HexFormatter:    """ 订阅者 """    def notify_by(self, publisher):        print("{}: '{}' has now hex data = {}".format(            type(self).__name__, publisher.name, hex(publisher.data)))class BinaryFormatter:    """ 订阅者 """    def notify_by(self, publisher):        print("{}: '{}' has now bin data = {}".format(            type(self).__name__, publisher.name, bin(publisher.data)))if __name__ == "__main__":    df = DefaultFormatter('test1')    print(df)    print()    hf = HexFormatter()    df.add(hf)    df.data = 3    print(df)    print()    bf = BinaryFormatter()    df.add(bf)    df.data = 21    print(df)State面向对象编程（OOP）侧重于维护彼此交互的对象的状态。在解决许多问题时模拟状态转换的非常方便的工具被称为有限状态机状态机是一个抽象机器，它有两个关键组件，即状态和转换。状态机在任何时间点只能有一个活动状态。转换是从当前状态切换到新状态。  状态是系统的当前（活动）状态。  转换是从一种状态切换到另一种状态状态机的一个很好的特性是它们可以表示为图形（称为状态图），其中每个状态是一个节点，每个转换是两个节点之间的边缘。state_machine@acts_as_state_machineclass Person():    name = 'Billy'    sleeping = State(initial=True)    running = State()    cleaning = State()    run = Event(from_states=sleeping, to_state=running)    cleanup = Event(from_states=running, to_state=cleaning)    sleep = Event(from_states=(running, cleaning), to_state=sleeping)    @before('sleep')    def do_one_thing(self):        print "{} is sleepy".format(self.name)    @before('sleep')    def do_another_thing(self):        print "{} is REALLY sleepy".format(self.name)    @after('sleep')    def snore(self):        print "Zzzzzzzzzzzz"    @after('sleep')    def big_snore(self):        print "Zzzzzzzzzzzzzzzzzzzzzz"person = Person()print person.current_state == Person.sleeping       # Trueprint person.is_sleeping                            # Trueprint person.is_running                             # Falseperson.run()print person.is_running                             # Trueperson.sleep()# Billy is sleepy# Billy is REALLY sleepy# Zzzzzzzzzzzz# Zzzzzzzzzzzzzzzzzzzzzzprint person.is_sleeping                            # TrueInterpreter给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子当有一个语言需要解释执行, 并且你可将该语言中的句子表示为一个抽象语法树时，可使用解释器模式。而当存在以下情况时该模式效果最好：该文法简单对于复杂的文法, 文法的类层次变得庞大而无法管理。此时语法分析程序生成器这样的工具是更好的选择。它们无需构建抽象语法树即可解释表达式, 这样可以节省空间而且还可能节省时间。效率不是一个关键问题最高效的解释器通常不是通过直接解释语法分析树实现的, 而是首先将它们转换成另一种形式。例如，正则表达式通常被转换成状态机。但即使在这种情况下, 转换器仍可用解释器模式实现, 该模式仍是有用的。Strategy定义一系列的算法,把它们一个个封装起来, 并且根据某些条件使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。每当我们希望能够动态和透明地应用不同的算法时，策略就是可行的方法Memento对象的内部状态快照  Memento，一个包含基本状态存储和检索功能的简单对象  Originator，一个获取和设置Memento实例的值的对象  Caretaker，存储和检索所有以前创建Memento实例pickleTemplate该模式侧重于消除代码冗余。我们的想法是，我们应该能够在不改变其结构的情况下重新定义算法的某些部分。模板设计模式侧重于消除代码重复。如果我们注意到算法中存在具有结构相似性的可重复代码，我们可以将算法的不变（公共）部分保留在模板方法/函数中，并在动作/钩子方法/函数中移动变体（不同）部分。def dots_style(msg):    msg = msg.capitalize()    msg = '.' * 10 + msg + '.' * 10    return msgdef admire_style(msg):    msg = msg.upper()    return '!'.join(msg)def generate_banner(msg, style=dots_style):    print('-- start of banner --')    print(style(msg))    print('-- end of banner --\n\n')def main():    msg = 'happy coding'    [generate_banner(msg, style) for style in (dots_style, admire_style)]if __name__ == "__main__":    main()]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[密码]]></title>
      <url>/blog/2019/03/30/crypto</url>
      <content type="text"><![CDATA[私钥/公钥加密算法（对称/非对称）很讨厌一样东西有两样名字，考题考官总能拿没听过的考你🤣算法生成一对密钥用于加密解密  密钥相同 对称/私钥 加密  密钥不同 非对称/公钥 加密 公钥可公开，私钥自己留着，同一密钥对可以解对方加的密常用算法对称AES,DES,3DES，RC5、RC6  加密解密效率高，速度快，适合进行大数据量的加解密非对称RSA、DSA、ECC  算法复杂，加解密速度慢，但安全性高hashMD5、SHA系列、HMAC 加盐同输入同算法同输出，同输出不一定输入相同(不能反推)密钥的格式ASN.1 format本身只是表示接口数据的通用语法，没有限定编码方法。是一种协议RSA生成的密钥就是使用ASN.1中的DER(Distinguished Encoding Rules)来编码成二进制PEM 对DER编码转码为Base64，解码后，可以还原为DER格式-----BEGIN PUBLIC KEY-----xxx-----END PUBLIC KEY-----PEM 格式  PKCS1专门为 RSA 密钥进行定义 有 RSA 字样-----BEGIN RSA PRIVATE KEY-----BASE64 DATA-----END RSA PRIVATE KEY-----  PKCS8一种密钥格式的通用方案，不仅仅为 RSA 所使用，也可以被其它密钥所使用消息加密  通信安全用非对称加密，对称的话密钥泄漏就完蛋  S : 发送者  R : 接收者数字签名  确保消息来源,不被篡改  S : 发送者  R : 接收者  M: Man-in-the-middle attack  这里的原始消息也可以经过加密，这样就不是明文了，安全性更好  Why 加密摘要 not 直接发摘要  直接发摘要M可以替换发送的消息和摘要，但无法完成加密。通过私钥加密证明发送者的身份，只有唯一的公钥解密，其他人无法伪造。  Why 加密摘要 not 加密消息  消息量大的话加密时间长，摘要是固定长度的。数字证书当R拿到S公钥其实是M公钥时，一切崩坏。M可以完全取代S发送消息，R会觉得M才是S，S是其他人。  证书中心（certificate authority，简称CA）为公钥做认证。证书中心用自己的私钥，对S公钥和一些相关信息一起加密签名，生成数字证书（Digital Certificate）知名CA机构的根证书会内置于游览器中，用来确保 CA 机构本身的身份，其中就包含了 CA 机构自身的公钥。会验证数字证书可靠性。  Why CA 公钥非伪造          CA是第三方机构，CA公钥是公开的，接收方可以跟别人比对（比如在网上查询），因此不可能伪造。但是发送方公钥，接收方是通过通信得到的，收到后无法验证。      HTTPSHello 阶段  Client端发送https请求      支持的协议版本    随机数 random1，稍后用于生成”对话密钥”。    支持的加密方法等信息    Server端      确认使用的加密通信协议版本    发送随机数 random2，稍后用于生成”对话密钥”。    确认加密方法等信息    发送数字证书  验证数字证书      CA身份验证/过期 CA根证书验证: 浏览器内置一个受信任的CA机构列表，并保存了这些CA机构的证书。        Server端的数字证书信息与当前正在访问的网站（域名等）一致: Server端是可信的 -&gt; 获得Server公钥    Client端是否能够信任这个站点的证书，首先取决于Client端程序是否导入了证书颁发者的根证书。抓包工具要抓https也要导入工具的证书协商通信密钥传输内容的加密是采用的对称加密，对加密的密钥使用公钥进行非对称加密  Server端验证客户端(optional) 客户端的证书 + 随机数(Client端签名) = 客户端身份验证  验证通过后Client端生成 随机码(PreMaster key)Server公钥加密 [+ 客户端身份验证] -&gt; Server端PreMaster key =   RSA或者Diffie-Hellman等加密算法生成  PreMaster key是在客户端使用RSA或者Diffie-Hellman等加密算法生成的。  PreMaster key前两个字节是TLS的版本号，这是一个比较重要的用来核对握手数据的版本号，因为在Client Hello阶段，客户端会发送一份加密套件列表和当前支持的SSL/TLS的版本号给服务端，而且是使用明文传送的，如果握手的数据包被破解之后，攻击者很有可能串改数据包，选择一个安全性较低的加密套件和版本给服务端，从而对数据进行破解。所以，服务端需要对密文中解密出来对的PreMaster版本号跟之前Client Hello阶段的版本号进行对比，如果版本号变低，则说明被串改，则立即停止发送任何消息。  Server端验证 客户端身份(optional)过后，解密得到PreMaster key      双端生成Master key = 一系列算法(PreMaster key + random1 +  random2)    双端：发送ChangeCipherSpec协议，数据包中就是一个字节的数据，用于告知服务端，客户端已经切换到之前协商好的加密套件（Cipher Suite）的状态，准备使用之前协商好的加密套件加密数据并传输了。  双端：协商好的加密套件和Session Secret加密一段 Finish 的数据传送给服务端，此数据是为了在正式传输应用数据之前对刚刚握手建立起来的加解密通道进行验证。  如果PreMaster key泄漏 完蛋了通信过程S: 加密(消息+消息摘要) R: 解密 对比摘要hash 密码引用 hashing-security保护密码的最好办法是使用加盐密码哈希salted password hashing放弃编写自己的密码哈希加密代码的念头，要使用成熟方案。hash what哈希算法散列算法是单向函数。他们将任意数量的数据转换为固定长度的指纹，无法逆转。破解hash  字典攻击 Dictionary  字典攻击使用包含单词，短语，公共密码和其他可能用作密码的字符串的文件。对文件中的每个单词进行哈希处理，并将其哈希值与密码哈希值进行比较。  暴力攻击 Brute Force Attacks  对于给定的密码长度，尝试每一种可能的字符组合。这种方式会消耗大量的计算，也是破解哈希加密效率最低的办法，但最终会找出正确的密码。因此密码应该足够长，以至于遍历所有可能的字符组合，耗费的时间太长令人无法承受，从而放弃破解。  查表法（ Lookup Tables）  查找表是一种非常有效的方法，可以非常快速地破解相同类型的多个哈希值。一般的想法是在密码字典中预先计算密码的哈希值，并将它们及其相应的密码存储在查找表数据结构中。查找表的良好实现每秒可以处理数百个哈希查找，即使它们包含数十亿个哈希值  反向查表法（ Reverse Lookup Tables）  这种攻击允许攻击者无需预先计算好查询表的情况下同时对多个哈希值发起字典攻击或暴力攻击。首先，攻击者从被黑的用户帐号数据库创建一个用户名和对应的密码哈希表，然后，攻击者猜测一系列哈希值并使用该查询表来查找使用此密码的用户。通常许多用户都会使用相同的密码，因此这种攻击方式特别有效。  彩虹表（ Rainbow Tables）  与查表法相似，只是它为了使查询表更小，牺牲了破解速度。因为彩虹表更小，所以在单位空间可以存储更多的哈希值，从而使攻击更有效。salt查找表和彩虹表起作用，因为每个密码都以完全相同的方式进行哈希处理。相同的密码有相同的密码哈希值密码中加入一段随机字符串再进行哈希加密，这个被加的字符串称之为盐，使得相同的密码每次都被加密为完全不同的字符串。我们需要盐值来校验密码是否正确。通常和密码哈希值一同存储在帐号数据库中，或者作为哈希字符串的一部分。盐值无需加密。由于随机化了哈希值，查表法、反向查表法和彩虹表都会失效。错误用法  盐值复用（ Salt Reuse）  相同的密码，他们仍然会有相同的哈希值。攻击者仍然可以使用反向查表法对每个哈希值进行字典攻击 每次用户创建帐户或更改密码时，都必须生成新的随机盐  短盐值（ Short Slat）  攻击者可以为每种可能的盐构建查找表 为了使攻击者无法为每个可能的盐创建查找表，盐必须很长。一个好的经验法则是使用与散列函数输出大小相同的salt。例如，SHA256的输出是256位（32字节），因此salt应至少为32个随机字节  古怪哈希的算法组合  尝试不同的哈希函数相结合一起使用，希望让数据会更安全。但在实践中，这样做并没有什么好处。它带来了函数之间互通性的问题，而且甚至可能会使哈希变得更不安全。永远不要试图去创造你自己的哈希加密算法，要使用专家设计好的标准算法。当攻击者不知道哈希加密算法的时候，是无法发起攻击的。但是要考虑到Kerckhoffs的原则，攻击者通常会获得源代码（尤其是免费或者开源软件）。通过系统中找出密码 - 哈希值对应关系，很容易反向推导出加密算法。正确做法  加盐  Web应用中永远在服务端上进行哈希加密  慢哈希函数（ Slow Hash Function）为了降低使这些攻击的效率，我们可以使用一个叫做密钥扩展（ key stretching）的技术。密钥扩展的实现使用了一种 CPU 密集型哈希函数（ CPU-intensive hash function）  这类算法采取安全因子或迭代次数作为参数。此值决定哈希函数将会如何缓慢。对于桌面软件或智能手机应用，确定这个参数的最佳方式是在设备上运行很短的性能基准测试，找到使哈希大约花费半秒的值。通过这种方式，程序可以尽可能保证安全而又不影响用户体验。缺点  需要额外的计算资源来处理大量的身份认证请求，并且密钥扩展也容易让服务端遭受DDoS  加密hash  ASE算法对哈希值加密 密钥必须被存储在外部系统codedef to_bytes(bytes_or_str):    if isinstance(bytes_or_str, str):        return bytes_or_str.encode()    return bytes_or_strdef to_str(bytes_or_str):    if isinstance(bytes_or_str, bytes):        return bytes_or_str.decode()    return bytes_or_strbase64def t_b64():    import base64    def encode(raw):        """        二进制数据进行Base64编码        将非ASCII字符的数据转换成ASCII字符的一种方法 对数据内容进行编码来适合传输、保存，e.g ascii码有些值不可见        根证书，email附件 Base64编码图片        """        return base64.b64encode(to_bytes(raw))    def decode(raw_b):        return to_str(base64.b64decode(raw_b))    encode_b = encode('fafaf个把个关')    print(encode_b)  # b'ZmFmYWbkuKrmiorkuKrlhbM='    print(decode(encode_b))  # fafaf个把个关def gen_user_token(login_type: int) -&gt; str:    raw = get_random_bytes(USER_TOKEN_BITS) + struct.pack('&gt;L', int(time.time()))    return safe_base64_encode(raw).decode()def rand_byte2b64(bits: int) -&gt; bytes:    return base64.urlsafe_b64encode(gen_random_key(bits))def safe_base64_encode(s: bytes, encode_func: typing.Callable = base64.urlsafe_b64encode) -&gt; bytes:    # cookie 不支持 =    return encode_func(s).replace(b'=', b'')def safe_base64_decode(s: bytes, decode_func: typing.Callable = base64.urlsafe_b64decode) -&gt; bytes:    s += b'==='    b = s[:-1 * (len(s) % 4)]    return decode_func(b)hashdef t_hmac():    import hmac    import hashlib    def encode(secret_key, payload):        """        加密用的散列函数 64位  md5、sha3 消息无限大        """        # return hmac.new(to_bytes(secret_key), to_bytes(payload), digestmod=hashlib.sha256).digest()        return hmac.new(to_bytes(secret_key), to_bytes(payload), digestmod=hashlib.sha256).hexdigest()    signature = encode('21a1f34d2c785f296c90ba54ead31d98e5c1838351cdfea0434c9edfcd1a0252', 'wo我是msg')    print(signature)def t_md5():    import hashlib    def encode(payload):        """        32 位 hash函数        """        # return hashlib.md5(to_bytes(payload)).digest()        return hashlib.md5(to_bytes(payload)).hexdigest()    print(encode('wo我是msgfafafdfdfadfafafafafafa'))# pysodium hashENCODED_PASSWORD_BYTES = pysodium.crypto_auth_KEYBYTES + pysodium.crypto_pwhash_SALTBYTESdef _crypto_hash(password: str, salt: bytes) -&gt; bytes:    return pysodium.crypto_pwhash(pysodium.crypto_auth_KEYBYTES, password, salt,                                  pysodium.crypto_pwhash_argon2id_OPSLIMIT_INTERACTIVE,                                  pysodium.crypto_pwhash_argon2id_MEMLIMIT_INTERACTIVE,                                  pysodium.crypto_pwhash_ALG_ARGON2ID13)def encode_password(password) -&gt; bytes:    salt = _generate_salt()    password_hash = _crypto_hash(password, salt)    return password_hash + saltdef verify_password(password, encoded_password) -&gt; bool:    if len(encoded_password) != ENCODED_PASSWORD_BYTES:        return False    password_hash = encoded_password[:pysodium.crypto_auth_KEYBYTES]    salt = encoded_password[-pysodium.crypto_pwhash_SALTBYTES:]    return _crypto_hash(password, salt) == password_hashpycryptodomeRSAdef t_RSA(msg, passwd=None):    from Crypto.PublicKey import RSA    from Crypto.Cipher import PKCS1_v1_5 as Cipher_pkcs1_v1_5    from Crypto.Signature import PKCS1_v1_5 as Signature_pkcs1_v1_5    from Crypto.Hash import SHA256    def generate(bits=2048, passwd=None, path='id_rsa', ):        key = RSA.generate(bits)        private_key = key.export_key(passphrase=passwd)        with open(path + '.pub', "wb")as pub:            pub.write(key.publickey().export_key())        with open(path, "wb")as pub:            pub.write(private_key)    def encrypt(msg, passwd=None, path='id_rsa'):        """公钥加密"""        msg = to_bytes(msg)        key = RSA.import_key(open(path + '.pub').read(), passphrase=passwd)        cipher = Cipher_pkcs1_v1_5.new(key)        result = cipher.encrypt(msg)        return result    def decrypt(encrypt_txt, passwd=None, path='id_rsa'):        key = RSA.import_key(open(path).read(), passphrase=passwd)        cipher = Cipher_pkcs1_v1_5.new(key)        result = cipher.decrypt(encrypt_txt, None)        return result    def sign(msg, passwd=None, path='id_rsa'):        msg = to_bytes(msg)        key = RSA.import_key(open(path).read(), passphrase=passwd)        signer = Signature_pkcs1_v1_5.new(key)        digest = SHA256.new(msg)        signature = signer.sign(digest)        return signature    def check_sign(msg, sign, passwd=None, path='id_rsa'):        msg = to_bytes(msg)        key = RSA.import_key(open(path + '.pub').read(), passphrase=passwd)        signer = Signature_pkcs1_v1_5.new(key)        digest = SHA256.new(msg)        assert signer.verify(digest, sign) is True    generate(passwd=passwd)    result = decrypt(encrypt(msg), passwd)    assert msg == to_str(result)    signature = sign(msg, passwd)    check_sign(msg, signature)AESdef t_AES(msg, passwd):    from Crypto.Cipher import AES    from Crypto import Random    import hashlib    import base64    BS = AES.block_size    """    pad和unpad分别是填充函数和逆填充函数     文本长度正好是BlockSize长度的倍数，也会填充一个BlockSize长度的值    缺几位就补几 要填充8个字节,那么填充的字节的值就是0x08        AES加密对加密文本有长度要求    必须是AES.block_size的整倍数        """    pad = lambda s: s + (BS - len(s) % BS) * chr(BS - len(s) % BS)    unpad = lambda s: s[:-ord(s[len(s) - 1:])]    """    实际上AES加密有AES-128、AES-192、AES-256三种，    分别对应三种密钥长度128bits（16字节）、192bits（24字节）、256bits（32字节）。    当然，密钥越长，安全性越高，加解密花费时间也越长。    """    passwd = hashlib.sha256(to_bytes(passwd)).digest()    def encrypt(msg):        msg = to_bytes(pad(msg))        iv = Random.new().read(BS)        cipher = AES.new(passwd, mode=AES.MODE_CBC, iv=iv)        result = cipher.encrypt(msg)        return base64.b64encode(iv + result)    def decrypt(encrypt_txt):        encrypt_txt = base64.b64decode(encrypt_txt)        iv = encrypt_txt[:BS]        cipher = AES.new(passwd, mode=AES.MODE_CBC, iv=iv)        result = unpad(cipher.decrypt(encrypt_txt[BS:]))        return result    result = decrypt(encrypt(msg))    assert msg == to_str(result)完善版python 加密解密def sign(private_key, data: bytes) -&gt; bytes:    h = SHA256.new(data)    return pkcs1_15.new(private_key).sign(h)def verify_sign(pub_key, data, signature) -&gt; bool:    h = SHA256.new(data)    try:        pkcs1_15.new(pub_key).verify(h, signature)    except (ValueError, TypeError):        raise False    return Truedef aes_encrypt(aes_key: bytes, data: bytes) -&gt; bytes:    cipher = AES.new(aes_key, AES.MODE_CBC)    ct = cipher.encrypt(pad(data, AES.block_size))    return cipher.iv + ctdef aes_decrypt(aes_key: bytes, data: bytes) -&gt; bytes:    iv, ct = data[:AES.block_size], data[AES.block_size:]  # 确认 iv 未被使用    cipher = AES.new(aes_key, AES.MODE_CBC, iv)    return unpad(cipher.decrypt(ct), AES.block_size)def encrypt_license(data: typing.Union[dict, list], aes_key: bytes, schema: dict = LICENSE_SCHEMA) -&gt; bytes:    data = avro_encode(data, schema)    private_key = SERVER_PRIVATE_KEY    # hash 签名    signature = sign(private_key, data)    signed_data = data + signature    # aes 加密    license_data = aes_encrypt(aes_key, signed_data)    return license_datadef decrypt_license(license_data: bytes, aes_key: bytes, schema: dict = LICENSE_SCHEMA) -&gt; typing.Optional[typing.Union[dict, list]]:    pub_key = CLIENT_PUB_KEY    # aes 解密    data = aes_decrypt(aes_key, license_data)    # 验证签名    data, signature = data[:-512], data[-512:]    if not verify_sign(pub_key, data, signature):        return None    data = avro_decode(data, schema)    return datadef gen_rsa_keys():    """生成并返回私钥和公钥"""    key = RSA.generate(4096)    private_key = key    pub_key = key.publickey()    return private_key, pub_keydef init_rsa_keys():    global SERVER_PRIVATE_KEY, CLIENT_PUB_KEY, CLIENT_PRIVATE_KEY, SERVER_PUB_KEY    def init_key(private_path, pub_path):        if os.path.exists(private_path) and os.path.exists(pub_path):            with open(private_path) as f:                private_key = f.read()            private_key = RSA.import_key(private_key)            with open(pub_path) as f:                pub_key = f.read()            pub_key = RSA.import_key(pub_key)            return private_key, pub_key        private_key, pub_key = gen_rsa_keys()        with open(private_path, 'wb') as f:            f.write(private_key.export_key())        with open(pub_path, 'wb') as f:            f.write(pub_key.export_key())        return private_key, pub_key    SERVER_PRIVATE_KEY, CLIENT_PUB_KEY = init_key(_SERVER_PRIVATE_KEY_PATH, _CLIENT_PUB_KEY_PATH)    CLIENT_PRIVATE_KEY, SERVER_PUB_KEY = init_key(_CLIENT_PRIVATE_KEY_PATH, _SERVER_PUB_KEY_PATH)# AES keydef gen_random_key(bits: int) -&gt; bytes:    return get_random_bytes(bits)golang 加密解密func pkcs7Unpadding(data []byte) []byte {	pDataLen := len(data)	paddingLen := int(data[pDataLen-1])	return data[:(pDataLen - paddingLen)]}func pkcs7Padding(data []byte, blockSize int) []byte {	padding := blockSize - len(data)%blockSize	padtext := bytes.Repeat([]byte{byte(padding)}, padding)	return append(data, padtext...)}func aesEncrypt(aesKey []byte, data []byte) (cipherText []byte, err error) {	block, err := aes.NewCipher(aesKey)	if err != nil {		fmt.Println(err)		return	}	blockSize := block.BlockSize()	data = pkcs7Padding(data, blockSize)	cipherText = make([]byte, blockSize+len(data))	iv := cipherText[:blockSize]	if _, err := io.ReadFull(rand.Reader, iv); err != nil {		panic(err)	}	mode := cipher.NewCBCEncrypter(block, iv)	mode.CryptBlocks(cipherText[blockSize:], data)	return}func aesDecrypt(aesKey []byte, data []byte) (b []byte, err error) {	block, err := aes.NewCipher(aesKey)	if err != nil {		fmt.Println(err)		return	}	iv, ct := data[:block.BlockSize()], data[block.BlockSize():]	blockMode := cipher.NewCBCDecrypter(block, iv)	origData := make([]byte, len(ct))	blockMode.CryptBlocks(origData, ct)	b = pkcs7Unpadding(origData)	return}func getPubKey() (pubKey *rsa.PublicKey) {	pub, _ := ioutil.ReadFile("client.pub")	block, _ := pem.Decode(pub)	parsedKey, _ := x509.ParsePKIXPublicKey(block.Bytes)	pubKey = parsedKey.(*rsa.PublicKey)	return}func getPrivateKey() (privateKey *rsa.PrivateKey) {	private, err := ioutil.ReadFile("client")	if err != nil {		fmt.Println(err)		return	}	pubPem, _ := pem.Decode(private)	privateKey, _ = x509.ParsePKCS1PrivateKey(pubPem.Bytes)	return}func sign(data []byte) (s []byte) {	privateKey := getPrivateKey()	h := sha256.New()	h.Write(data)	s, _ = rsa.SignPKCS1v15(rand.Reader, privateKey, crypto.SHA256, h.Sum(nil))	return}func verifySign(data []byte, signature []byte) bool {	hash := sha256.New()	hash.Write(data)	pubKey := getPubKey()	err := rsa.VerifyPKCS1v15(pubKey, crypto.SHA256, hash.Sum(nil), signature)	if err != nil {		return false	}	return true}func DecryptLicense(licenseData []byte, aesKey []byte, schema string) (t map[string]interface{}, err error) {	// aes 解密	origData, err := aesDecrypt(aesKey, licenseData)	if err != nil {		fmt.Println(err)		return	}	// 签名验证	origDataLen := len(origData)	data, signature := origData[:origDataLen-512], origData[origDataLen-512:]	ok := verifySign(data, signature)	if ok == false {		fmt.Println("sign verify fail")		return nil, errors.New("sign verify fail")	}	license, err := avroDecode(data, schema)	if err != nil {		fmt.Println(err)		return nil, errors.New("avro decode fail")	}	t, _ = license.(map[string]interface{})	return}func EncryptLicense(licenseData interface{}, aesKey []byte, schema string) (license []byte, err error) {	data, err := avroEncode(licenseData, schema)	if err != nil {		fmt.Println(err)		return	}	buffer := bytes.Buffer{}	buffer.Write(data)	signature := sign(data)	buffer.Write(signature)	license, err = aesEncrypt(aesKey, buffer.Bytes())	if err != nil {		fmt.Println(err)		return	}	return}]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> 密码 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概念]]></title>
      <url>/blog/2019/03/28/fact_dim</url>
      <content type="text"><![CDATA[事实表保存详细记录，包含多个维度的大量行表每个数据仓库都包含一个或者多个事实数据表。事实数据表通常包含大量的行。事实数据表的主要特点是包含数字数据(事实)，并且这些数字信息可以汇总，以提供有关单位作为历史的数据，每个事实数据表包含一个由多个部分组成的索引，该索引包含作为外键的相关性维度表的主键，而维度表包含事实记录的特性。事实数据表不应该包含描述性的信息，也不应该包含除数字度量字段及使事实与维度表中对应项的相关索引字段之外的任何数据。包含在事实数据表中的“度量值”有两中：一种是可以累计，另一种是非累计。一般来说，一个事实数据表都要和一个或多个维度表相关联，用户在利用事实数据表创建多维数据集时，可以使用一个或多个维度表。维度表在数据仓库中，维度表中的键属性必须为维度的每个成员包含一个对应的唯一值(其实就是键属性为主键，作为事实表的外键 )。操作简单, 执行快速, 屏蔽错误, 统一口径数据分析常用宽表，宽表的设计需要做一个折中. 一方面设计完备的数据仓库是不现实的,另一方面宽表的前提是足够常用。对于不常用的数据我们的数据平台是支持直接操作的。  近源数据层作为数据源, 主要是不常用的, 简单的数据.  数据中间层, 使用频率很高的基于主题的数据.  基础指标中间层, 基于数据中间层的基础聚合, 使用频率更高. 简化复杂BI过程.OLTPOnline Transaction Processing执行基本日常的事务处理，比如数据库记录的增删查改。比如在银行的一笔交易记录，就是一个典型的事务。  专门用来做日常的，基本的操作  实时性要求高  处理的数据量在G级别  高并发，并且要求满足ACID原则  事务的吞吐量是关键性能指标OLAPOnLine Analytical Processing  列存储模式或者说nosql模式数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。典型的应用就是复杂的动态的报表系统  实时性要求不是很高  复杂查询  数据量大TB-PB级别  对数据加工分析，读多写少星型/雪花模式星型模型和雪花型模型比较        星形维度(左)只有当维度表极大，存储空间是个问题时，才考虑雪花型维度多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度,数据有一定的冗余e.g 地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高雪花型维度(右)雪花型维度描述了更清晰的层次概念通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。  多数情况下，选择星型，但是不排除使用雪花型的情况数仓工具  数据地图 查看所有报表的路径和执行过程. 这样我们可以追查特定字段的数据来源, 广泛用于对账和对数.提供数据任务间的依赖关系, 从而进行快速的全局数据的修补  元数据管理系统 数据字典的文档]]></content>
      <categories>
        
          <category> BI </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[sql中行列互換]]></title>
      <url>/blog/2019/03/28/pivot</url>
      <content type="text"><![CDATA[准备CREATE TABLE tb (	name VARCHAR ( 10 ),	course VARCHAR ( 10 ),	score INT);insert into tb values('张三' , '语文' , 74);insert into tb values('张三' , '数学' , 83); insert into tb values('张三' , '物理' , 93);insert into tb values('李四' , '语文' , 74);insert into tb values('李四' , '数学' , 84);insert into tb values('李四' , '物理' , 94); 行-&gt;列simple        SELECT	name AS name,	max( CASE course WHEN '语文' THEN score ELSE 0 END ) 语文,	max( CASE course WHEN '数学' THEN score ELSE 0 END ) 数学,	max( CASE course WHEN '物理' THEN score ELSE 0 END ) 物理 FROM	tbGROUP BY	namemax( CASE course WHEN '语文' THEN score ELSE 0 END )SUM(IF(course="语文",score,0))CASE  WHEN signal_type='macd_15_dea_up_0' and op_type='buy' THEN 1 WHEN  signal_type='macd_15_dea_up_0' and op_type='sell' THEN -1  ELSE 0 END as macd_15_dea_up_0IF(expr1,expr2,expr3)if expr1 then expr2 else expr3IFNULL(expr1,expr2)if expr1 not NULL then expr1 else expr2CASE value WHEN [compare_value] THEN result [WHEN [compare_value] THEN result ...] [ELSE result] END]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[EXPLAIN]]></title>
      <url>/blog/2019/03/28/explain</url>
      <content type="text"><![CDATA[Mysql执行计划explain select ...id子查询的执行顺序  id相同执行顺序由上至下  id不同，id值越大优先级越高，越先被执行。如果是子查询，id的序号会递增  id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中select_type子查询的查询类型            select_type      description                  SIMPLE      简单的select查询 不包含任何子查询或union等查询              PRIMARY      查询中包含任何复杂的子部分，最外层查询则被标记为primary              SUBQUERY      在 select 或 where 中包含了子查询              DERIVED      在from中包含的子查询被标记为derived（衍生）结果放在临时表****里              UNION      出现在union后的select语句中,若union包含在from子句的子查询中，外层select将被标记为derived              UNION RESULT      从UNION中获取结果集 id 为NULL &lt;union1,2&gt;      table查询的数据表 table_name  &lt;derivedid&gt;  &lt;unionid,id&gt;type访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是  system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL一般来说，好的sql查询至少达到range级别，最好能达到ref  system 表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，可以忽略不计  const PRIMARY KEY或 UNIQUE索引的所有部分与常量值进行比较时使用SELECT * FROM tbl_name WHERE primary_key=1;SELECT * FROM tbl_name  WHERE primary_key_part1=1 AND primary_key_part2=2;  eq_ref 在join查询中使用PRIMARY KEY or UNIQUE NOT NULL索引关联 比较值可以是常量/表达式SELECT * FROM ref_table,other_table  WHERE ref_table.key_column=other_table.column;SELECT * FROM ref_table,other_table  WHERE ref_table.key_column_part1=other_table.column  AND ref_table.key_column_part2=1;      ref 非唯一性索引扫描，返回匹配某个单独值的所有行。    fulltext 使用全文索引  ref_or_null 对Null进行索引的优化的 ref  index_merge 索引合并优化 单个表的索引扫描多行 range扫描和他们的结果合并到一个 and or SELECT * FROM tbl_name WHERE key1 = 10 OR key2 = 20;SELECT * FROM tbl_name  WHERE (key1 = 10 OR key2 = 20) AND non_key = 30;SELECT * FROM t1, t2  WHERE (t1.key1 IN (1,2) OR t1.key2 LIKE 'value%')  AND t2.key1 = t1.some_col;SELECT * FROM t1, t2  WHERE t1.key1 = 1  AND (t2.key1 = t1.some_col OR t2.key2 = t1.some_col2);  unique_subquery 在子查询中使用 eq_ref  index_subquery 在子查询中使用 ref  range 索引范围查找 where语句中出现了bettween、&lt;、&gt;、in等的查询  index 遍历索引  ALL 扫描全表数据possible_keys可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。key显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。key_length索引长度ref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值rows返回估算的结果集数目，并不是一个准确的值。extra不适合在其他字段中显示，但是十分重要的额外信息  Using filesortmysql对数据使用一个外部的索引排序，而不是按照表内的索引进行排序读取。也就是说mysql无法利用索引完成的排序操作成为文件排序  Using temporary使用临时表保存中间结果，也就是说mysql在对查询结果排序时使用了临时表，常见于order by 和 group by]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[主键，外键，索引，唯一索引]]></title>
      <url>/blog/2019/03/27/key</url>
      <content type="text"><![CDATA[主键能够唯一标识表中某一行的属性或属性组。一个表只能有一个主键作为外键维护两个表之间数据的一致性保证记录的唯一和非空外键用于建立和加强两个表数据之间的链接的一列或多列。外键约束主要用来维护两个表之间数据的一致性表的外键（必须是索引）就是另一表的主键（数据类型相似 int和tinyint可以，而int和char则不可以），外键将两表联系起来。一般情况下，要删除一张表中的主键必须首先要确保其它表中的没有相同外键（即该表中的主键没有一个外键和它相关联）M 主表 F 外键表（mid）constraint `xxx` foreign key (`mid`) references `M`(`id`) on delete cascade on update cascade;  有关联动作 ON DELETE/ON UPDATE F跟随M  F.mid插入的值必须在M.id存在  无有关联动作M.id记录不能随便删除，先删除F中F.mid=M.id的记录关联[ON DELETE {RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT}][ON UPDATE {RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT}]ON DELETE、ON UPDATE表示事件触发限制，可设参数：  RESTRICT（限制外表中的外键改动)  CASCADE（跟随外键改动）  SET NULL（设空值）  SET DEFAULT（设默认值）  NO ACTION（无动作，默认的）索引快速地寻找那些具有特定值的记录。主要是为了检索的方便，是为了加快访问速度， 按一定的规则创建的，一般起到排序作用。所谓唯一性索引，这种索引和前面基本相同，但有一个区别：索引列的所有值都只能出现一次，即必须唯一，可以为空。]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[DDL, DML, DCL，TCL]]></title>
      <url>/blog/2019/03/27/DDL_DML_DCL</url>
      <content type="text"><![CDATA[DDLData Definition Language 数据定义语言这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义会自动提交 implict commit 隐式提交  如果事务编写DML DDL夹杂   DDL执行不管报错与否，都会执行一次commitSET AUTOCOMMIT = 1;BEGIN;INSERT INTO t1 VALUES (1);CREATE TABLE t2 (pk int primary key);INSERT INTO t2 VALUES (2);ROLLBACK;t1 插入 t2创建 t2插入SET AUTOCOMMIT = 0;BEGIN;INSERT INTO t1 VALUES (1);CREATE TABLE t2 (pk int primary key);INSERT INTO t2 VALUES (2);ROLLBACK;t1 插入 t2创建   自动开启一个新事务 t2插入被回滚DMLData Manipulation Language 数据操纵语句用于添加、删除、更新和查询数据库记录，并检查数据完整性delete 语句是数据库操作语言(dml)，这个操作会放到 rollback segement 中，事务提交之后才生效DCLData Control Language 数据控制语句用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。TCLTransaction Control Language  事务控制]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Markov Model 马尔科夫模型]]></title>
      <url>/blog/2019/03/16/markov_model</url>
      <content type="text"><![CDATA[  状态转移概率  状态数量  初始位置概率马尔科夫链Markov Chain描述离散状态之间在不同时刻的转移关系假设  t+m 时刻系统状态的概率分布只与t时刻的状态有关，与t时刻以前的状态无关 （m阶Markov Chain 简化m=1 ）  状态转移概率 与 t 无关一个马尔可夫链模型可表示为=(S，P，Q)  S是系统所有可能的状态所组成的非空的状态集  P是状态转移矩阵 $P_{kl\;}=\;P(X_t\;=\;S_l\vert X_{t-m}\;=\;S_k)$ t 时刻从k状态转移到l状态的概率  Q初始概率分布隐马尔科夫 Hide Markov Model HMM在状态 State的基础上加入Token概念，每个状态以不同的概率产生一组可观察的Token =&gt; 生成概率（Emission Probability）HMM里面State是不可见的，只能观察Token同一个Token序列有不同State路径 argmaxP(S|T)  token 反推state 选概率大]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> Markov </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[高并发I/O]]></title>
      <url>/blog/2019/03/05/IO</url>
      <content type="text"><![CDATA[web服务的高并发性能指标可以通过QPS（Query per second）来衡量，QPS指每秒处理请求数，可用（并发数/一般响应时间）来计算网络I/O过程涉及到应用进程以及linux内核两个对象，分为两个阶段  数据准备：通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区  数据拷贝：把数据从内核缓冲区复制到应用进程缓冲区五种IO模型阻塞式I/O进程调用recvfrom，直到数据包到达且被复制到应用程序的缓冲区或发生错误才返回。最常见的错误是系统调用信号被中断。我们称进程在从调用recvfrom开始到返回的整段时间内是阻塞的。BIO，通过多线程提高并发能力,解决串行问题。非阻塞式I/O进程轮询数据准备的状态，如果没准备好，则立即返回错误，如果准备好并且拷贝完成，则返回成功NIO，轮询占用CPU效率,一般很少直接使用这种模型，而是在其他I/O模型中使用非阻塞I/O这一特性多路复用I/Oselect/poll/epoll模型，进程调用select/poll，阻塞等待套接字变为可读。当select返回套接字可读这一条件时，进程调用recvfrom把所读数据复制到应用进程缓冲区。IO多路复用，使用select函数进行I/O请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。但是使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的I/O请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个I/O请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。调用select/poll该方法由一个用户态线程负责轮询多个socket,直到某个阶段1的数据就绪,再通知实际的用户线程执行阶段2的拷贝。通过一个专职的用户态线程执行非阻塞I/O轮询,模拟实现了阶段一的异步化。信号驱动式I/O进程向系统注册感兴趣的信号，并立即返回，当数据准备完成的信号触发时，系统通知进程，进程调用recvfrom把所读数据复制到应用程序缓冲区。信号驱动，在一个tcp连接的生命周期内，有大量的信号需要监听，并且信号目的难以区分，使得对于socket几乎无用，在udp服务程序中可用。异步I/O进程调用系统函数，并告诉内核当整个操作完成时如何通知进程。该系统调用立即返回，而且在等待I/O完成期间，进程不被阻塞。当数据准备好，并且复制到应用进程缓冲区后，内核才会产生这个通知。AIO，需要底层操作系统支持，而linux系统并不完全支持异步，windows提供了IOCP的接口支持异步。可以看到，尽管阻塞式I/O、非阻塞式I/O、多路复用I/O、信号驱动式I/O在第一阶段的处理不同，但在数据拷贝阶段都处于同步阻塞等待状态，因此可以看作是同步I/O的一种，这里的同步指的是recvfrom这个系统调用。]]></content>
      <categories>
        
          <category> IO </category>
        
      </categories>
      <tags>
        
          <tag> IO </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Hive]]></title>
      <url>/blog/2019/03/03/hive</url>
      <content type="text"><![CDATA[HiveHive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上。  Hive Home  Hive Tutorial通过HSQL访问在Hadoop上的文件或者HBase上的数据，实现extract/transform/load(ETL)和数据分析，etc。Tez，Spark或 MapReduce执行引擎,支持UDFmetastore 保存了于Hive到HDFS映射。Hive使用HQL操作HDFS的数据表外部表分区桶建表CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][ROW FORMAT row_format][STORED AS file_format][ROW FORMAT DELIMITED]关键字，是用来设置创建的表在加载数据的时候，支持的列分隔符。列终止符，行终止符，并保存的文件类型ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'LINES TERMINATED BY '\n'STORED IN TEXTFILEhive&gt; create table wyp(id int,    &gt; name string,    &gt; age int,    &gt; tele string)    &gt; ROW FORMAT DELIMITED    &gt; FIELDS TERMINATED BY '\t'    &gt; STORED AS TEXTFILE;OKTime taken: 0.759 secondsLOAD DATA语句LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]hive-site.xmlhive.metastore.warehouse.dirHive表数据存放的路径,每创建一个表都会在hive.metastore.warehouse.dir指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面。load data local inpath '/home/wyp/data/wyp.txt' into table wyp;drophive&gt; drop table wyp;Moved: 'hdfs://mycluster/user/hive/warehouse/wyp' to         trash at: hdfs://mycluster/user/hdfs/.Trash/CurrentOKTime taken: 2.503 seconds如果你的Hadoop没有取用垃圾箱机制，那么drop table wyp命令将会把属于wyp表的所有数据全部删除！外部表创建表的时候加上external关键字，同时指定外部表存放数据的路径。(不指定外部表的存放路径，这样Hive将在HDFS上的/user/hive/warehouse/文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里)load data 数据是被移动到创建表时指定的目录hive&gt; create external table exter_table(    &gt; id int,    &gt; name string,    &gt; age int,    &gt; tel string)    &gt; location '/home/wyp/external';OKTime taken: 0.098 seconds在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！catdesc bigdata_user; //查看表的简单结构show create table bigdata_user; //查看bigdata_user表的各种属性；SqoopSqoop MySQL导入数据到Hive和HBase]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spark]]></title>
      <url>/blog/2019/03/01/spark</url>
      <content type="text"><![CDATA[Spark集群计算框架,相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，内存内运算，比Hadoop快100倍。  spark  pyspark优势Hadoop比架构其基础的程序抽象则称为弹性分布式数据集（RDDs），是一个可以并行操作、有容错机制的数据集合。Spark应用程序作为集群上的独立进程集运行，由SparkContext 主程序中的对象driver program主导  应用提交driver创建SparkContext，SparkContext向Cluster Manager注册，申请运行Executor资源  申请资源后，Executor启动，将运行状况发送到Cluster Manager  SparkContext根据RDD依赖关系构建DAG，DAGScheduler解析DAG分成多个Stage(阶段 任务集)，计算Stage之间依赖关系,将Stages交给TaskScheduler  Executor向SparkContext申请任务，TaskScheduler分发将任务到Executor，SparkContext分发应用代码到Executor  任务在Executor上执行，反馈到TaskScheduler，DAGScheduler。完成后写入数据，释放资源。RDD好处结构只读分区记录集合，一个RDD多个分区（数据集片段），不同分区可存储到集群上不同节点进行并行计算。RDD不能直接修改  只能基于物理存储的数据转换创建RDD  基于其他RDD转换创建新的RDD。transformations and actions  transformat操作(map,filter,groupBy,join)接受RDD返回RDD  action操作(collect,collect)接受RDD返回一个值或者结果。transformat不会马上计算，Spark会记住对于base dataset的所有转换（RDD依赖关系）。当有action时才会计算出结果（Spark根据RDD依赖关系生成DAG）返回到the driver program。避免多次转换操作之间数据同步等待，不担心有过多的中间数据。DAG拓扑排序连接一系列RDD操作实现管道化。默认情况下，每次action都会计算，即使是相同的transform。 可以通过持久化（缓存）机制避免这种重复计算的开销。可以使用persist()方法对一个RDD标记为持久化，之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算生成RDD并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化，持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用。&gt;&gt;&gt; list = ["Hadoop","Spark","Hive"]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; rdd.cache()  //会调用persist(MEMORY_ONLY)，但是，语句执行到这里，并不会缓存rdd，这是rdd还没有被计算生成&gt;&gt;&gt; print(rdd.count()) //第一次行动操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd放到缓存中3&gt;&gt;&gt; print(','.join(rdd.collect())) //第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rddHadoop,Spark,Hive  RDD APIdef t_map():    data = [1, 2, 3, 4, 5]    with SparkContext() as sc:        rdd = sc.parallelize(data)        gl = rdd.map(lambda x: x * 2)        col = gl.collect()        print(col)Why 高效  高容错          根据DAG重新计算，获得丢失数据，避免数据复制高开销，重算过程多节点并行        中间结果持久化到内存  存放数据都是Java对象，减少序列化和反序列化RDD依赖看RDD分区设置分区分区原则是使得分区的个数尽量等于集群中的CPU核心（core）数目&gt;&gt;&gt; array = [1,2,3,4,5]&gt;&gt;&gt; rdd = sc.parallelize(array,2) #设置两个分区划分DAG在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖把当前RDD加入到当前stage，尽量窄依赖划分到同一个阶段。运行过程RUNInteractivemaster 参数  localRun Spark with one worker thread.  local[n]Run Spark with n worker threads.  spark://HOST:PORTConnect to a Spark standalone cluster.  mesos://HOST:PORTConnect to a Mesos cluster.pyspark --master local[4]runrun spark scriptspark-submit --master local word_count.pysc = SparkContext(master='local[4]')py local  The RDD.glom() method returns a list of all of the elements within each partition, and the RDD.collect() method brings all the elements to the driver node.get RDD from collectsc.parallelizeget filesc.textFilediff flatMap and mapdef t_map():    data = [[1, 2], [], [3, 4, 5]]    with SparkContext() as sc:        rdd = sc.parallelize(data)        gl = rdd.map(lambda x: x)        col = gl.collect()        print(col)"""[[1, 2], [], [3, 4, 5]]"""def t_flatmap():    data = [[1, 2], [], [3, 4, 5]]    with SparkContext() as sc:        rdd = sc.parallelize(data)        gl = rdd.flatMap(lambda x: x)        print(gl.collect())"""[1, 2, 3, 4, 5]""" def t_flatmap1():    # 先map 再flat    data = [[1, 2], [], [3, 4, 5]]    with SparkContext() as sc:        rdd = sc.parallelize(data)        gl = rdd.flatMap(lambda x: x * 2)        print(gl.collect())"""[1, 2, 1, 2, 3, 4, 5, 3, 4, 5]"""if __name__ == '__main__':    t_map()    t_flatmap()    t_flatmap1()printrdd.take(100).foreach(print)rdd.foreach(print)但是，由于collect()方法会把各个worker节点上的所有RDD元素都抓取到Driver Program中，因此，这可能会导致内存溢出rdd.collect().foreach(print)共享变量 broadcast/accumulators  广播变量用来把变量在所有节点的内存之间进行共享  累加器则支持在所有不同节点之间进行累加计算（比如计数或者求和）广播变量允许程序开发人员在每个机器上缓存一个只读的变量，而不是为机器上的每个任务都生成一个副本。  When use Broadcast variable          只有在跨多个阶段的任务需要相同数据,数据重要，不更改的值。仅在需要时加载，数据仅发送到包含需要它的执行程序的节点。      &gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&gt;&gt; broadcastVar.value[1,2,3]              In spark, how does broadcast work?累加器运行在集群中的任务，就可以使用add方法来把数值累加到累加器上，但是，这些任务只能做累加操作，不能读取累加器的值，只有任务控制节点（Driver Program）可以使用value方法来读取累加器的值。&gt;&gt;&gt; accum = sc.accumulator(0)&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x : accum.add(x))&gt;&gt;&gt; accum.value10文件读写因为Spark采用了惰性机制，在执行转换操作的时候，即使我们输入了错误的语句，pyspark也不会马上报错，而是等到执行“行动”类型的语句时启动真正的计算，那个时候“转换”操作语句中的错误就会显示出来      本地file:///usr/local/spark/mycode/wordcount/writeback.txt        hdfshdfs://localhost:9000/user/hadoop/word.txt  &gt;&gt;&gt; val textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")&gt;&gt;&gt; val textFile = sc.textFile("/user/hadoop/word.txt")&gt;&gt;&gt; val textFile = sc.textFile("word.txt")from pyspark import SparkContextPATH = 'hdfs://localhost:9000'def main():    with SparkContext(appName='sparkwordcount') as sc:        # 采用了惰性机制 即使路径错误 也不会马上报错        input_file = sc.textFile(PATH + '/user/wyx/input/input.txt')        counts = input_file.flatMap(lambda line: line.split()) \            .map(lambda word: (word, 1)) \            .reduceByKey(lambda a, b: a + b)        # 是一个目录路径        counts.saveAsTextFile(PATH + '/user/wyx/output')if __name__ == '__main__':    main()&gt;&gt;&gt; peopleDF = spark.read.format("json").load("file:///usr/local/spark/examples/src/main/resources/people.json")&gt;&gt;&gt; peopleDF.select("name", "age").write.format("csv").save("file:///usr/local/spark/mycode/newpeople.csv")&gt;&gt;&gt; peopleDF = spark.read.format("json").load("file:///usr/local/spark/examples/src/main/resources/people.json"&gt;&gt;&gt; peopleDF.rdd.saveAsTextFile("file:///usr/local/spark/mycode/newpeople.txt")spark = SparkSession.builder.getOrCreate()df = spark.read.json('file:///Users/wyx/project/py3.7aio/spark/a.json')df.select(df.name.alias("username"), df.age).write.csv('file:///Users/wyx/project/py3.7aio/spark/a.csv',header=True,mode='overwrite')df = spark.read.csv('file:///Users/wyx/project/py3.7aio/spark/a.csv',header=True)df.show()dataframe  pyspark.sql.functionsfrom pyspark.sql.functions import *spark = SparkSession.builder.getOrCreate()df = spark.read.json('file:///Users/wyx/project/py3.7aio/spark/a.json')df.show()df.printSchema()df.select(df.name, (df.age + 1)).show()df.sort(df.age.desc(), df.name.asc()).show()df.groupBy("age").count().show()df.agg(avg("age").alias("avg_age")).show()df.filter(df.age &gt; 20).show()df.select(df.name.alias("username"), df.age).show()df = df.filter(df.price != '面议').withColumn("price", df["price"].cast(IntegerType())).\    filter(df.price &gt;= 50).filter(df.price &lt;= 40000)mean_list[0] = df.filter(df.area == "海沧").agg({"price": "mean"}).first()['avg(price)']max_list[5] = df.filter(df.area == "同安").agg({"price": "max"}).first()['max(price)']mid_list[5] = df.filter(df.area == "同安").approxQuantile("price", [0.5], 0.01)[0]from spark.ml import sparkfrom pyspark.sql.functions import *cols = [('A', 'xx', 'D', 'vv', 4), ('C', 'xxx', 'D', 'vv', 10), ('A', 'x', 'A', 'xx', 3),            ('E', 'xxx', 'B', 'vv', 3), ('E', 'xxx', 'F', 'vvv', 6), ('F', 'xxxx', 'F', 'vvv', 4),            ('G', 'xxx', 'G', 'xxx', 4), ('G', 'xxx', 'G', 'xx', 4), ('G', 'xxx', 'G', 'xxx', 12),            ('B', 'xxxx', 'B', 'xx', 13)]df = spark.createDataFrame(cols, ['col1', 'col2', 'col3', 'col4', 'd'])df.show()# indf.filter(col('col4').isin(['vv','xx'])).show()# not indf.filter(~col('col4').isin(['vv','xx'])).show()df.filter((df.d &lt; 5) &amp; (        (col('col1') != col('col3')) |        ((col('col1') == col('col3')) &amp; (col('col2') != col('col4'))))).show()rdd to df  反射机制推断RDD&gt;&gt;&gt; from pyspark.sql.types import Row&gt;&gt;&gt; def f(x):...     rel = {}...     rel['name'] = x[0]...     rel['age'] = x[1]...     return rel... &gt;&gt;&gt; peopleDF = sc.textFile("file:///usr/local/spark/examples/src/main/resources/people.txt").map(lambda line : line.split(',')).map(lambda x: Row(**f(x))).toDF()&gt;&gt;&gt; peopleDF.createOrReplaceTempView("people")  //必须注册为临时表才能供下面的查询使用 &gt;&gt;&gt; personsDF = spark.sql("select * from people")&gt;&gt;&gt; personsDF.rdd.map(lambda t : "Name:"+t[0]+","+"Age:"+t[1]).foreach(print) Name: 19,Age:JustinName: 29,Age:MichaelName: 30,Age:Andy  编程方式定义RDD&gt;&gt;&gt;  from pyspark.sql.types import Row&gt;&gt;&gt;  from pyspark.sql.types import StructType&gt;&gt;&gt; from pyspark.sql.types import StructField&gt;&gt;&gt; from pyspark.sql.types import StringType //生成 RDD&gt;&gt;&gt; peopleRDD = sc.textFile("file:///usr/local/spark/examples/src/main/resources/people.txt") //定义一个模式字符串&gt;&gt;&gt; schemaString = "name age" //根据模式字符串生成模式&gt;&gt;&gt; fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True), schemaString.split(" ")))&gt;&gt;&gt; schema = StructType(fields)//从上面信息可以看出，schema描述了模式信息，模式中包含name和age两个字段  &gt;&gt;&gt; rowRDD = peopleRDD.map(lambda line : line.split(',')).map(lambda attributes : Row(attributes[0], attributes[1])) &gt;&gt;&gt; peopleDF = spark.createDataFrame(rowRDD, schema) //必须注册为临时表才能供下面查询使用scala&gt; peopleDF.createOrReplaceTempView("people") &gt;&gt;&gt; results = spark.sql("SELECT * FROM people")&gt;&gt;&gt; results.rdd.map( lambda attributes : "name: " + attributes[0]+","+"age:"+attributes[1]).foreach(print) name: Michael,age: 29name: Andy,age: 30name: Justin,age: 19stream创建 StreamingContextfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContext# 1表示每隔1秒钟就自动执行一次流计算，这个秒数可以自由设定。ssc = StreamingContext(sc, 1)setAppName(“TestDStream”)是用来设置应用程序名称。setMaster(“local[2]”)括号里的参数local[2]字符串表示运行在本地模式下，并且启动2个工作线程。from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextconf = SparkConf()conf.setAppName('TestDStream')conf.setMaster('local[2]')sc = SparkContext(conf = conf)ssc = StreamingContext(sc, 1)File流输入ssc.start()以后，程序就开始自动进入循环监听状态，监听程序只监听目录下在程序启动后新增的文件，不会去处理历史上已经存在的文件。from operator import addfrom pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextconf = SparkConf()conf.setAppName('TestDStream')conf.setMaster('local[2]')sc = SparkContext(conf = conf)ssc = StreamingContext(sc, 20)lines = ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')words = lines.flatMap(lambda line: line.split(' '))wordCounts = words.map(lambda x : (x,1)).reduceByKey(add)wordCounts.pprint()ssc.start()ssc.awaitTermination()socketsc = SparkContext(appName="PythonStreamingNetworkWordCount")ssc = StreamingContext(sc, 1)lines = ssc.socketTextStream(host, port)counts = lines.flatMap(lambda line: line.split(" "))\              .map(lambda word: (word, 1))\              .reduceByKey(lambda a, b: a+b)counts.pprint()ssc.start()ssc.awaitTermination()RDD队列流(DStream)sc = SparkContext(appName="PythonStreamingQueueStream")ssc = StreamingContext(sc, 1)# Create the queue through which RDDs can be pushed to# a QueueInputDStreamrddQueue = []for i in range(5):    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]# Create the QueueInputDStream and use it do some processinginputStream = ssc.queueStream(rddQueue)mappedStream = inputStream.map(lambda x: (x % 10, 1))reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)reducedStream.pprint()ssc.start()time.sleep(6)ssc.stop(stopSparkContext=True, stopGraceFully=True)高级流Kafka和Flume等高级输入源，需要依赖独立的库（jar文件）。按照我们前面安装好的Spark版本，这些jar包都不在里面。需要自己下载对应jar(spark-streaming-kafka spark-streaming-flume),复制到Spark目录的jars目录  Kafkafrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtils     sc = SparkContext(appName="PythonStreamingKafkaWordCount")ssc = StreamingContext(sc, 1)kvs = KafkaUtils.createStream(ssc, host:port, "spark-streaming-consumer", {topic: 1})lines = kvs.map(lambda x: x[1])counts = lines.flatMap(lambda line: line.split(" ")) \    .map(lambda word: (word, 1)) \    .reduceByKey(lambda a, b: a+b)counts.pprint()ssc.start()ssc.awaitTermination()  Flumefrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.streaming.flume import FlumeUtilsimport pysparksc = SparkContext(appName="FlumeEventCount")ssc = StreamingContext(sc, 2)stream = FlumeUtils.createStream(ssc, hostname, port,pyspark.StorageLevel.MEMORY_AND_DISK_SER_2)stream.count().map(lambda cnt : "Recieve " + str(cnt) +" Flume events!!!!").pprint()ssc.start()ssc.awaitTermination()Dstearm transfer  无状态转换：每个批次的处理不依赖于之前批次的数据。  有状态转换：当前批次的处理需要使用之前批次的数据或者中间结果。有状态转换包括基于滑动窗口的转换和追踪状态变化的转换(updateStateByKey)。updateStateByKey 跨批次之间维护状态# RDD with initial state (key, value) pairsinitialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])def updateFunc(new_values, last_sum):    return sum(new_values) + (last_sum or 0)lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))running_counts = lines.flatMap(lambda line: line.split(" "))\                      .map(lambda word: (word, 1))\                      .updateStateByKey(updateFunc, initialRDD=initialStateRDD)window设定一个滑动窗口的长度,并且设定滑动窗口的时间间隔（每隔多长时间执行一次计算)下面给给出一些窗口转换操作的含义：  window(windowLength, slideInterval) 基于源DStream产生的窗口化的批数据，计算得到一个新的DStream；  countByWindow(windowLength, slideInterval) 返回流中元素的一个滑动窗口数；  reduceByWindow(func, windowLength, slideInterval) 返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数func必须满足结合律，从而可以支持并行计算；  reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) 应用到一个(K,V)键值对组成的DStream上时，会返回一个由(K,V)键值对组成的新的DStream。每一个key的值均由给定的reduce函数(func函数)进行聚合计算。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。可以通过numTasks参数的设置来指定不同的任务数；  reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 更加高效的reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）；  countByValueAndWindow(windowLength, slideInterval, [numTasks]) 当应用到一个(K,V)键值对组成的DStream上，返回一个由(K,V)键值对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。Dstearm outputsaveAsTextFiles存储到mysqldef func(rdd):    """    每次保存RDD到MySQL中，都需要启动数据库连接，如果RDD分区数量太大，那么就会带来多次数据库连接开销，    为了减少开销，就有必要把RDD的分区数量控制在较小的范围内，所以，这里就把RDD的分区数量重新设置为3    """    repartitionedRDD = rdd.repartition(3)    repartitionedRDD.foreachPartition(dbfunc) running_counts.foreachRDD(func)MLLibSpark官方推荐使用spark.mlpiplinefrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegression, SparkSessionfrom pyspark.ml.feature import HashingTF, Tokenizerspark = SparkSession.builder.master("local").appName("Word Count").getOrCreate()training = spark.createDataFrame([    (0, "a b c d e spark", 1.0),    (1, "b d", 0.0),    (2, "spark f g h", 1.0),    (3, "hadoop mapreduce", 0.0)], ["id", "text", "label"])test = spark.createDataFrame([    (4, "spark i j k"),    (5, "l m n"),    (6, "spark hadoop spark"),    (7, "apache hadoop")], ["id", "text"])tokenizer = Tokenizer(inputCol="text", outputCol="words")hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")lr = LogisticRegression(maxIter=10, regParam=0.001)pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])model = pipeline.fit(training)prediction = model.transform(test)prediction.show()selected = prediction.select("id", "text", "probability", "prediction")for row in selected.collect():    rid, text, prob, prediction = row    print("(%d, %s) --&gt; prob=%s, prediction=%f" % (rid, text, str(prob), prediction))    &gt;&gt;&gt;+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+| id|              text|               words|            features|       rawPrediction|         probability|prediction|+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+|  4|       spark i j k|    [spark, i, j, k]|(262144,[20197,24...|[-1.6609033227472...|[0.15964077387874...|       1.0||  5|             l m n|           [l, m, n]|(262144,[18910,10...|[1.64218895265644...|[0.83783256854767...|       0.0||  6|spark hadoop spark|[spark, hadoop, s...|(262144,[155117,2...|[-2.5980142174393...|[0.06926633132976...|       1.0||  7|     apache hadoop|    [apache, hadoop]|(262144,[66695,15...|[4.00817033336812...|[0.98215753334442...|       0.0|+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+(4, spark i j k) --&gt; prob=[0.1596407738787475,0.8403592261212525], prediction=1.000000(5, l m n) --&gt; prob=[0.8378325685476744,0.16216743145232562], prediction=0.000000(6, spark hadoop spark) --&gt; prob=[0.06926633132976037,0.9307336686702395], prediction=1.000000(7, apache hadoop) --&gt; prob=[0.9821575333444218,0.01784246665557808], prediction=0.000000TF-IDF词语由t表示，文档由d表示，语料库由D表示。  词频TF(t,d)是词语t在文档d中出现的次数  文件频率DF(t,D)是包含词语的文档的个数首先使用分解器Tokenizer把句子划分为单个词语。对每一个句子（词袋），我们使用HashingTF将句子转换为特征向量，最后使用IDF重新调整特征向量。这种转换通常可以提高使用文本特征的性能。sentenceData = spark.createDataFrame(    [(0, "I heard about Spark and I love Spark"), (0, "I wish Java could use case classes"),     (1, "Logistic regression models are neat")]).toDF("label", "sentence")tokenizer = Tokenizer(inputCol="sentence", outputCol="words")wordsData = tokenizer.transform(sentenceData)hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=20)featurizedData = hashingTF.transform(wordsData)idf = IDF(inputCol="rawFeatures", outputCol="features")idfModel = idf.fit(featurizedData)rescaledData = idfModel.transform(featurizedData)rescaledData.select("label", "features").show()rescaledData.show()&gt;&gt;&gt;+-----+--------------------+|label|            features|+-----+--------------------+|    0|(20,[0,5,9,13,17]...||    0|(20,[2,7,9,13,15]...||    1|(20,[4,6,13,15,18...|+-----+--------------------++-----+--------------------+--------------------+--------------------+--------------------+|label|            sentence|               words|         rawFeatures|            features|+-----+--------------------+--------------------+--------------------+--------------------+|    0|I heard about Spa...|[i, heard, about,...|(20,[0,5,9,13,17]...|(20,[0,5,9,13,17]...||    0|I wish Java could...|[i, wish, java, c...|(20,[2,7,9,13,15]...|(20,[2,7,9,13,15]...||    1|Logistic regressi...|[logistic, regres...|(20,[4,6,13,15,18...|(20,[4,6,13,15,18...|+-----+--------------------+--------------------+--------------------+--------------------+word2vec  CBOW ，其思想是通过每个词的上下文窗口词词向量来预测中心词的词向量。  Skip-gram，其思想是通过每个中心词来预测其上下文窗口词，并根据预测结果来修正中心词的词向量documentDF = spark.createDataFrame([    ("Hi I heard about Spark".split(" "),),    ("I wish Java could use case classes".split(" "),),    ("Logistic regression models are neat".split(" "),)], ["text"])word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text", outputCol="result")model = word2Vec.fit(documentDF)result = model.transform(documentDF)result.show()for row in result.collect():    text, vector = row    print("Text: [%s] =&gt; \nVector: %s\n" % (", ".join(text), str(vector)))&gt;&gt;&gt;&gt;Text: [Hi, I, heard, about, Spark] =&gt; Vector: [0.02057519257068634,0.027896256744861604,0.0453112430870533]Text: [I, wish, Java, could, use, case, classes] =&gt; Vector: [0.014936649905783788,-0.0039047444505350927,-0.009782610195023673]Text: [Logistic, regression, models, are, neat] =&gt; Vector: [-0.08842044896446169,0.010355661995708943,-0.04723416268825531]CountVectorizer 词频统计df = spark.createDataFrame([    (0, "a b c".split(" ")),    (1, "a b c a a".split(" "))], ["id", "words"])# fit a CountVectorizerModel from the corpus.cv = CountVectorizer(inputCol="words", outputCol="features", vocabSize=3, minDF=2.0)model = cv.fit(df)result = model.transform(df)result.show(truncate=False)&gt;&gt;&gt;+---+---------------+-------------------------+|id |words          |features                 |+---+---------------+-------------------------+|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])||1  |[a, b, c, a, a]|(3,[0,1,2],[3.0,1.0,1.0])|+---+---------------+-------------------------+trick# 数据集随机分成训练集和测试集，其中训练集占70%trainingData, testData = df.randomSplit([0.7,0.3])# 还能看到我们已经设置的参数的结果print("LogisticRegression parameters:\n" + LogisticRegression().explainParams())# 模型评估evaluator = MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction")lrAccuracy = evaluator.evaluate(lrPredictions)print("Test Error = " + str(1.0 - lrAccuracy))evaluatorClassifier = MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")accuracy = evaluatorClassifier.evaluate(predictionsClassifier)print("Test Error = " + str(1.0 - accuracy))evaluatorRegressor = RegressionEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("rmse")rmse = evaluatorRegressor.evaluate(predictionsRegressor)print("Root Mean Squared Error (RMSE) on test data = " +str(rmse))# pipeline get stagelrModel = lrPipelineModel.stages[2]print("Coefficients: " + str(lrModel.coefficients)+"Intercept: "+str(lrModel.intercept)+"numClasses: "+str(lrModel.numClasses)+"numFeatures: "+str(lrModel.numFeatures))Coefficients: [-0.0396171957643483,0.0,0.0,0.07240315639651046]Intercept: -0.23127346342015379numClasses: 2numFeatures: 4]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[brew install 指定 version]]></title>
      <url>/blog/2019/02/24/homebrew-install-specific-version</url>
      <content type="text"><![CDATA[brew search package@versionsearchbrew search package@url  homebrew-core Formulago tohttps://github.com/Homebrew/homebrew-core/blob/master/Formula/[package].rbhistoryget target url https://www.apache.org/dyn/closer.lua?path=spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgzget github urlhttps://raw.githubusercontent.com/Homebrew/homebrew-core/716a9e6e54fd75711ac2254fcaaa3723bf5112ca/Formula/apache-spark.rbbrew install &lt;url&gt;]]></content>
      <categories>
        
          <category> 生活 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Hadoop]]></title>
      <url>/blog/2019/02/20/hadoop</url>
      <content type="text"><![CDATA[Hadoop系统的基本组成架构包含两个部分：分布式存储和并行计算两部分  HDFSNameNode作为分布式存储的主控节点，用以存储和管理分布式文件系统的元数据，同时使用DataNode作为实际存储大规模数据从节点。  MapReduceHadoop使用JobTracker作为MapReduce框架的主控节点，用来管理和调度作业的执行，用TaskTracker管理每个计算从节点上任务的执行。HDFSHadoop Distributed File System (HDFS)旨在存储大量信息，通常为PB。不支持随机读写  单文件TB PB/千万级文件数  顺序访问 提高大规模数据访问效率，支持顺序读，随机访问负载高  一次写多次读访问文件，不支持更新，允许文件尾部添加。块结构文件系统来完成的。单个文件被拆分为固定大小的块，这些块存储在集群上。由多个块组成的文件通常不会将所有块存储在同一台机器上。为了确保可靠性，块在集群中存在副本。复制因子默认是3，每个块在集群中存在3份。通常64MB为一块数据块越大，减少寻址频度和时间开销  NameNote 保存文件系统的元数据（数据块与文件名映射表，数据块副本位置）和命名空间（系统目录结构），处理访问文件的请求  DataNode 存储组成文件的块NameNode和DataNode进程可以在一台机器上运行，但HDFS集群通常由运行NameNode进程的专用服务器和可能有数千台运行DataNode进程的计算机组成。          snakebite  pyarrowNameNodemetadata 包括 文件名，文件权限以及每个文件的每个块的位置。保存在内存。NameNode还跟踪块的复制因子，确保机器故障不会导致数据丢失。由于NameNode是单点故障，因此可以使用辅助NameNode生成主NameNode内存结构的快照，从而降低NameNode失败时数据丢失的风险。当DataNode失败时，NameNode将复制丢失的块以确保每个块满足最小复制因子。FsImage EditLogSecondNameNode  解决EditLog变大，启动慢的问题          SecondNameNode 每隔一段时间拉取和合并NameNode里的FsImageEditLog，这段时间的更新写入到NameNode的EditLog.new合并结束后,新FsImage发送到NameNode覆盖旧的FsImage，EditLog.new覆盖EditLog        如果NameNode在合并时间段内发生故障，系统会丢数据。DataNote读过程写过程复制流程数据接收者会向发送者发送确认包恢复数据NameNodeDataNode数据MD5 sha1局限高级操作  archive 压缩      balancer DataNode数据块副本均匀分布        distcp hdfs拷贝数据  MapReduce  mrjobMapreduce并行模型Mapreduce执行过程                  MapReduce Tutorials  详细讲解MapReduce过程inputformat负责以什么样的格式输入数据，InputFormat可以验证作业数据的输入形式和格式；将输入数据分割为若干个逻辑意义上的InputSplits。RecordReader负责从数据分块中读取数据记录转化为键值对splitsplit基本上和hdfs的基本存储块block同样大小，一个split对应一个map，你可以把它当做map的单位块来理解，投喂进map的时候必须要这样的格式Block and InputSplitmap处理一堆键值对，mapper会顺序单独处理每个键值对，产生若干个output键值对shuffle分为map端操作和reduce端操作map  map任务的输出默认是放在本地磁盘的临时缓冲目录中，每当缓冲区到达上限80%的时候，就会启动一个Spill(溢写)进程，它的作用是把内存里的map task的结果写入到磁盘。  Partitioner 分区 按照key分别映射给不同的reduce,默认是hash(key)%R哈希函数分散到各个reducer去  combiner (optional 自定义 &lt;’a’,1&gt;和&lt;’a’,1&gt; -&gt; &lt;’a’,2&gt;) 减轻网络压力，提高程序效率。Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。  Sort 默认升序  Merge 每次溢写会在磁盘上生成一个溢写文件,当Map输出结果真的很大,存在多个溢写文件（&lt;’a’,1&gt;和&lt;’a’,1&gt; -&gt; &lt;’a’,&lt;1,1» ）将多个溢写文件合并到一个文件，所以可能也有相同的key存在，在这个过程中如果client设置过Combiner，也会使用Combiner来合并相同的key。reducereduce从JobTracker那里获取map task是否完成的信息，得到通知,Shuffle的后半段过程开始启动。  copy Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求从不同的map端 TaskTracker获取map task的输出文件  spill merge combiner sort -&gt; 最终的那个文件  当Reducer的输入文件已定，整个Shuffle才最终结束。默认情况下，这个文件是存放于磁盘中reduce唯一key包含不唯一的值，reducer为每个唯一key合并它们的value,产生若干键值对mapreduce 优化MapReduce Job Optimization  配置优化  Set mapred.compress.map.output to true to enable LZO compression  任务量大，块大小提高，更长时间的任务&gt;1min,reduce任务的数量等于或略小于集群中reduce槽的数量。  Combiner数据输入小文件处理  合并小文件：对小文件进行归档（har）、自定义inputformat将小文件存储成sequenceFile文件  采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。  对于大量小文件Job，可以开启JVM重用map  增大环形缓冲区大小，缓冲区溢写的比例  减少对溢写文件的merge次数  采用combiner提前合并，减少 I/Oreduce  合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。  设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。  规避使用reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。  增加每个reduce去map中拿数据的并行数  集群性能可以的前提下，增大reduce端存储数据内存的大小。IO  采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。  使用SequenceFile二进制文件YARNhttps://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html局限性经典 MapReduce 的最严重的限制主要关系到可伸缩性、资源利用和对与 MapReduce 不同的工作负载的支持。在 MapReduce 框架中，作业执行受两种类型的进程控制：一个称为 JobTracker 的主要进程，它协调在集群上运行的所有作业，分配要在 TaskTracker 上运行的 map 和 reduce 任务。许多称为 TaskTracker 的下级进程，它们运行分配的任务并定期向 JobTracker 报告进度。  单个 JobTracker 导致的可伸缩性瓶颈  Hadoop 设计为仅运行 MapReduce 作业。随着替代性的编程模型（比如 Apache Giraph 所提供的图形处理）的到来，除 MapReduce 外，越来越需要为可通过高效的、公平的方式在同一个集群上运行并共享资源的其他编程模型提供支持。改变我们减少了单个 JobTracker 的职责，将部分职责委派给 TaskTracker，因为集群中有许多 TaskTracker。在新设计中，这个概念通过将 JobTracker 的双重职责（集群资源管理和任务协调）分开为两种不同类型的进程来反映。不再拥有单个 JobTracker，一种新方法引入了一个集群管理器，它惟一的职责就是跟踪集群中的活动节点和可用资源，并将它们分配给任务。对于提交给集群的每个作业，会启动一个专用的、短暂的 JobTracker 来控制该作业中的任务的执行。有趣的是，短暂的 JobTracker 由在从属节点上运行的 TaskTracker 启动。因此，作业的生命周期的协调工作分散在集群中所有可用的机器上。得益于这种行为，更多工作可并行运行，可伸缩性得到了显著提高。yarn（资源管理，任务调度，任务监控）  ResourceManager 代替集群管理器 资源管理  ApplicationMaster 代替一个专用且短暂的 JobTracker 任务调度，任务监控  NodeManager 代替 TaskTracker  一个分布式应用程序代替一个 MapReduce 作业在 YARN 架构中，一个全局 ResourceManager 以主要后台进程的形式运行，它通常在专用机器上运行，在各种竞争的应用程序之间仲裁可用的集群资源。ResourceManager 会追踪集群中有多少可用的活动节点和资源，协调用户提交的哪些应用程序应该在何时获取这些资源。ResourceManager 是惟一拥有此信息的进程，所以它可通过某种共享的、安全的、多租户的方式制定分配（或者调度）决策（例如，依据应用程序优先级、队列容量、ACLs、数据位置等）。在用户提交一个应用程序时，一个称为 ApplicationMaster 的轻量型进程实例会启动来协调应用程序内的所有任务的执行。这包括监视任务，重新启动失败的任务，推测性地运行缓慢的任务，以及计算应用程序计数器值的总和。这些职责以前分配给所有作业的单个 JobTracker。ApplicationMaster 和属于它的应用程序的任务，在受 NodeManager 控制的资源容器中运行。NodeManager 是 TaskTracker 的一种更加普通和高效的版本。没有固定数量的 map 和 reduce slots，NodeManager 拥有许多动态创建的资源容器。容器的大小取决于它所包含的资源量，比如内存、CPU、磁盘和网络 IO。目前，仅支持内存和 CPU (YARN-3)。未来可使用 cgroups 来控制磁盘和网络 IO。一个节点上的容器数量，由配置参数与专用于从属后台进程和操作系统的资源以外的节点资源总量（比如总 CPU 数和总内存）共同决定。有趣的是，ApplicationMaster 可在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。您还可以实现一个自定义的 ApplicationMaster 来运行特定的任务，进而发明出一种全新的分布式应用程序框架，改变大数据世界的格局。您可以查阅 Apache Twill，它旨在简化 YARN 之上的分布式应用程序的编写。一个可运行任何分布式应用程序的集群ResourceManager、NodeManager 和容器都不关心应用程序或任务的类型。所有特定于应用程序框架的代码都转移到它的 ApplicationMaster，以便任何分布式框架都可以受 YARN 支持 — 只要有人为它实现了相应的 ApplicationMaster。  向yarn提交应用程序  ResourceManager接收处理客户端请求，为应用程序分配一个容器，与容器内NodeManager通信，启动ApplicationMaster。  ApplicationMaster创建后向ResourceManager注册，ResourceManager可以了解应用程序状态。  ApplicationMaster向ResourceManager申请资源，成功后，与容器内NodeManager通信，启动Tasks。  Tasks通过RPC反馈状态到ApplicationMaster。Task失败会ApplicationMaster重启  Tasks完成,ApplicationMaster向ResourceManager注销，关闭自己。ApplicationMaster失败会ResourceManager重启，知道task完成。]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Bigdata for Mac Install]]></title>
      <url>/blog/2019/02/20/hadoop-for-mac</url>
      <content type="text"><![CDATA[Hadoop  Local (Standalone) Mode 本地（独立）模式  （默认情况）  Pseudo-Distributed Mode 伪分布式模式  Fully-Distributed Mode  全分布式模式伪分布式模式准备工作.ssh检查〜/ .ssh / id_rsa和〜/ .ssh / id_rsa.pub文件是否存在，以验证是否存在ssh localhost密钥。 如果这些存在继续前进，如果不存在ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys系统配置System Preferences -&gt; Sharing, change Allow access for: All Usersssh loginssh: connect to host localhost port 22: Connection refused 远程登录已关闭$ sudo systemsetup -getremoteloginRemote Login: offopen port 22$ sudo systemsetup -setremotelogin on$ ssh localhostLast login: ...install hadoop3.1.1 version不同版本的配置看 官方文档 Single Node Setupbrew install hadoophadoop配置go to /usr/local/Cellar/hadoop/3.1.1/libexec/etc/hadoophadoop-env.sh获取JAVA_HOME➜  hadoop echo $JAVA_HOME/Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Home加入hadoop-env.shexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Homecore-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;hdfs-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;yarn 配置mapred-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;yarn-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;启动关闭格式化hdfs namenode -formathadoopstart-dfs.shstop-dfs.shhttp://localhost:9870Permission denied: user=dr.who, access=READ_EXECUTE, inode="/tmp"core-site.xml&lt;property&gt;    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;    &lt;value&gt;&lt;user&gt;&lt;/value&gt;&lt;/property&gt;或者hdfs-default.xmldfs.permissions.enabled=true #是否在HDFS中开启权限检查,默认为trueyarnstart-yarn.shstop-yarn.shhttp://localhost:8088源码编译brew install gcc autoconf automake libtool cmake snappy gzip bzip2 protobuf@2.5 zlib openssl maven  必须是protobuf@2.5 安装后按照说明配置好PATH protoc version is 'libprotoc 3.6.1', expected version is '2.5.0'hadoop checknative -amvn package -Pdist,native -DskipTests -Dtar -eerrorNative Libraries GuideNative build fails on macos due to getgrouplist not foundOPENSSL_ROOT_DIRexport OPENSSL_ROOT_DIR="/usr/local/opt/openssl"export LDFLAGS="-L${OPENSSL_ROOT_DIR}/lib"export CPPFLAGS="-I${OPENSSL_ROOT_DIR}/include"export PKG_CONFIG_PATH="${OPENSSL_ROOT_DIR}/lib/pkgconfig"export OPENSSL_INCLUDE_DIR="${OPENSSL_ROOT_DIR}/include"check native经过好久终于可以了，配置好上面的话会更快。时间主要花在下载maven上面，老是ssl error复制native libraries到cp -R hadoop/hadoop-dist/target/hadoop-3.1.1/lib /usr/local/Cellar/hadoop/3.1.1/libexeccheck build➜  libexec hadoop checknative -a2019-02-24 21:19:16,068 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version2019-02-24 21:19:16,072 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library2019-02-24 21:19:16,078 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicableNative library checking:hadoop:  true /usr/local/Cellar/hadoop/3.1.1/libexec/lib/native/libhadoop.dylibzlib:    true /usr/lib/libz.1.dylibzstd  :  falsesnappy:  true /usr/local/lib/libsnappy.1.dyliblz4:     true revision:10301bzip2:   falseopenssl: false build does not support openssl.ISA-L:   false libhadoop was built without ISA-L support2019-02-24 21:19:16,106 INFO util.ExitUtil: Exiting with status 1: ExitExceptionSparkbrew install apache-spark scala2.4.0 versionspark-doc在环境配置上有HADOOP_CONF_DIR才能用spark-shell --master yarnexport HADOOP_CONF_DIR=/usr/local/Cellar/hadoop/3.1.1/libexec/etc/hadoop不要忘记sourceHivebrew install hive3.1.1 version  Embedded mode (Derby) 内嵌模式 (实验)  默认此模式下，Metastore使用Derby数据库，数据库和Metastore服务都嵌入在主HiveServer进程中。当您启动HiveServer进程时，两者都是为您启动的。此模式需要最少的配置工作量，但它一次只能支持一个活动用户，并且未经过生产使用认证。  Local mode 本地元存储          在本地模式下，Hive Metastore服务在与主HiveServer进程相同的进程中运行，但Metastore数据库在单独的进程中运行，并且可以位于单独的主机上。嵌入式Metastore服务通过JDBC与Metastore数据库通信。        Remote mode  远程元存储(推荐)  通过Thrift连接clinet和远程metastore服务hive.metastore.uris,metastore服务通过jdbc连接db’javax.jdo.option.ConnectionURL’  远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。单独的主机上运行HiveServer进程可提供更好的可用性和可伸缩性。metastore 支持常用的数据库  Derby  MySQL  MS SQL Server  Oracle  Postgresmysql 配置jdbc下载mysql-connector-java.jarcp jar /usr/local/Cellar/hive/3.1.1/libexec/libcp mysql-connector-java-8.0.15.jar /usr/local/Cellar/hive/3.1.1/libexec/libset up mysqlConfiguring the Hive Metastoremysql -u root -pCREATE DATABASE metastore;USE metastore;CREATE USER 'hive'@'metastorehost' IDENTIFIED BY 'mypassword';REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'metastorehost';GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'metastorehost';FLUSH PRIVILEGES;quit;hive confcd /usr/local/Cellar/hive/3.1.1/libexec/confcp hive-default.xml.template hive-site.xml先看一下hive-site.xml有没有报错艹尼玛 apache !!!!!再修改错误处理  :3210定位    Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8 at [row,col,system-id]: [3210,96,"file:/usr/local/Cellar/hive/3.1.1/libexec/conf/hive-site.xml"]  at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3003)  at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2931)  at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2806)  at org.apache.hadoop.conf.Configuration.get(Configuration.java:1460)  at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:4990)  at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:5063)  at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5150)  at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5093)  at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:97)  at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:81)  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:699)  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at org.apache.hadoop.util.RunJar.run(RunJar.java:318)  at org.apache.hadoop.util.RunJar.main(RunJar.java:232)Caused by: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8 at [row,col,system-id]: [3210,96,"file:/usr/local/Cellar/hive/3.1.1/libexec/conf/hive-site.xml"]  at com.ctc.wstx.sr.StreamScanner.constructWfcException(StreamScanner.java:621)  at com.ctc.wstx.sr.StreamScanner.throwParseError(StreamScanner.java:491)  at com.ctc.wstx.sr.StreamScanner.reportIllegalChar(StreamScanner.java:2456)  at com.ctc.wstx.sr.StreamScanner.validateChar(StreamScanner.java:2403)  at com.ctc.wstx.sr.StreamScanner.resolveCharEnt(StreamScanner.java:2369)  at com.ctc.wstx.sr.StreamScanner.fullyResolveEntity(StreamScanner.java:1515)  at com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2828)  at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1123)  at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3257)  at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3063)  at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2986)  ... 17 more        java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D启动HIVE时java.net.URISyntaxException将以下内容放在hive-site.xml的开头&lt;property&gt;    &lt;name&gt;system:java.io.tmpdir&lt;/name&gt;    &lt;value&gt;/tmp/hive/java&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;system:user.name&lt;/name&gt;    &lt;value&gt;${user.name}&lt;/value&gt;&lt;/property&gt;hive-site.xml&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  &lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&lt;/value&gt;  &lt;description&gt;the URL of the MySQL database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;  &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;  &lt;value&gt;mypassword&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.metastore.uris&lt;/name&gt;  &lt;value&gt;thrift://localhost:9083&lt;/value&gt;  &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt;&lt;/property&gt;初始化metastore &amp; 测试schematool -dbType mysql -initSchemahivehive&gt; show tables;ERRORschematool -dbType mysql -initSchema  https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool  https://stackoverflow.com/questions/42209875/hive-2-1-1-metaexceptionmessageversion-information-not-found-in-metastoreFAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient因为没有正常启动Hive 的 Metastore Server服务进程。hive --service metastoreMetaException(message:Version information not found in metastore.)Hive on Spark  Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Hive on Spark doc&lt;property&gt;  &lt;name&gt;hive.execution.engine&lt;/name&gt;  &lt;value&gt;spark&lt;/value&gt;&lt;/property&gt;版本问题  Hive on Spark: Getting StartedFailed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create Spark client for Spark session 54d3f345-ca78-42c5-b3f4-5e7f07f77ae0)FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session 54d3f345-ca78-42c5-b3f4-5e7f07f77ae0spark 通过hive.metastore.uris 连接hivecp hive-site.xml /usr/local/Cellar/apache-spark/2.4.0/libexec/conf启动Hive Metastore Serverhive --service metastorefrom pyspark.sql import SparkSessionspark = SparkSession.builder\    .config("hive.metastore.uris", "thrift://localhost:9083/")\    .enableHiveSupport() \    .getOrCreate()    spark.sql('....')]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Bigdata Element]]></title>
      <url>/blog/2019/02/20/bigdata-element</url>
      <content type="text"><![CDATA[Pigpig online doc  高级数据流语言 Pig Latin  运行Pig Latin程序的执行环境Pig能够让你专心于数据及业务本身，而不是纠结于数据的格式转换以及MapReduce程序的编写。Execution Modeslocalonly requires a single machine. Pig will run on the local host and access the local filesystem.pig -x local ...MapReducepig ...Interactive ModePig can be run interactively in the Grunt shell.pig -x local...grunt&gt;Batch Modeuse pig scriptpig -x local id.pigpig 语法Each statement is an operator that takes a relation as an input, performs a transformation on that relation,and produces a relation as an out‐ put. Statements can span multiple lines,;结尾。  A LOAD statement that reads the data from the filesystem  One or more statements to transform the data  A DUMP or STORE statement to view or store the resultsloadUSING default keyword \t AS default not named and type bytearrayLOAD 'data' [USING function] [AS schema];A = LOAD 'students' AS (name:chararray, age:int);DUMP A; (john,21,3.89) (sally,19,2.56) (alice,22,3.76) (doug,19,1.98) (susan,26,3.25)Transforming Data条件关系 and or notFILTER处理列A = LOAD 'students' AS (name:chararray, age:int, gpa:float);DUMP A; (john,21,3.89)(sally,19,2.56) (alice,22,3.76)(doug,19,1.98)(susan,26,3.25)R = FILTER A BY age&gt;=20;DUMP R; (john,21,3.89) (alice,22,3.76) (susan,26,3.25)FOREACH处理行 有点像selectR = FOREACH A GENERATE *;GROUPB = GROUP A BY age;STORESTORE alias INTO 'directory' [USING function];A = LOAD 'students' AS (name:chararray, age:int, gpa:float);STORE A INTO 'output' USING PigStore('|');CAT output;UDFpig_utilfrom pig.pig_util import outputSchema@outputSchema('word:chararray')def reverse(word):    """    Return the reverse text of the provided word """    return word[::-1]@outputSchema('length:int')def num_chars(word):    """    Return the length of the provided word """    return len(word)REGISTER 'my_udf.py' USING streaming_python AS string_udf;term_length = FOREACH unique_terms GENERATE word, string_udf.num_chars(word) as length;HiveHiveHBase数据模型概念表行列族限定字符单元格时间戳列族存储          降低IO  大并发查询  高数据压缩比架构与Hadoop访问过程，结构有点像zookeepermasterregion表 RegionRegion 定位结构MemStore容量有限，周期性写入到StoreFile,HLog写入一个标记。每次缓存刷新生成新的StoreFile，当StoreFile数量到达某个阈值，会合并一个大StoreFile。当大StoreFile大小到达某个阈值，会分裂。读写局限NoSQL不需要事务，读写实时性，没有复杂SQL查询。种类SparkSpark流计算  静态数据 批量计算 时间充足批量处理海量数据  流数据 实时计算流数据特征实时采集实时计算实时查询Storm  streamparse基本组件在Storm上实际上运行的是Topologystream无限Tuple序列spoutsTuple序列源头bolts处理Tuples ,创建stream。Topologyspouts和bolts组成的网络抽象成Topology，组件之间沟通Stream Groups告知Topology如何在两个组件之间进行Tuple传输。架构Zookeeper作为分布式协调组件，负责Nimbus和多个supervisor之间的协调工作。Spark Streaming将stream拆分成小量批处理, 做不到毫秒级别，storm 可以对比storm做不到毫秒级别，storm 可以]]></content>
      <categories>
        
          <category> big data </category>
        
      </categories>
      <tags>
        
          <tag> big data </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[apk]]></title>
      <url>/blog/2019/02/16/apk</url>
      <content type="text"><![CDATA[APK 打包过程dex 文件odex 文件结构]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Dalvik]]></title>
      <url>/blog/2019/02/15/dalvik</url>
      <content type="text"><![CDATA[DalvikDalvik 字节码JVM 基于栈架构，程序run时频繁栈上读写数据，耗费CPU时间Dalvik 虚拟机基于寄存器架构dex 反编译得到.smalidex 优化体积Dalvik 指令寄存器字节码类型方法public void onResume() {    super.onResume();    n();    m();}.method public onResume()V    .locals 0    .line 485    invoke-super {p0}, Lcom/awesapp/isafe/core/ToolbarActivity;-&gt;onResume()V    .line 486    invoke-direct {p0}, Lcom/awesapp/isafe/core/MainActivity;-&gt;n()V    .line 487    invoke-direct {p0}, Lcom/awesapp/isafe/core/MainActivity;-&gt;m()V    return-void.end method字段public static g a;private long A;MusicPlayerBottomSheetFragment c;boolean d;private ImageView l;private BottomSheetBehavior&lt;LinearLayout&gt; m;@BindView(2131296417)DrawerLayout mDrawerLayout;# static fields.field public static a:Lcom/awesapp/isafe/svs/a/g;# instance fields.field private A:J.field c:Lcom/awesapp/isafe/musicplayer/MusicPlayerBottomSheetFragment;.field d:Z.field private l:Landroid/widget/ImageView;.field private m:Landroid/support/design/widget/BottomSheetBehavior;    .annotation system Ldalvik/annotation/Signature;        value = {            "Landroid/support/design/widget/BottomSheetBehavior&lt;",            "Landroid/widget/LinearLayout;",            "&gt;;"        }    .end annotation.end field.field mDrawerLayout:Landroid/support/v4/widget/DrawerLayout;    .annotation build Lbutterknife/BindView;        value = 0x7f0900a1    .end annotation.end fieldnopmovereturnconst操作指令数组指令异常跳转gotoswitchif字段操作方法调用运算内部类 xx$xx]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[actions]]></title>
      <url>/blog/2019/02/07/action</url>
      <content type="text"><![CDATA[smali语法案例有点像汇编ARM寻址寄存器IDAdebug 静态分析apktool d test.apk &lt;-o output&gt;apktool b bar &lt;-o new_bar.apk&gt;检查apk校验工作日志信息adb logcat -s &lt;Tag&gt;分析nativeIDA实战小demo 找不到合适IDA for mac破解版 莫名crash 难以实战分析思路动态分析smali步骤decompile apkapktool d test.apk &lt;-o output&gt;修改 AndroidManifest.xml&lt;application android:debuggable="true"不然debug会报错  Unable to open debugger port (localhost:7800): java.net.SocketException "connection reset"）入口Activity一般有action和category&lt;activity  android:name="xxx.MainActivity" &gt;    &lt;intent-filter&gt;        &lt;action android:name="android.intent.action.MAIN"/&gt;        &lt;category android:name="android.intent.category.LAUNCHER"/&gt;    &lt;/intent-filter&gt;&lt;/activity&gt;rebuild apkapktool 会遇到错误apktool b bar &lt;-o new_bar.apk&gt; &lt; 2&gt; error.txt &gt;  error: Error retrieving parent for item: No resource found that matches the given name ‘@android:xxxx’.一般情况下，此时更新最新的apktool.jar就可以解决问题，但有时单纯更新apktool也不行。其实问题不在apktool，而是framework太旧了！如果你只是更新了apktool.jar，而没有更新framework，这时候很有可能还会遇到最初的问题。如果你不知道自己的framework路径，就去运行一遍apktool d命令，命令行中会显示使用的framework路径rm -rf /Users/xxxxx/Library/apktool/framework/1.apk当你下次运行apktool时，会自动安装对应的framework，这样一来就算完全更新了apktool。  error: Public symbol xxx declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:25: error: Public symbol array/aouts declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:26: error: Public symbol array/aouts_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:27: error: Public symbol array/audio_title_alignment_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:28: error: Public symbol array/audio_title_alignment_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:29: error: Public symbol array/chroma_formats declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:30: error: Public symbol array/chroma_formats_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:31: error: Public symbol array/deblocking_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:32: error: Public symbol array/deblocking_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:33: error: Public symbol array/dev_hardware_decoder_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:34: error: Public symbol array/dev_hardware_decoder_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:35: error: Public symbol array/hardware_acceleration_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:36: error: Public symbol array/hardware_acceleration_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:37: error: Public symbol array/screen_orientation_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:38: error: Public symbol array/screen_orientation_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:39: error: Public symbol array/subtitles_encoding_list declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:40: error: Public symbol array/subtitles_encoding_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:41: error: Public symbol array/vouts declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:42: error: Public symbol array/vouts_froyo declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:43: error: Public symbol array/vouts_values declared here is not defined.W: /Users/wyx/stash/isafe/res/values/public.xml:44: error: Public symbol array/vouts_values_froyo declared here is not defined.我注释掉对应文件的对应行重新打包签名手动签名zipalign 可以确保所有未压缩的数据的开头均相对于文件开头部分执行特定的字节对齐，这样可减少应用消耗的 RAM 量。可选zipalign -v -p 4 my-app-unsigned.apk my-app-unsigned-aligned.apk更新到sdk/build-tools/28.0.3使用apksigner进行签名使用 keytool 生成一个私钥 my-release-key.jkskeytool -genkey -v -keystore my-release-key.jks -keyalg RSA -keysize 2048 -validity 10000 -alias my-alias在本例中，在使用单密钥库文件 my-release-key.jks 中存储的私钥和证书签署 APK 后，将以 my-app-release.apk 的形式输出签署的 APK。apksigner sign --ks my-release-key.jks --out my-app-release.apk my-app-unsigned-aligned.apk验证您的 APK 是否已签署：apksigner verify my-app-release.apk安装apk先卸载原来的apk 否侧adb: failed to install xxx.apk: Failure [INSTALL_FAILED_UPDATE_INCOMPATIBLE: Package [package name] signatures do not match previously installed version; ignoring!]adb install my-app-release.apk导入到AS安装插件安装 smali插件下载导入，重启AS导入工程导入反编译后的文件夹 一直next直到finish配置project这步做不做好像没所谓配置jdk        debug 配置 remote打开对应的APP➜ adb shell dumpsys activity top|grep ACTIVITY  ACTIVITY com.github.shadowsocks/.MainActivity 6b899d7 pid=25351  ACTIVITY net.oneplus.launcher/.Launcher ab1eb3 pid=3549  ACTIVITY com.awesapp.isafe/.core.MainActivity cec1c35 pid=29081端口转发adb forward tcp:5005 jdwp:29081下断点res/values/public.xml了解程序结构，找破解的地方。在View确定控件。  apk toast message 反馈 直接查字符串  手动logadb logcat -s SN:v  逐步调试  DDMS 里面的Method Profiling  找控件 adb shell dumpsys activity top &gt; 4.txt 定位Activity,再debug小思路内购登录注册动态分析so逆向加固  apk 先解压出classes.dex使用dex2jar+jdgui  使用apktool反编译]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[tools]]></title>
      <url>/blog/2019/02/04/tools</url>
      <content type="text"><![CDATA[工具汇总apktool源码得到smali 使用apkstudio查看  jd-gui  enjarify听说比dex2jar强  ApkDecoder()核心类xposed免root xposed VirtualXposed基于xposed脱壳 ZjDroid脱壳Cydia Substrate可以hook java or sotoolbrew install apktool dex2jar jadx  ClassyShark.jar小工具  Android-Crack-Tool for mac  BombAPKexe 套装  [AndroidKiller]  [GDA]  ApkToolBox工具镜像  santoku安全测试框架  drozer 源Mercury 权限攻击（Receiver）  Android应用的动态分析 droidbox  androguard]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[permission 权限]]></title>
      <url>/blog/2019/02/02/permission</url>
      <content type="text"><![CDATA[辅助功能监听当前window变化风险设备管理通知栏allowBackupAndroidManifest.xml application  android:allowBackup="true"应用数据备份和恢复可以设置密码adb backup -nosystem -f &lt;xx.ab&gt; &lt;package name&gt;abe解析.abab 2 tarabe unpack &lt;backup.ab&gt; &lt;backup.tar&gt; [password]adb restore &lt;.ab&gt;]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[fence 防护机制]]></title>
      <url>/blog/2019/02/02/fence</url>
      <content type="text"><![CDATA[混淆安全防护 减小包大小代码混淆  classes.dex + dex2jar -&gt; jar-&gt; jd-gui  apktool -&gt; smali资源混淆AndResGuard类似Java Proguard，但是只针对资源，将原本冗长的资源路径变短签名没有签名apk不能安装，每个app都有一个唯一签名。同一包名不同签名不允许。debug有个默认签名文件进行签名。为了防止二次打包，检查签名。都二次打包改代码了，干掉检查很难？APK签名机制数字签名  确保消息来源  确保消息不被篡改S : 发送者R : 接收者对比摘要A 和摘要B是否一致 不能确保正确的公钥-&gt;数字证书signjarsign , signapk用keytool生成keystore 使用 jarsign 进行签名签名检查class.dex CRC反调试反IDATracerPid &gt; 0反Androidxml android:debuggable=True调试器模拟器Android组件安全Activity串谋权限攻击防止被外部利用劫持Broadcast Receiver发送安全ServiceContent Provider数据安全外部存储权限solve内部存储MODE_PRIVATE linux 权限  通过root/等更高权限可以破解MODE_WORLD_READABLE其他apk 加固解合并脱壳流程summary  源APK  脱壳APK = 壳dex + 壳rest  加密解密程序encrypt/decrypt(可以使用native来做)加壳classes.dex = encrypt(源APK) + 壳dexAPK = sign(classes.dex + 壳rest)脱壳run : APK-&gt; classes.dex -&gt; decrypt(加密源APK) -&gt; 加载源APKso好难弄懂so加载过程init_array段是在so加载的时候执行的 执行顺序要优先于 JNI_OnLoad  所以这里是最早被执行的函数 把反调试和so的解密放到这里是比较好的选择。JNI_OnLoadapk 签名检验public class myJNI {　　//加载so库    static {        System.loadLibrary("JniTest");    }　　//native方法    public static native String sayHello();}调用myJNI.sayHello();动态注册section 加固原理实现summary函数加密]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[android file parse]]></title>
      <url>/blog/2019/02/01/androidparse</url>
      <content type="text"><![CDATA[so fileELFELF(Executable and Linking Format)是一种对象文件的格式用于定义不同类型的对象文件(Object files)中都放了什么东西、以及都以什么样的格式去放这些东西  可重定位的对象文件(Relocatable file)由汇编器汇编生成的 .o 文件  可执行的对象文件(Executable file)可执行应用程序  可被共享的对象文件(Shared object file)动态库文件，也即 .so 文件参数so 头readelf -h xx.so  -a –all              全部       Equivalent to: -h -l -S -s -r -d -V -A -I  -h –file-header      文件头   Display the ELF file header  -l –program-headers  程序 Display the program headers  –segments An alias for –program-headers  -S –section-headers  段头 Display the sections’ header  –sections            An alias for –section-headers  -e –headers          全部头      Equivalent to: -h -l -S  -s –syms             符号表      Display the symbol table  –symbols             An alias for –syms  -n –notes            内核注释     Display the core notes (if present)  -r –relocs           重定位     Display the relocations (if present)  -u –unwind            Display the unwind info (if present)  -d –dynamic          动态段     Display the dynamic segment (if present)  -V –version-info     版本    Display the version sections (if present)  -A –arch-specific    CPU构架   Display architecture specific information (if any).  -D –use-dynamic      动态段    Use the dynamic section info when displaying symbols  -x –hex-dump= 显示 段内内容Dump the contents of section   -w[liaprmfFso] or  -I –histogram         Display histogram of bucket list lengths  -W –wide              宽行输出      Allow output width to exceed 80 characters  -H –help              Display this information  -v –version           Display the version number of readelf  Seation HeaderSeation Header一般都是多个用一个List来保存  Program HeadersAndroid manifestR idaapt将Android打包成resource.arsc 从文本xml -&gt; 二进制xml在[sdk home]/build-toolsaapt l -a &lt;apk&gt; &gt; xx.xmlapk 构建过程Resource.arscdex在[sdk home]/build-tools编译出dexdx -dex --output=&lt;path&gt; &lt;.class&gt;]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[常用命令]]></title>
      <url>/blog/2019/01/31/command_cmd</url>
      <content type="text"><![CDATA[常用命令  adb连接情况adb devices -l  获取当前包名和入口类名adb shell dumpsys window windows | grep -E 'mCurrentFocus'adb shell dumpsys activity top|grep ACTIVITY包名/入口类名mCurrentFocus=Window{c61e50b u0 com.tencent.mm/com.tencent.mm.ui.LauncherUI}  包信息AndroidManifest.xmladb shell dumpsys package &lt;包名&gt;aapt dump xmltree &lt;apk&gt; AndroidManifest.xml &gt; &lt;.txt&gt;  db信息adb shell dumpsys dbinfo &lt;包名&gt;  apk 文件apkadb install [-r 升级安装] /uninstallget apk# show all packageadb shell pm list packages# get base apkadb shell pm path &lt;pageage&gt;adb pull &lt;apk path&gt; &lt;local path&gt;文件adb pull（拉到本地） / push（push到设备）  src tar截图rootadb shell screencap -p &lt;设备路径&gt;adb shell screenrecord &lt;设备路径 sdcard/tmp.mp4&gt;adb pull sdcard/tmp.mp4 Downloadsadb shell rm -rf sdcard/tmp.mp4  logadb logcat -s tag]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> android hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Web Hack 工具]]></title>
      <url>/blog/2019/01/28/web_hack_tool</url>
      <content type="text"><![CDATA[nmapnmap  通过对设备或者防火墙的探测来审计其安全性。  探测目标主机的开放端口。  网络存储、网络映射、维护和资产管理。（这个有待深入）  通过识别新的服务器审计网络的安全性。  探测网络上的主机。    script    NSE 的脚本引擎 --script=&lt;类型&gt;    auth: 负责处理鉴权证书（绕开鉴权）的脚本  broadcast: 在局域网内探查更多服务开启状况，如dhcp/dns/sqlserver等服务  brute: 提供暴力破解方式，针对常见的应用如http/snmp等  default: 使用-sC或-A选项扫描时候默认的脚本，提供基本脚本扫描能力  discovery: 对网络进行更多的信息，如SMB枚举、SNMP查询等  dos: 用于进行拒绝服务攻击  exploit: 利用已知的漏洞入侵系统  external: 利用第三方的数据库或资源，例如进行whois解析  fuzzer: 模糊测试的脚本，发送异常的包到目标机，探测出潜在漏洞  intrusive: 入侵性的脚本，此类脚本可能引发对方的IDS/IPS的记录或屏蔽  malware: 探测目标机是否感染了病毒、开启了后门等信息  safe: 此类与intrusive相反，属于安全性脚本  version: 负责增强服务与版本扫描（Version Detection）功能的脚本  vuln: 负责检查目标机是否有常见的漏洞（Vulnerability），如是否有MS08_067scan要扫描1 ~ 1024 端口，详细输出，并且探测操作系统。nmap 192.168.1.1 -p 1-1024 -vv -Osqlmappip install sqlmap直接使用-u命令把 URL 给 SqlMap 会判断注入点判断所有的动态参数sqlmap -u http://localhost/sql.php?id=指定某个参数，使用-psqlmap -u http://localhost/sql.php?id= -p id获取数据库及用户名称–dbs用于获取所有数据库名称，–current-db用于获取当前数据库，–current-user获取当前用户sqlmap -u http://localhost/sql.php?id= -p id --current-db获取表名-D用于指定数据库名称，如果未指定则获取所有数据库下的表名。–tables用于获取表名。sqlmap -u http://localhost/sql.php?id= -p id -D test --tables获取列名-T用于指定表名，–columns用于获取列名。sqlmap -u http://localhost/sql.php?id= -p id -D test -T email --columns获取记录–dump用于获取记录，使用-C指定列名的话是获取某一列的记录，不指定就是获取整个表。sqlmap -u http://localhost/sql.php?id= -p id -D test -T email --dump]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> web hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Web Hack 原理]]></title>
      <url>/blog/2019/01/27/%E5%8E%9F%E7%90%86</url>
      <content type="text"><![CDATA[http://tool.chinaz.com/map.aspx信息whois 域名 注册人名 邮箱http://whois.chinaz.comipping ip get cdn ip http://ping.chinaz.comhow get src ip  check subdomain 一般只有主站cdn    https://toolbar.netcraft.com/site_report?url=历史记录cdn前一个ip就是真实ip  目录  gh 用法  gh dbCSRF跨越请求伪造 cross-site request forgeryCSRF 攻击可以在受害者毫不知情的情况下以受害者名义伪造请求发送给受攻击站点，从而在并未授权的情况下执行在权限保护之下的操作。攻击者创建一个链接，受害者点击它之后就可以完成攻击者想要的操作，这些操作一般是删除文章，创建用户之类。比如某网站的删除文章链接是http://www.xxx.com/post/&lt;id&gt;/delete，那么攻击者可以直接构造出来发给有权限的人，它点击之后就可以将文章删除。当然，攻击者也可以使用当下流行的短网址服务来伪造 URL，避免受到怀疑。请求会附带浏览器中的目标域名的cookie一起发向目标服务器tar查询类的API不用保护。改变类的需要csrf保护防策略HTTP Referer  简单易行，安全敏感的请求统一增加一个拦截器来检查 Referer 的值就可以  浏览器Referer被篡改  用户自己可以设置浏览器使其在发送请求时不再提供Referercsrf token要抵御 CSRF，关键在于在请求中放入黑客所不能伪造的信息，并且该信息不存在于 cookie 之中。HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。  非常麻烦参数          在一个网站中，可以接受请求的地方非常多，要对于每一个请求都加上 token 是很麻烦的，并且很容易漏掉，通常使用的方法就是在每次页面加载时，使用 javascript 遍历整个 dom 树，对于 dom 中所有的 a 和 form 标签后加入 token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的 html 代码，这种方法就没有作用，还需要程序员在编码时手动添加 token。        难以保证 token 本身的安全          系统可以在添加 token 的时候增加一个判断，如果这个链接是链到自己本站的，就在后面添加 token，如果是通向外网则不加。      黑客的网站也同样可以通过 Referer 来得到这个 token 值以发动 CSRF 攻击。      Header 中自定义属性并验证使用 token 并进行验证 ,放到 HTTP 头中自定义的属性。解决了上种方法在请求中加入 token 的不便，同时，通过 XMLHttpRequest 请求的地址不会被记录到浏览器的地址栏，也不用担心 token 会透过 Referer 泄露到其他网站中去。局限性非常大，并非所有的请求都适合用XMLHttpRequest。对于没有进行 CSRF 防护的遗留系统来说，要采用这种方法来进行防护，要把所有请求都改为 XMLHttpRequest 请求，这样几乎是要重写整个网站，这代价无疑是不能接受的。summary通过上文讨论可知，目前业界应对 CSRF 攻击有一些克制方法，但是每种方法都有利弊，没有一种方法是完美的。如何选择合适的方法非常重要。如果网站是一个现有系统，想要在最短时间内获得一定程度的 CSRF 的保护，那么验证 Referer 的方法是最方便的，要想增加安全性的话，可以选择不支持低版本浏览器，毕竟就目前来说，IE7+, FF3+ 这类高版本浏览器的 Referer 值还无法被篡改。如果系统必须支持 IE6，并且仍然需要高安全性。那么就要使用 token 来进行验证，在大部分情况下，使用 XmlHttpRequest 并不合适，token 只能以参数的形式放于请求之中，若你的系统不支持用户自己发布信息，那这种程度的防护已经足够，否则的话，你仍然难以防范 token 被黑客窃取并发动攻击。在这种情况下，你需要小心规划你网站提供的各种服务，从中间找出那些允许用户自己发布信息的部分，把它们与其他服务分开，使用不同的 token 进行保护，这样可以有效抵御黑客对于你关键服务的攻击，把危害降到最低。毕竟，删除别人一个帖子比直接从别人账号中转走大笔存款严重程度要轻的多。如果是开发一个全新的系统，则抵御 CSRF 的选择要大得多。笔者建议对于重要的服务，可以尽量使用 XMLHttpRequest 来访问，这样增加 token 要容易很多。另外尽量避免在 js 代码中使用复杂逻辑来构造常规的同步请求来访问需要 CSRF 保护的资源，比如 window.location 和 document.createElement(“a”) 之类，这样也可以减少在附加 token 时产生的不必要的麻烦。SQL 注入基于 回显页面中显示db信息  主要通过union注入点and 1=1  and 1=2列数order by ? ? &gt;= 1 数字 直到N -&gt; 列数 = N-1用户 db1 and 1=2 union select 1,concat(current_user(),' ',database())表数量1 and 1=2 union select 1,count(table_name) from information_schema.tables where table_schema=database()表名1 and 1=2 union select 1,table_name from information_schema.tables where table_schema=database() limit ?,1表的列数量1 and 1=2 union select 1,count(column_name) from information_schema.columns where table_name='email'表列名1 and 1=2 union select 1,column_name from information_schema.columns where table_name='email' limit ?,1行数1 and 1=2 union select 1, count(1) from email记录1 and 1=2 union select 1,concat(userid,' ',email) from email limit ?,1基于 bool在一些情况下，页面上是没有回显的。也就是说，不显示任何数据库中的信息。我们只能根据输出判断是否成功、失败、或者错误。这种情况就叫做盲注。如果我们想查询整数值，构造布尔语句直接爆破；如果想查询字符串值，先爆破它的长度，再爆破每一位查询用户及数据库名称1 and (select length(database()))=?sqlmappip install sqlmap用法SSRF很多 Web 应用都提供了从其他服务器上获取数据的功能。使用用户指定的 URL，web 应用可以获取图片，下载文件，读取文件内容等。这个功能如果被恶意使用，可以利用存在缺陷的 Web 应用作为代理，攻击远程和本地服务器（内网）。这种形式的攻击成为服务器请求伪造（SSRF）漏洞可能性  分享：通过 URL 分享网页内容如果在此功能中没有对目标地址范围做过滤与限制，就存在 SSRF 漏洞。  转码服务：通过 URL 地址把原地址的网页内容调优使其适合手机屏幕浏览  在线翻译：通过 URL 地址翻译对应文本的内容。提供此功能的国内公司有百度、有道等  图片加载与下载：通过 URL 地址加载或下载图片  图片、文章收藏功能排除ssrf图片是百度上的，你调用的是搜狗API，浏览器向百度请求图片，那么就不存在 SSRF 漏洞。如果浏览器向搜狗请求图片，那么就说明搜狗服务器发送了请求，向百度请求图片，可能存在 SSRF。绕过方法URL 绕过访问@后面的urlip 转换127.0.0.1-&gt;0x7F.0x00.0x00.0x01 or 0177.0000.0000.0001URL 跳转&lt;?php header("Location: $_GET['url']"); ?&gt;保存为urllocation.php然后部署，之后可以用http:///urllocation.php?url=来跳转。短网址百度短网址自定义dns解析IP xip.ioXSS跨站脚本攻击（Cross Site Scripting）恶意攻击者往 Web 页面里插入恶意 js 代码，当用户浏览器该页之时，嵌入 Web 页面里的代码会被执行，从而达到恶意攻击用户的目的。XSS 是实现 CSRF 的诸多途径中的一条，但绝对不是唯一的一条。一般习惯上把通过 XSS 来实现的 CSRF 称为 XSRF。攻击方式payload  在 XSS 中指代攻击代码或攻击语句  反射型：Payload 经过后端，不经过数据库  存储型：Payload 经过后端，经过数据库  DOM：Payload 不经过后端反射型需要欺骗用户点击链接才能触发 XSS 代码（数据库中没有这样的页面和内容）。Payload 一般存在于 URL 或者 HTTP 正文中，需要构造页面，或者构造 URL。提交js代码到表单，没有过滤。浏览器直接解析代码。&lt;?php//关闭浏览器xss保护header('X-XSS-Protection: 0');?&gt;    &lt;p&gt;反射型 XSS 演示&lt;/p&gt;    &lt;form action="" method="get"&gt;        &lt;input type="text" name="xss"/&gt;        &lt;input type="submit" value="test"/&gt;    &lt;/form&gt;&lt;?php$xss = @$_GET['xss'];if ($xss !== null) {    echo $xss;}输入&lt;script&gt;alert(document.cookie)&lt;/script&gt;这个例子中 URL 为http://localhost/xss.php?xss=%3Cscript%3Ealert%281%29%3C%2Fscript%3E，这个 URL 容易引起怀疑，可以使用短网址工具缩短后发送给受害者。从上面的例子中，我们可以看出，反射型 XSS 的数据流向是：浏览器 -&gt; 后端 -&gt; 浏览器。存储型代码储存在数据库中。如在个人信息或发表文章等地方，假如代码，如果没有过滤或过滤不严，那么这些代码将储存到数据库中，用户访问该页面的时候出发代码执行。这种 XSS 比较危险，容易造成蠕虫，盗窃 Cookie 等。浏览器 -&gt; 后端 -&gt; 数据库 -&gt; 后端 -&gt; 浏览器。xss平台我们可能需要通过 XSS 来获得用户 Cookie 或其他有用信息，利用平台负责接收并保存这些信息。另外，利用平台能够托管利用脚本，于是我们可以向页面只注入一个脚本链接，使长度极大缩短。  xss平台第三方漏洞  域名商  云服务器商  外源js  Java中间件口令爆破  Burp Suite  PKAV Fuzzer提权越权  水平越权：权限不变，身份改变  垂直越权：身份不变，权限改变信息遍历通过遍历id=xxx来获取其他信息。信息遍历属于水平越权，因为用户还是普通用户，但是身份变成了其它的普通用户。burp 爆破这玩意慢死了，如果不复杂的话，还不如直接上手编程。隐藏式后台扫描器扫出后台地址，然后尝试访问。绕过修改cookie的某些值，后台不做检查可以绕过。改密码等上传文件上传漏洞的目的是拿到 WebShell，也就是取得一定的服务器权限。绕过 Content-type限制利用 Burp Suite 直接改Content-type        检验 文件后缀名貌似绕不开]]></content>
      <categories>
        
          <category> hack </category>
        
      </categories>
      <tags>
        
          <tag> web hack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[量化择时]]></title>
      <url>/blog/2019/01/25/%E6%8B%A9%E6%97%B6</url>
      <content type="text"><![CDATA[判断大势涨、跌、盘整趋势型指标MAMACDDMATRIX组合指标降低风险，提高收益率自适应指标定参限制，震荡短期均线频繁转向，市场快速上升或下跌长期均线反应迟钝。目标：震荡用长均线 趋势用短均线summary市场情绪熊牛线马尔科夫方向模型分形有效市场分形市场SVM异常指标噪音行业集中度套利条件      定价模型        指数复制        冲击成本        期现套利  ]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化投资策略技术 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[衡量指标]]></title>
      <url>/blog/2019/01/24/%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87</url>
      <content type="text"><![CDATA[收益总收益率年化收益相对收益α收益率风险β 系数绝对值越大 相对于大盘波动性越大。 if &lt; 0 变化方向与大盘方向相反夏普率局限最大回撤最大回撤往往代表亏损极限信息比率用来衡量超额风险带来的超额收益，比率高说明超额收益高，单位主动风险所带来超额收益。]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化投资策略技术 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[选股]]></title>
      <url>/blog/2019/01/16/%E9%80%89%E8%82%A1</url>
      <content type="text"><![CDATA[选择股票组合多因子核心  因子选取  因子综合得到最终判断how 选取候选因子选取主要靠经验计算有效性  因子的大小和收益明显的相关  极端1、n组合能够有明显的指示作用，高区分度。删除冗余因子模型判断  加权打分  回归根据打分选择TOP100/TOP20%summary要根据市场变化不断维护因子轮动市场之间存在循环的相关性分类根据某种标准区分市场：市值、收益率、市场情绪等。不同的市场用不同策略，动量（可以理解为惯性），动量翻转, 持有的时间。when 转换市场使用logistic n个因素判断是否转换，操作收益 if &gt; 简单买入并持有的收益。市场情绪MACD趋势型，剧烈波动或盘整。观测频率为周，可以过滤大量市场噪音，而保留主要趋势。  关键在于明确市场情绪指标选取，指标参数，适当止损。可以多参数多策略同时轮动投资，提高策略稳健性。资金流MF 指标注意问题  取样的时间频率  参考价格计算  指标失效CMSMF 指标策略动量反转  动量（ps:怎么不叫惯性，刚接触一面懵逼）前段时间怎样，未来一段时间就怎样  反转（ps:怎么不叫惯性，刚接触一面懵逼）前段时间怎样，未来一段时间就反趋势重点参数  趋势持续时间，趋势幅度、持有时间EMH有效市场假说（Efficient Markets Hypothesis，简称EMH）  在市场上的每个人都是理性的经济人，金融市场上每只股票所代表的各家公司都处于这些理性人的严格监视之下，他们每天都在进行基本分析，以公司未来的获利性来评价公司的股票价格，把未来价值折算成今天的现值，并谨慎地在风险与收益之间进行权衡取舍。  股票的价格反映了这些理性人的供求的平衡，想买的人正好等于想卖的人，即，认为股价被高估的人与认为股价被低估的人正好相等，假如有人发现这两者不等，即存在套利的可能性的话，他们立即会用买进或卖出股票的办法使股价迅速变动到能够使二者相等为止。  股票的价格也能充分反映该资产的所有可获得的信息，即”信息有效”，当信息变动时，股票的价格就一定会随之变动。一个利好消息或利空消息刚刚传出时，股票的价格就开始异动，当它已经路人皆知时，股票的价格也已经涨或跌到适当的价位了。“有效市场假说”实际上意味着”天下没有免费的午餐”，世上没有唾手可得之物。在一个正常的有效率的市场上，每个人都别指望发意外之财，所以我们花时间去看路上是否有钱好拣是不明智的，我们费心去分析股票的价值也是无益的，它白费我们的心思。CAMP资本资产定价模型（Capital Asset Pricing Model，CAPM)CAPMmba-wiki动量测试测试步骤超额收益，战胜基准组合反转测试测试步骤策略  等权重再平衡趋势跟踪等待突破后的大趋势波段，在突破（阻力位/压力位）进行建仓/平仓操作。判断大趋势，减少扰动对趋势影响。  寻找刻画趋势的指标  使用样本外的若干几年进行测试，调试指标参数  参数稳定后，实盘操作模型  均线  HH、LL 记录局部最高最低点  均线过滤盘整通过转折点 判断主趋势高点：比两端高 L低点：比两端低 H高低点比较  信号          buy 发出sell后 第一个当前L &gt; L_pre      sell 发出buy后 第一个当前H &lt; H_pre        局限          出现频率高      假突破，幅度不够      没有考虑形成趋势的时间长短            补充 drift    时间短的趋势比时间长同涨幅的趋势，买卖信号要强          buy 当前L &gt; L_pre + drift      sell 当前H &lt; H_pre + drift        高低点突破有可能不存在第二个次高峰，直接从最高峰不断下滑。  buy H &gt; H_pre + drift  sell L &lt; L_pre + drift大波段保护现价 P  涨幅 C  止损幅度Dif P &gt; L_pre(1+C) than  when P &lt; (L_pre+drift)(1+D) sell长均线保护MA(E) &gt; P 不买  直到突破]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化投资策略技术 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概要]]></title>
      <url>/blog/2019/01/16/%E6%A6%82%E8%A6%81</url>
      <content type="text"><![CDATA[概要量化选股  公司估值 基本面判断  趋势  资金流方向量化择时通过历史预测价格变动套利利用价差获利，[不合理的价格，合理价格]盈利空间  套利空间计算  交易成本  市场容量机器学习  关联分析  分类/预测 svm  聚类  分形  随机过程 马尔可夫随机  小波分析 wavelet 行情走势进行波形处理，预测未来走势]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化投资策略技术 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[抽奖问题]]></title>
      <url>/blog/2019/01/10/lottery</url>
      <content type="text"><![CDATA[            奖项      中奖人数 or 权重                  1      500              5      100              10      20              50      5              100      1      区间            奖项      中奖人数 or 权重      区间                  1      500      [0,500)              5      100      [500,600)              10      20      [600,620)              50      5      [620,625)              100      1      [625,626)      随机 rand = Rand([ 0 , sum(权重) ]) rand 在哪个区间就中什么。问题  如果先随机再减库存          只抽sum(权重)次，大概率抽不完。如果有不中奖这个选项还好。      极端奖项，大概率在最后抽到。      可能会不断调整区间，剔除已抽到的项等        库存减少要考虑并发我的解法  在mysql中准备好库存  从库存中随机抽出 select * where xx=null order by rand() for update limit 1 锁住抽出的record  update 该记录 xx字段保证并发，保证抽的完。]]></content>
      <categories>
        
          <category> 生活 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[区块tx解析]]></title>
      <url>/blog/2019/01/06/%E5%8C%BA%E5%9D%97%E8%A7%A3%E6%9E%90</url>
      <content type="text"><![CDATA[维护节点需要巨额的资源，ssd硬盘，很大的内存等。钱包需要交易记录，之前完全没有碰过这方面记录一下。EOS  EOS主网节点  EOSrpc api出块速度0.5s，好尼玛快，比我获取区块信息的速度快get_info 获取高度{	"server_version": "d9ad8eec",	"head_block_num": 8592,	"last_irreversible_block_num": 8591,	"head_block_id": "00002190e805475db152be7d3f4f1a075efaed42827cd551b0e23c7feabbedac",	"head_block_time": "2018-04-27T17:40:34",	"head_block_producer": "eosio"}last_irreversible_block_num 就是最新的不可逆区块高度。get_block 区块信息{    'timestamp': '2019-01-05T13:01:42.500',    'producer': 'eosbeijingbp', 'confirmed': 0,    'previous': '0222e1cb394fc47010985ce7f2a40d65a5333c6b620dac48c7948a3bf40315a1',    'transaction_mroot': '4a7f7396734abcdd519046362a293a17ddd85fe9a9a3ac1d12624320241df6c7',    'action_mroot': 'a80c78de8b736d8c12c7366aa3a799f71b8a9ac9b8532c4e3b065b315d1b4d81', 'schedule_version': 649,    'new_producers': None,    'header_extensions': [],    'producer_signature': 'SIG_K1_KmQRYtEYYqAKMyi1RjQ3YasVuBpqpjyUM4eyQGrKvushRkVN7GdyfkJLZPqoskXPqj58BAVQdJN4CJeW5APBVjZZAQ5R6h',    'transactions':[...]    'block_extensions': [],    'id': '0222e1cc4d5a0d80a5eb1df4c362be4e97adcf361ed1402c3724edd35e1993dd',    'block_num': 35840460,    'ref_block_prefix': 4095601573  }timestamp是区块UTC时间这非常恶心,为什么不直接给个timestampdef dt2ts(dt, utc=False):    if utc:        return calendar.timegm(dt.timetuple())    if dt.tzinfo is None:        return int(time.mktime(dt.timetuple()))    utc_dt = dt.astimezone(tz.tzutc()).timetuple()    return calendar.timegm(utc_dt)def dtstr2ts(dtstr, fmt='%Y-%m-%d %H:%M:%S', utc=False):    dt = datetime.datetime.strptime(dtstr, fmt)    return dt2ts(dt, utc)def blockts(dtstr, utc=True):    fmt = '%Y-%m-%dT%H:%M:%S.%f' if '.' in dtstr else '%Y-%m-%dT%H:%M:%S'    return dtstr2ts(dtstr, fmt, utc)transactions 就是需要的交易记录内联tx一般内联交易为trx是tx_hash，可以理解为函数里面的调用其他函数,合约代码执行个人看法，看过有些内联交易可以包含很多action普通tx普通交易个人叫法 我也不知道叫法种类很多，还可以自定义action。看看 EOS智能合约的源码 对比 区块浏览器 容易理解很多。普通交易trx里面通常长这个样子eos transfer        个人看法：  account 就是合约名=&gt;代码中包的名字,name就是action名=&gt;函数名，data=&gt;函数的参数常用的action在源码的eosio.system和eosio.token总结EOS 共有三大资源：CPU、NET（网络带宽）和RAM（内存）  我们知道，比特币和以太坊中的交易手续费机制，其目的就是防止大量交易使系统拥堵。而EOS取消了交易手续费，那么如何避免系统资源的滥用？因而EOS设计了一种新的资源使用机制：根据账户中EOS的数量来分配系统资源，包括：RAM(内存), Network Ba​​ndWidth (NET带宽) 以及CPU BandWidth (CPU 带宽）。RAM(内存)在EOS 中, RAM(内存)的主要特点包括：  要将数据存储在区块链中需要消耗RAM，比如在EOS 中转账、购买资源、投票等操作的时候，都有可能会消耗RAM (内存)。  如果你的RAM 消耗殆尽，那么你是无法进行上述这些需要消耗RAM的操作的，所以我们需要有足够的RAM。  通过购买获得的EOS RAM 资源可以买卖，买卖的价格根据市场行情动态调节，这个特点与买卖EOS一样。  RAM可以通过EOS购买的方式获得也可以通过好友帮你购买，这个特点和通过抵押方式获取CPU 资源以及NET 资源不太一样。  用户在买卖RAM 资源的时候，各需要消耗0.5 % (千分之五) 的手续费，总共是1% 的手续费。这笔费用被存在eosio.ramfee 中，由BP 节点进行管理。  内存是消耗资源，不可赎回，只能买卖。  RAM本质上是为智能合约中调用的每个交易提供资源的gas。NET带宽与CPU带宽在EOS中，NET带宽与CPU带宽的特性差不多，它们的主要特点包括：  它们采用抵押EOS的方式获取。当不再需要CPU与带宽时，抵押的EOS通证可以赎回，在赎回的时候，存在三天的赎回期。  如果你持有全网1%的EOS，那就可以抵押这些EOS来获得全网1%的CPU和带宽。这样就可以隔离开所有的DAPP，防止资源竞争和恶意的DDOS供给，无论其他的DAPP如何拥堵， 你自己的带宽都不受影响。  每次使用转账功能时，都会消耗网络带宽资源。  网络带宽取决于过去三天消费的平均值，作为你下一次执行操作的费率。  如果没有足够的网络带宽资源的话，你是无法使用EOS 网络转账等基本功能的。  带宽资源是可以随着时间的推移，自动释放。  NET带宽用于增加算力，CPU带宽增加网络资源ETH  web3 py已经封装好大量api,虽然不太会用。  解析参考  数据源infura15s出块eth_blockNumber 获取高度返回就是最新高度，这意味着eth_blockNumber-30个确认eth_getBlockByNumber 区块信息主要解析transactions中内容.....from: 20 Bytes - address of the sender.to: 20 Bytes - address of the receiver. null when its a contract creation transaction.value: value transferred in Wei.gasPrice: gas price provided by the sender in Wei.gas: gas provided by the sender.input: the data send along with the transaction.这玩意只有ETH的transfer，token没有。对于手续费更是一无所知。 这些都需要绕一个弯刚接触的我懵逼了好久。eth_getTransactionReceipt用于确认tx执行情况....from: 20 Bytes - address of the sender.to: 20 Bytes - address of the receiver. Null when the transaction is a contract creation transaction.gasUsed: the amount of gas used by this specific transaction alone.contractAddress: 20 Bytes - the contract address created, if the transaction was a contract creation, otherwise - null.logs: Array - Array of log objects, which this transaction generated.status: either 1 (success) or 0 (failure)status最终确认tx是否完成。gas这涉及到gas的理解  tx的执行依靠消耗gas支付给矿工，无论成功与否都要收取手续费。  gasPrice 用户愿意花费于每个 Gas 单位的价钱， gas =&gt; Gas limit用户愿意为执行某个操作或确认交易支付的最大Gas量,fee = gasUsed * gasPrice手续费，用不完会退还给用户。  Gas Price越高，交易优先级越高，打包交易速度越快。消耗的gas &gt; Gas limit则tx失败单位处理 一般都是区块中用最小单位WEI# web3 api'gas_price' = Web3.fromWei(raw_trx['gasPrice'], 'ether')units = {    'wei':          decimal.Decimal('1'),  # noqa: E241    'kwei':         decimal.Decimal('1000'),  # noqa: E241    'babbage':      decimal.Decimal('1000'),  # noqa: E241    'femtoether':   decimal.Decimal('1000'),  # noqa: E241    'mwei':         decimal.Decimal('1000000'),  # noqa: E241    'lovelace':     decimal.Decimal('1000000'),  # noqa: E241    'picoether':    decimal.Decimal('1000000'),  # noqa: E241    'gwei':         decimal.Decimal('1000000000'),  # noqa: E241    'shannon':      decimal.Decimal('1000000000'),  # noqa: E241    'nanoether':    decimal.Decimal('1000000000'),  # noqa: E241    'nano':         decimal.Decimal('1000000000'),  # noqa: E241    'szabo':        decimal.Decimal('1000000000000'),  # noqa: E241    'microether':   decimal.Decimal('1000000000000'),  # noqa: E241    'micro':        decimal.Decimal('1000000000000'),  # noqa: E241    'finney':       decimal.Decimal('1000000000000000'),  # noqa: E241    'milliether':   decimal.Decimal('1000000000000000'),  # noqa: E241    'milli':        decimal.Decimal('1000000000000000'),  # noqa: E241    'ether':        decimal.Decimal('1000000000000000000'),  # noqa: E241    'kether':       decimal.Decimal('1000000000000000000000'),  # noqa: E241    'grand':        decimal.Decimal('1000000000000000000000'),  # noqa: E241    'mether':       decimal.Decimal('1000000000000000000000000'),  # noqa: E241    'gether':       decimal.Decimal('1000000000000000000000000000'),  # noqa: E241    'tether':       decimal.Decimal('1000000000000000000000000000000'),  # noqa: E241}token transfer不要尝试解析 input data,这是不准的，有可能会坑。例如0x8ea17abad8c19cf910dcb97a060b1a74d461426c6c8dd8d7bbefc76ee3e84d9b，最好的方法是解析logs。解析logs 每一个log就相当于EOS里面的action，也是多种多样的。....address: 20 Bytes - address from which this log originated.data: contains one or more 32 Bytes non-indexed arguments of the log.topics: Array of 0 to 4 32 Bytes of indexed log arguments. (In solidity: The first topic is the hash of the signature of the event (e.g. Deposit(address,bytes32,uint256)), except you declared the event with the anonymous specifier.)....注意力放在topics数组，通常正常的transfer，topic[0]是函数名,其他的是函数参数。token transfer的函数名TOKEN_TRANSFER_EVENT_TOPIC = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef'这玩意是怎么来的def test3():    t = 'Transfer(address,address,uint256)'    print(Web3.sha3(text=t).hex())'0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef'  log.address =&gt; token_address我们仍然不知道token的symbol是什么？contract ERC20Interface {    function totalSupply() public constant returns (uint);    function balanceOf(address tokenOwner) public constant returns (uint balance);    function allowance(address tokenOwner, address spender) public constant returns (uint remaining);    function transfer(address to, uint tokens) public returns (bool success);    function approve(address spender, uint tokens) public returns (bool success);    function transferFrom(address from, address to, uint tokens) public returns (bool success);    event Transfer(address indexed from, address indexed to, uint tokens);    event Approval(address indexed tokenOwner, address indexed spender, uint tokens);}ETH合约编译后产生ABI,ABI就是告诉我们合约怎样解析，如果没有严格遵循ERC20的规则，就可能解析不出来。字符集必须使用utf8mb4。解析token需要获取symbol和decimals，decimals是小数点偏移的位数ERC20_ABI = json.loads('''[{"constant":true,"inputs":[],"name":"name","outputs":[{"name":"","type":"string"}],"payable":false,"stateMutability":"view","type":"function"},{"constant":false,"inputs":[{"name":"_spender","type":"address"},{"name":"_value","type":"uint256"}],"name":"approve","outputs":[{"name":"","type":"bool"}],"payable":false,"stateMutability":"nonpayable","type":"function"},{"constant":true,"inputs":[],"name":"totalSupply","outputs":[{"name":"","type":"uint256"}],"payable":false,"stateMutability":"view","type":"function"},{"constant":false,"inputs":[{"name":"_from","type":"address"},{"name":"_to","type":"address"},{"name":"_value","type":"uint256"}],"name":"transferFrom","outputs":[{"name":"","type":"bool"}],"payable":false,"stateMutability":"nonpayable","type":"function"},{"constant":true,"inputs":[],"name":"decimals","outputs":[{"name":"","type":"uint8"}],"payable":false,"stateMutability":"view","type":"function"},{"constant":true,"inputs":[{"name":"_owner","type":"address"}],"name":"balanceOf","outputs":[{"name":"","type":"uint256"}],"payable":false,"stateMutability":"view","type":"function"},{"constant":true,"inputs":[],"name":"symbol","outputs":[{"name":"","type":"string"}],"payable":false,"stateMutability":"view","type":"function"},{"constant":false,"inputs":[{"name":"_to","type":"address"},{"name":"_value","type":"uint256"}],"name":"transfer","outputs":[{"name":"","type":"bool"}],"payable":false,"stateMutability":"nonpayable","type":"function"},{"constant":true,"inputs":[{"name":"_owner","type":"address"},{"name":"_spender","type":"address"}],"name":"allowance","outputs":[{"name":"","type":"uint256"}],"payable":false,"stateMutability":"view","type":"function"},{"anonymous":false,"inputs":[{"indexed":true,"name":"_from","type":"address"},{"indexed":true,"name":"_to","type":"address"},{"indexed":false,"name":"_value","type":"uint256"}],"name":"Transfer","type":"event"},{"anonymous":false,"inputs":[{"indexed":true,"name":"_owner","type":"address"},{"indexed":true,"name":"_spender","type":"address"},{"indexed":false,"name":"_value","type":"uint256"}],"name":"Approval","type":"event"}]''')checksum_address = self._web3.toChecksumAddress(token_address)contract = self._web3.eth.contract(address=checksum_address, abi=eth_config.ERC20_ABI)# symbol 入库用utf8mb4 utf8mb4是utf8的超集 支持更多的特殊符号symbol = self._call_contract_function(contract.functions.symbol())decimals = self._call_contract_function(contract.functions.decimals())通过 etherscan.io 查看BNB read contract        通常topic有三项  topic[0] =&gt; 函数名 TOKEN_TRANSFER_EVENT_TOPIC  topic[1] =&gt; from 发送人 uint256  topic[2] =&gt; to 接收者 uint256# uint256 还在前面补0 实质上获取后40位就可以了def address(addr_str, eth_addr_len=eth_cfg.ETH_ADDRESS_LEN):    addr_str = addr_str.strip()    if addr_str is None:        return None    if addr_str and len(addr_str) &gt; eth_addr_len:        return ('0x' + addr_str[-eth_addr_len:]).lower()    return normalized_address(addr_str)def normalized_address(address):    if address is None or not isinstance(address, str):        return address    return address.lower().strip()  log.data =&gt; quantity 发送数量 token_value( hex_to_int , token获得的 decimals )def hex_to_int(hex_string):    if hex_string is None:        return None    if hex_string == '0x':        return 0    return int(hex_string, 16)# 单位转化def token_value(int_value, decimals):    if decimals is None:        return decimal.Decimal(int_value)    unit = decimal.Decimal(10 ** decimals)    with localcontext() as ctx:        ctx.prec = 999        d = decimal.Decimal(value=int_value, context=ctx)        result_value = d / unit    return result_value总结  智能合约传上以太坊之后，它就变得不可更改, 这种永固性意味着你的代码永远不能被调整或更新。完全不用担心函数被人篡改而得到意外的结果。  可以留个后门修改DAPP的关键部分，同时声明合约函数的所有权，不然后门被人随便用。所以最好阅读并理解它的源代码，才能防止其中没有被部署者恶意植入后门，也可以找到别人的漏洞为所欲为。  合约中有些不要gas的view函数,存储操作非常多的gas在 ETH 中有两种类型的账户:  一种是被私钥控制的账户，它没有任何的代码.  合约代码控制的账户，能够在每一次收到消息时，执行保存在 contract_code 中的代码，所有的合约在网络中都能够响应其他账户的请求和消息并提供一些服务。账户余额模型每一个账户都包含四个字段 (nonce, ether_balance, contact_code, storage)所有账户的 nonce 都必须从 0 开始递增，当前账户每使用 nonce 签发并广播一笔交易之后，都会将其 +1,来解决重放攻击的问题。BTC  json-rpc-api10min出块，解析起来比ETH更难getblockhash &amp; getblockheight -&gt; getblockhash -&gt; block_hash -&gt; getblock解析出来 只有tx_id数组...tx：区块内所有交易组成的数组，成员为交易id...getrawtransaction{    。。。。    "vin": [        {            "txid": "2ac0daff49a4ff82a35a4864797f99f23c396b0529c5ba1e04b3d7b97521feba",            "vout": 0,            "scriptSig": {                "asm": "3044022013d212c22f0b46bb33106d148493b9a9723adb2c3dd3a3ebe3a9c9e3b95d8cb00220461661710202fbab550f973068af45c294667fc4dc526627a7463eb23ab39e9b[ALL] 0479be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8",                "hex": "473044022013d212c22f0b46bb33106d148493b9a9723adb2c3dd3a3ebe3a9c9e3b95d8cb00220461661710202fbab550f973068af45c294667fc4dc526627a7463eb23ab39e9b01410479be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8"            },            "sequence": 4294967295        }    ],    "vout": [        {            "value": 0.06990000,            "n": 0,            "scriptPubKey": {            "asm": "OP_DUP OP_HASH160 01b81d5fa1e55e069e3cc2db9c19e2e80358f306 OP_EQUALVERIFY OP_CHECKSIG",                "hex": "76a91401b81d5fa1e55e069e3cc2db9c19e2e80358f30688ac",                "reqSigs": 1,                "type": "pubkeyhash",                "addresses": [                    "1A6Ei5cRfDJ8jjhwxfzLJph8B9ZEthR9Z"                ]            }        }    ]}UXTOUnspent Transaction output  未被使用的交易输出。把UXTO想象成一张纸币，是不可分割的整体，使用时存在着找零。账户中的余额并非是一个数字，而是由当前区块链网络中所有跟当前账户有关的UTXO组成的。基于 UTXO 模型的tx由输入和输出两个部分组成。sum(inputs.value) = sum(outputs.value) + feesum(outputs.value) = 需要转账的数量 + 找零vin里面txid,vout说明了in的来源。换句话说 UXTO就是较早已确认区块的未作为in的out        左图的vin中txid=’5f01378fae317945e37200f0445aaf160271c506a48d465c09c8b814ba84b3c8’ vout=2如此类推，就好像一条链一样，有因才有果，可以防止同一个UXTO被使用两次。由于解析有点复杂，因为每个in都不知道对应的地址和数量，必须通过请求一次getrawtransaction才知道答案，而BTC的tx有点多都是几千，一个tx有若干个in（曾经试过一个tx900+in）,从主网节点拿数据，解析很慢。除非从创世块开始保存。由于结构比较怪异类似于多对多的结构，所以分开in、out两个表用tx_hash连接]]></content>
      <categories>
        
          <category> 区块链 </category>
        
      </categories>
      <tags>
        
          <tag> 区块链 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[区块共识]]></title>
      <url>/blog/2019/01/04/%E4%B8%80%E8%87%B4%E6%80%A7</url>
      <content type="text"><![CDATA[分布式系统由于引入了多个节点，所以系统中会出现各种非常复杂的情况；随着节点数量的增加，节点失效、故障或者宕机就变成了一件非常常见的事情，解决分布式系统中的各种边界条件和意外情况也增加了解决分布式一致性问题的难度。节点之间的网络通信收到干扰甚至阻断以及分布式系统的运行速度的差异都是解决分布式系统一致性所面临的难题。保证一致性、可用性和分区容错性一是节点直接可以正常通信，二是冲突需要在有限的时间内解决，只有在这两个条件成立时才能达到最终一致性。拜占庭将军问题它是分布式领域中最复杂、最严格的容错模型。系统不会对集群中的节点做任何的限制，它们可以向其他节点发送随机数据、错误数据，也可以选择不响应其他节点的请求，这些无法预测的行为使得容错这一问题变得更加复杂。有一组将军分别指挥一部分军队，每一个将军都不知道其它将军是否是可靠的，也不知道其他将军传递的信息是否可靠，但是它们需要通过投票选择是否要进攻或者撤退：它们都认为自己的选择是大多数人的选择，这时就出现了严重的不一致问题。拜占庭帝国想要进攻一个强大的敌人，为此派出了10支军队去包围这个敌人。这个敌人虽不比拜占庭帝国，但也足以抵御5支常规拜占庭军队的同时袭击。这10支军队在分开的包围状态下同时攻击。他们任一支军队单独进攻都毫无胜算，除非有至少6支军队（一半以上）同时袭击才能攻下敌国。他们分散在敌国的四周，依靠通信兵骑马相互通信来协商进攻意向及进攻时间。困扰这些将军的问题是，他们不确定他们中是否有叛徒，叛徒可能擅自变更进攻意向或者进攻时间。在这种状态下，拜占庭将军们才能保证有多于6支军队在同一时间一起发起进攻，从而赢取战斗？拜占庭将军问题中并不去考虑通信兵是否会被截获或无法传达信息等问题，即消息传递的信道绝无问题。Lamport已经证明了在消息可能丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。所以，在研究拜占庭将军问题的时候，已经假定了信道是没有问题的.如果叛徒的数量大于或等于1/3，拜占庭问题不可解。假设只有3个人，A、B、C，三人中如果其中一个是叛徒。当A发出进攻命令时，B如果是叛徒，他可能告诉C，他收到的是“撤退”的命令。这时C收到一个“进攻”，一个“撤退“，于是C被信息迷惑，而无所适从。如果A是叛徒。他告诉B“进攻”，告诉C“撤退”。当C告诉B，他收到“撤退”命令时，B由于收到了司令“进攻”的命令，而无法与C保持一致。正由于上述原因，在只有三个角色的系统中，只要有一个是叛徒，即叛徒数等于1/3，拜占庭问题便不可解。Paxos  Raft节点并不会故意的发送错误信息，在类似系统中，最常见的问题就是节点失去响应或者失效工作量证明（POW，Proof-of-Work）请求服务的节点解决一个不容易解答但是容易验证的问题,每一个节点不断改变 NONCE    来得到不同的结果 HASH，如果得到的 SHA-256(block header+NONCE) 结果在小于某个范围 难度 越小难度越大,来获得当前区块的记账权随着网络算力的不断改变比特币也会不断改变当前问题的难度，保证每个区块被发现的时间在 10min 左右防止错误或者无效请求的原理就是增加客户端请求服务的工作量，而适合难度的谜题又能够保证合法的请求不会受到影响攻击者对已出现的区块信息进行修改，必须完成该区块外加之后所有区块的工作量，并最终赶上和超越诚实节点的工作量。除非攻击者掌握过半数的算力POS (Proof-of-Stake)权益证明试图解决POW机制中大量资源被浪费的情况。占有币越多，时间越长， 币龄（每个币每天产生1币龄）越高 就有记账权 每被清空365个币龄，就会从区块中获得0.05个币的利息。委托权益证明（DPOS，Delegated Proof-of-Stake）每一个人选出可以代表自己利益的人参与到记账权的争夺中，这样多个小股东就能够通过投票选出自己的代理人，保障自己的利益。每一个参与者都能够选举任意数量的节点生成下一个区块，得票最多的前 N 个节点会被选择成为区块的创建者，下一个区块的创建者就会从这样一组当选者中随机选取，除此之外，N 的数量也是由整个网络投票决定的，所以可以尽可能地保证网络的去中心化]]></content>
      <categories>
        
          <category> 区块链 </category>
        
      </categories>
      <tags>
        
          <tag> 区块链 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python 部署]]></title>
      <url>/blog/2018/12/11/deploy</url>
      <content type="text"><![CDATA[部署脚本备用查看Linux信息lsb_release -acentossudo yum install -y python-devel openssl-devel python-pip mysql-devel python3-dev sqlite-devel supervisorubuntusudo apt-get install -y build-essential python-dev libmysqlclient-dev python3-dev libssl-dev libffi-dev virtualenv git supervisor wget libsqlite3-devPy3wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tar.xzxz -d Python-3.6.5.tar.xztar -xvf Python-3.6.5.tarcd Python-3.6.5./configuresudo makesudo make install安装virtualenvsudo pip install --upgrade pipsudo pip install virtualenv阿里云ubuntu apt-get update 连接失败一开始以为是网络问题，纠结好久，上网找了些其他源也不行。MMP  /etc/apt/apt.conf 貌似干掉这个就好 这个对更新源有限制]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[anti防盗链]]></title>
      <url>/blog/2018/12/09/anti-theft-chain</url>
      <content type="text"><![CDATA[图片防盗链图片爬虫加 meatahtml head 加meta&lt;meta name="referrer" content="never"&gt;imgimg[‘referrerPolicy’] = ‘no-referrer’# 图片处理    imgs = tag.find_all('img')    if imgs:        for img in imgs:            if img['data-src']:                # img['src'] = anti_referer(img['data-src'])                img['src'] = img['data-src'].split('?')[0]                img['referrerPolicy'] = 'no-referrer'base64适合图不大 不多def anti_referer(url):    b64_src = 'data:image/{content_type};base64,'    content_type_re = re.search(r'mmbiz.qpic.cn/mmbiz_(?P&lt;wx_fmt&gt;.*?)/', url)    try:        content_type = content_type_re.group('wx_fmt')        src = b64_src.format(content_type=content_type)        resp = requests.get(url)        content = resp.content        b64 = base64.b64encode(content).decode()        src += b64        return src    except:        logger.get('error-log').error('[{}]\n'.format(url) + util.error_msg())        return url第三方代理国外首选第三方代理有几率被墙https://images.weserv.nl/?url=nginx没试过 觉得麻烦 上面的轻松nginx 解决微信文章图片防盗链]]></content>
      <categories>
        
          <category> spider </category>
        
      </categories>
      <tags>
        
          <tag> spider </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[WordPress for mac]]></title>
      <url>/blog/2018/11/24/wordpress</url>
      <content type="text"><![CDATA[mampmamp for mac安装过程就是不停next,装完后非常蛋疼， 明明我只想要mamp TMD 还装了mamp pro我有点洁癖，但是不敢动手干掉pro,  因为php不懂端口设置一下 端口，由于本来就有mysql:3306 所以我选了default建库      start server 去到 http//localhost:8888/phpMyAdmin        或者到 /Applications/MAMP/bin/phpMyAdmin/config.inc.php 瞄一下配置才知道默认是这样的  $cfg['Servers'][$i]['user']          = 'root';$cfg['Servers'][$i]['password']      = 'root';由于没用过phpMyAdmin就用客户端连上了。wordpresswordpress下载下载解压放到 htdocs 下面访问 http://localhost:8888/wordpress 进行配置 （路径和解压后的文件名一致 可以改名）按照提示填写 数据库 填上上面的建的库设置中文设置API  WP API doc  JWT Authentication配置 .htaccessfind / -name .htaccess# 添加在配置最前面 不然会报错RewriteEngine onRewriteCond %{HTTP:Authorization} ^(.*)RewriteRule ^(.*) - [E=HTTP_AUTHORIZATION:%1]SetEnvIf Authorization "(.*)" HTTP_AUTHORIZATION=$1不要配置在里面不然会因为某些原因重写.htaccess被覆盖# BEGIN WordPress&lt;IfModule mod_rewrite.c&gt;RewriteEngine OnRewriteBase /wordpress/RewriteRule ^index\.php$ - [L]RewriteCond %{REQUEST_FILENAME} !-fRewriteCond %{REQUEST_FILENAME} !-dRewriteRule . /wordpress/index.php [L]&lt;/IfModule&gt;# END WordPress配置 wp-config.php 密钥 https://api.wordpress.org/secret-key/1.1/salt//** 设置WordPress变量和包含文件。 */define('JWT_AUTH_SECRET_KEY', '');define('JWT_AUTH_CORS_ENABLE', true);...ACF APIdemo自定义字段需要fields[xx]post请求。编辑字段需要先官方API发表文章获取id,再通过ACF api 来修改acf字段]]></content>
      <categories>
        
          <category> tools </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[gunicorn nginx flask supervisor deploy]]></title>
      <url>/blog/2018/11/24/flask_web</url>
      <content type="text"><![CDATA[supervisorbrew install supervisorbrew info supervisorget info for supervisorbrew services start supervisor or ini in /usr/local/etc/supervisord.inimodify iniget[include]files = /usr/local/etc/supervisor.d/*.confgunicorndocinstallpip install geventpip install gunicorngunicorn + gevent + supervisormake log rotate and program autostart[program:gemsky]command = xx/gemsky/.env/bin/gunicorn -w 4 -k gevent app:app -b localhost:5000 --access-logfile - --log-file -directory = xx/gemsky/autostart = trueautorestart = truestartsecs = 5startretries = 3redirect_stderr         = truestdout_logfile_maxbytes = 50MBstdout_logfile_backups  = 3stdout_logfile          = xx/gemsky/logs/gunicorn-supervisor.lognginxdocbrew install nginx/usr/local/etc/nginx/nginx.confnginx will load all files in /usr/local/etc/nginx/servers/brew services start nginx注释 nginx.conf 里面的serveradderror_log   /usr/local/etc/nginx/logs/error.log;include servers/*.conf;在 ‘servers/’添加upstream flask_servers {    server 127.0.0.1:5000;}server {    listen       80;    server_name  192.168.1.101;    charset     utf-8;    location / {        proxy_pass         http://flask_servers;        proxy_set_header   Host             $host;        proxy_set_header   X-Real-IP        $remote_addr;        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;    }}ERRORNginx: 24: Too Many Open Files ErrorIN OSXlaunchctl limit maxfiles# mode 644sudo vim /Library/LaunchDaemons/limit.maxfiles.plist&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"        "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;&lt;plist version="1.0"&gt;  &lt;dict&gt;    &lt;key&gt;Label&lt;/key&gt;    &lt;string&gt;limit.maxfiles&lt;/string&gt;    &lt;key&gt;ProgramArguments&lt;/key&gt;    &lt;array&gt;      &lt;string&gt;launchctl&lt;/string&gt;      &lt;string&gt;limit&lt;/string&gt;      &lt;string&gt;maxfiles&lt;/string&gt;      &lt;string&gt;64000&lt;/string&gt;      &lt;string&gt;524288&lt;/string&gt;    &lt;/array&gt;    &lt;key&gt;RunAtLoad&lt;/key&gt;    &lt;true/&gt;    &lt;key&gt;ServiceIPC&lt;/key&gt;    &lt;false/&gt;  &lt;/dict&gt;&lt;/plist&gt;加载plist文件(或重启系统后生效 launchd在启动时会自动加载该目录的plist)sudo launchctl load -w /System/Library/LaunchDaemons/org.apache.httpd.plist确认更改后的限制launchctl limit maxfilesLinuxulimit -a查看当前系统打开的文件数量lsof | wc -lwatch "lsof | wc -l"查看某一进程的打开文件数量lsof -p pid | wc -llsof -p 1234 | wc -l/etc/security/limits.conf* soft nofile 10240* hard nofile 10240Hard limit 跟Soft limit的差別  Hard limit:  一般指不管任何狀況下，使用者都不被允許超出使用限制量  Soft limit: 則是允許使用者在一定時間範圍內，可以使用超過使用限制量，之後才會作處理（甚至根本不處理）]]></content>
      <categories>
        
          <category> tools </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[AE VAE GAE]]></title>
      <url>/blog/2018/11/19/encode_decode</url>
      <content type="text"><![CDATA[AutoEncoderAE  encode input x, and encodes it using a transformation h to encoded signal y  decodeweights 可以share, encode decode的weights 可以简单地反过来用损失函数的不一样，hidden layer不一样 =&gt; 不同的AEVariational Autoencoder变分自编码器(VAE)根据很多个样本，学会生成新的样本学会数据 x 的分布 P (x)，这样，根据数据的分布就能轻松地产生新样本但数据分布的估计不是件容易的事情，尤其是当数据量不足的时候。VAE对于任意d维随机变量，不管他们实际上服从什么分布，我总可以用d个服从标准高斯分布的随机变量通过一个足够复杂的函数去逼近它。  AE 是还原x  VAE是产生新的x训练完后VAE，使用decode来生成新的X  KL 散度(Kullback–Leibler Divergence) 来衡量两个分布的差异，或者说距离cross-entry 与 KL散度等效 对于损失函数import numpy as npdef KL(P,Q):""" Epsilon is used here to avoid conditional code forchecking that neither P nor Q is equal to 0. """     epsilon = 0.00001     # You may want to instead make copies to avoid changing the np arrays.     P = P+epsilon     Q = Q+epsilon     divergence = np.sum(P*np.log(P/Q))     return divergenceGenerative Adversarial NetworkGAN 生成式对抗网络同时训练两个神经网络  生成器(Generator):记作 G，通过对大量样本的学习，输入一些随机噪声能够生成一些以假乱真的样本  判别器(Discriminator):记作D，接受真实样本和G生成的样本，并进行判别和区分  G 和 D 相互博弈，通过学习，G 的生成能力和 D 的判别能力都逐渐 增强并收敛,交替地训练，避免一方过强核心代码with tf.variable_scope('gan'):    G = generator(noise)    real, r_logits = discriminator(X, reuse=False)    # 注意共享变量    fake, f_logits = discriminator(G, reuse=True)核心Loss# 计算logits, labels距离def sigmod_cost(logits, labels):    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))with tf.variable_scope('loss'):    """    接近1为real 接近0为fake    目标：        G 制造以假乱真的        D 轻易区分真假    就是 G 和 D 之间的对抗    """    g_loss = sigmod_cost(logits=f_logits, labels=tf.ones_like(f_logits))    tf.summary.scalar('g_loss', g_loss)    r_loss = sigmod_cost(logits=r_logits, labels=tf.ones_like(r_logits))    f_loss = sigmod_cost(logits=f_logits, labels=tf.zeros_like(f_logits))    d_loss = r_loss + f_loss    tf.summary.scalar('d_loss', d_loss)Train 同时训练tvar = tf.trainable_variables()dvar = [var for var in tvar if 'discriminator' in var.name]gvar = [var for var in tvar if 'generator' in var.name]d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(d_loss, var_list=dvar)g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(g_loss, var_list=gvar)使用CNN的话没有GPU加速就别搞了，用CPU慢,超级慢。要非常注意shape用普通FC训练 G生成出来的结果def discriminator_fc(X, reuse=None):    """判别器"""    with tf.variable_scope('discriminator', reuse=reuse):        l1 = tf.layers.dense(X, 128, activation=tf.nn.relu, )        logits = tf.layers.dense(l1, 1, )        prob = tf.nn.sigmoid(logits)        return prob, logitsdef generator_fc(X):    """生成器"""    with tf.variable_scope('generator'):        l1 = tf.layers.dense(X, 128, activation=tf.nn.relu, )        prob = tf.layers.dense(l1, 784, activation=tf.nn.sigmoid)        return probwith tf.Session() as sess:    sess.run(tf.global_variables_initializer())    writer = tf.summary.FileWriter('gan_mnist/{}_{}'.format(mode, now), sess.graph)    for i in range(epoch):        batch_X, _ = mnist.train.next_batch(batch_size)        batch_noise = np.random.uniform(-1., 1., [batch_size, z_dim])        _, d_loss_print = sess.run([d_opt, d_loss], feed_dict={X: batch_X, noise: batch_noise, is_train: True})        _, g_loss_print = sess.run([g_opt, g_loss], feed_dict={X: batch_X, noise: batch_noise, is_train: True})        if i % 100 == 0:            summary = sess.run(merged_summary_op, feed_dict={X: batch_X, noise: batch_noise, is_train: True})            writer.add_summary(summary, i)            print('epoch:%d g_loss:%f d_loss:%f' % (i, g_loss_print, d_loss_print))        if i % 500 == 0:            batch_test_noise = np.random.uniform(-1., 1., [batch_size, 100])            samples = sess.run(G, feed_dict={noise: batch_test_noise, is_train: False})            fig = plot(samples)            plt.savefig('{}/{}.png'.format(output_path, str(num_img).zfill(3)), bbox_inches='tight')            num_img += 1            plt.close(fig)    to_gif(output_path, output_gif='{}_{}output'.format(mode, now))]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mnist 分类 tensorflow]]></title>
      <url>/blog/2018/11/09/mnist_clf</url>
      <content type="text"><![CDATA[  MNIST dataset  Tensorflow ValueError: No variables to save from  TensorFlow restore model Key Variable not found in checkpoint这个是我自己的问题🤬未解决 已解决 和 name_scope有关  Tensorflow: how to save/restore a model?#!/usr/bin/env python# -*- coding: utf-8 -*-"""@author = 'wyx'@time = 2018/11/7 15:45@annotation = ''"""from datetime import datetimeimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datafrom tensorflow.python.saved_model import tag_constantsmnist = input_data.read_data_sets("MNIST_data/", one_hot=True)save_model_path = 'mnist_model/model.ckpt'def train():    learning_rate = 0.05    batch_size = 100    max_epochs = 100    num_of_batch = int(mnist.train.num_examples / batch_size)    now = datetime.utcnow().strftime("%Y%m%d%H%M%S")    X = tf.placeholder(tf.float32, shape=[None, 784], name='X')    y = tf.placeholder(tf.float32, shape=[None, 10], name='y')    print(X.name, y.name)    W = tf.get_variable(shape=[784, 10], name='weight')    b = tf.get_variable(initializer=tf.zeros([10]), name='bais')    tf.summary.histogram("weights", W)    tf.summary.histogram("biases", b)    with tf.name_scope('pred'):        y_pred = tf.nn.softmax(tf.matmul(X, W) + b, name='predict')        print(y_pred.name)    with tf.name_scope('loss'):        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))        tf.summary.scalar('loss', loss)        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)    with tf.name_scope('acc'):        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='acc')        print(accuracy.name)    merged_summary_op = tf.summary.merge_all()    init_op = tf.global_variables_initializer()    saver = tf.train.Saver()    with tf.Session() as sess:        sess.run(init_op)        loss_avg = 0        writer = tf.summary.FileWriter('mnist/{}'.format(now), sess.graph)        for epoch in range(max_epochs):            for i in range(num_of_batch):                batch_x, batch_y = mnist.train.next_batch(batch_size)                summary_str, _, l = sess.run([merged_summary_op, optimizer, loss], feed_dict={X: batch_x, y: batch_y})                loss_avg += l                global_step = epoch * num_of_batch + i                writer.add_summary(summary_str, global_step)                if global_step % 100 == 0:                    print('Epoch {}: {} save model'.format(epoch, i))                    # save model in halfway                    saver.save(sess, save_model_path, global_step=global_step)            loss_avg /= num_of_batch            print('Epoch {}: Loss {}'.format(epoch, loss_avg))        print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))        saver.save(sess, save_model_path)        tf.saved_model.simple_save(sess, 'simple_model', inputs={            'X': X,            'y': y,        }, outputs={            'pred': y_pred,        })def predict(mode=1):    def _predict():        graph = tf.get_default_graph()        X = graph.get_tensor_by_name('X:0')        y = graph.get_tensor_by_name('y:0')        accuracy = graph.get_tensor_by_name('acc/acc:0')        print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))        pred = graph.get_tensor_by_name('pred/predict:0')        import matplotlib.pyplot as plt        i = 90        img_orign = mnist.train.images[i]        img = img_orign.reshape((28, 28))        plt.imshow(img, cmap='gray')        plt.title(mnist.train.labels[i])        plt.show()        a = sess.run(pred, feed_dict={X: img_orign.reshape(-1, 784)})        import numpy as np        print(np.argmax(a))    if mode == 1:        meta_path = 'mnist_model/model.ckpt.meta'        checkpoint_path = 'mnist_model'    elif mode == 2:        # stupid var WTF ValueError: No variables to save        _ = tf.Variable(0)        saver = tf.train.Saver()    with tf.Session() as sess:        sess.run(tf.global_variables_initializer())        if mode == 1:            saver = tf.train.import_meta_graph(meta_path)            saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))        elif mode == 2:            saver.restore(sess, save_model_path)        elif mode == 3:            tf.saved_model.loader.load(sess, [tag_constants.SERVING], 'simple_model')        _predict()def check_ckpt():    from tensorflow.python.tools import inspect_checkpoint as chkp    chkp.print_tensors_in_checkpoint_file(save_model_path, tensor_name='', all_tensors=True)if __name__ == '__main__':    # train()    predict(mode=3)    # check_ckpt()MLPdef multi_layer(X):    n_classes = 10    with tf.name_scope('layer'):        l1 = tf.layers.dense(X, 32, activation=tf.nn.relu)        l2 = tf.layers.dense(l1, 16, activation=tf.nn.relu)        out = tf.layers.dense(l2, n_classes)    with tf.name_scope('pred'):        y_pred = tf.nn.softmax(out, name='predict')        print(y_pred.name)    return y_predLiner layerdef liner_layer(X):    W = tf.get_variable(shape=[784, 10], name='weight')    b = tf.get_variable(initializer=tf.zeros([10]), name='bais')    tf.summary.histogram("weights", W)    tf.summary.histogram("biases", b)    with tf.name_scope('pred'):        y_pred = tf.nn.softmax(tf.matmul(X, W) + b, name='predict')        print(y_pred.name)    return y_pred]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[seq2seq]]></title>
      <url>/blog/2018/10/05/seq2seq</url>
      <content type="text"><![CDATA[Basic ModelsSequence to sequence（序列）模型在机器翻译和语音识别方面都有着广泛的应用，针对该机器翻译问题，可以使用“编码网络（encoder network）”+“解码网络（decoder network）”两个RNN模型组合的形式来解决。encoder network将输入语句编码为一个特征向量，传递给decoder network，完成翻译。Picking the most likely sentencemachine translation的目标就是根据输入语句，作为条件，找到最佳翻译语句，使其概率最大：greedy search每次只寻找一个最佳单词作为翻译输出，力求把每个单词都翻译准确。缺点：  每次只搜索一个单词，没有考虑该单词前后关系，概率选择上有可能会出错。  增加了运算成本，降低运算速度Beam SearchGreedy search每次是找出预测概率最大的单词，而beam search则是每次找出预测概率最大的B个单词。其中，参数B表示取概率最大的单词个数，可调。  先从词汇表中找出翻译的第一个单词概率最大的B个预测单词。  分别以B个预测单词为条件，计算每个词汇表单词作为预测第二个单词的概率。从中选择概率最大的B个作为第二个单词的预测值。以此类推，每次都取概率最大的三种预测。最后，选择概率最大的那一组作为最终的翻译语句。  实际应用中，可以根据不同的需要设置B为不同的值。一般B越大，机器翻译越准确，但同时也会增加计算复杂度。改进 beam searchBeam search中，最终机器翻译的概率是乘积的形式：多个概率相乘可能会使乘积结果很小，远小于1，造成数值下溢。为了解决这个问题，可以对上述乘积形式进行取对数log运算，即：因为取对数运算，将乘积转化为求和形式，避免了数值下溢，使得数据更加稳定有效。这种概率表达式还存在一个问题，就是机器翻译的单词越多，乘积形式或求和形式得到的概率就越小，这样会造成模型倾向于选择单词数更少的翻译语句，使机器翻译受单词数目的影响，这显然是不太合适的。因此，一种改进方式是进行长度归一化，消除语句长度影响。实际应用中，通常会引入归一化因子$\alpha$：若$\alpha=1$，则完全进行长度归一化；若$\alpha=0$，则不进行长度归一化。一般令$\alpha=0.7$，效果不错。Error analysis in beam search  beam search实际上不能够给你一个能使最大化$P(y\vert x)$的y值  虽然$y^{*}$是一个更好的翻译结果，RNN模型却赋予它更低的可能性，是RNN模型出现了问题如果beam search算法表现不佳，可以调试参数B；若RNN模型不好，则可以增加网络层数，使用正则化，增加训练样本数目等方法来优化。Bleu Score使用bleu score，对机器翻译进行打分。机器翻译越接近参考的人工翻译，其得分越高，方法原理就是看机器翻译的各个单词是否出现在参考翻译中。机器翻译单词出现在参考翻译单个语句中的次数，取最大次数。分母为机器翻译单词数目，分子为相应单词出现在参考翻译中的次数。这种评价方法较为准确。按照beam search的思想，另外一种更科学的打分方法是bleu score on bigrams，即同时对两个连续单词进行打分。Example：连续单词进行打分。仍然是上面那个翻译例子：French: Le chat est sur le tapis.Reference 1: The cat is on the mat.Reference 2: There is a cat on the mat.MT output: The cat the cat on the mat.bigrams出现在参考翻译单个语句中的次数（取最大次数）/ bigrams及其出现在MIT output中的次数count相应的bigrams precision为：Attention Model Intuition注意力模型原语句很长，要对整个语句输入RNN的编码网络和解码网络进行翻译，则效果不佳。相应的bleu score会随着单词数目增加而逐渐降低。对待长语句，正确的翻译方法是将长语句分段，每次只对长语句的一部分进行翻译，使得bleu score不太受语句长度的影响。Speech recognition语音识别的输入是声音，量化成时间序列。更一般地，可以把信号转化为频域信号，即声谱图（spectrogram），再进入RNN模型进行语音识别。每个单词分解成多个音素（phoneme），构建更精准的传统识别算法。但在end-to-end深度神经网络模型中，一般不需要这么做也能得到很好的识别效果。通常训练样本很大，需要上千上万个小时的语音素材。Trigger Word Detection]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> rnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NLP & Word Embeddings]]></title>
      <url>/blog/2018/10/04/NLP</url>
      <content type="text"><![CDATA[文字和图像有什么共同之处？乍一看，很少。 但是都可以表示成矩阵Matrix。图像的基本单位(cell)是像素(pixel)。我们如何将文本表示为矩阵？嗯，这很简单：矩阵的每一行都是一个向量，代表文本的基本单位。To define what a basic unit is , aggregate similar words together and then denoteeach aggregation (sometimes called cluster or embedding) with a representative symbol.文本实际上不是矩阵，而是更多的向量，因为位于相邻文本行中的两个单词几乎没有共同之处。实际上，对于图像存在显着差异，其中位于相邻列中的两个像素最可能具有一些相关性。  if we have a document with 10 lines of text and each line is a 100-dimensional embedding,then we will represent our text with a matrix 10 x 100. In this very particular image,a pixel is turned on if that sentence x contains the embedding represented by position y.为数据集中包含的文本构建嵌入。就目前而言，将此步骤视为一个黑框，它将单词并将它们映射到聚合（聚类），以便相似的单词可能出现在同一个聚类中。请注意，前面步骤的词汇表是离散的和稀疏的。通过嵌入，我们将创建一个将每个单词嵌入连续密集向量空间的地图。Word Embedding特征向量表述one-hot表征单词的方法最大的缺点就是每个单词都是独立的、正交的，无法知道不同单词之间的相似程度，这样使得算法对相关词的泛化能力不强。使用特征表征（Featurized representation）的方法对每个单词进行编码。也就是使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间。  举个例子，对于这些词，比如我们想知道这些词与Gender（性别）的关系。假定男性的性别为-1，女性的性别为+1，那么man的性别值可能就是-1，而woman就是-1。最终根据经验king就是-0.95，queen是+0.97，apple和orange没有性别可言。特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。该向量的每个元素表示该单词对应的某个特征值。这种特征表征的优点是根据特征向量能清晰知道不同单词之间的相似程度。这种单词“类别”化的方式，大大提高了有限词汇量的泛化能力。这种特征化单词的操作被称为Word Embeddings，即单词嵌入。每个单词都由高维特征向量表征，为了可视化不同单词之间的相似性，可以使用降维操作。featurized representation的优点是可以减少训练样本的数目，前提是对海量单词建立特征向量表述（word embedding），即使是训练样本中没有的单词，也可以根据word embedding的结果得到与其词性相近的单词，从而得到与该单词相近的结果，有效减少了训练样本的数量。Properties of word embeddings        如上图所示，根据等式$e_{man}-e_{woman}\approx e_{king}-e_?$得：余弦相似度 平方距离或者欧氏距离Embedding matrixGet EE 未知待求，每个单词可用embedding vector ew表示神经网络输入层包含6个embedding vactors，每个embedding vector维度是300，则输入层总共有1800个输入。Softmax层有10000个概率输出，与词汇表包含的单词数目一致。其中$E,W^{[1]},b^{[1]},W^{[2]},b^{[2]}$为待求值。对足够的训练例句样本，运用梯度下降算法，迭代优化，最终求出embedding matrix E。Word2Vec词向量就是一个二维矩阵，维度为 V × d，V 是词的总个数，d是词向量的维度。  Skip-Gram:根据当前词预测上下文词语  CBOW(Continuous Bag-of-Words): 根据上下文词语预测当前词Skip-Gramcontext和target的选择方法，比较流行的是采用Skip-Gram模型,I want a glass of orange juice to go along with my cereal.首先随机选择一个单词作为context，例如orange；然后使用一个宽度为5或10（自定义）的滑动窗，在context附近选择一个单词作为target，可以是juice、glass、my等等。最终得到了多个context—target对作为监督式学习样本。训练的过程是构建自然语言模型，经过softmax单元的输出为：其中，$\theta_t$为target对应的参数，$e_c$为context的embedding vector，且$e_c=E\cdot O_c$。相应的loss function为：然后，运用梯度下降算法，迭代优化，最终得到embedding matrix E。然而，这种算法计算量大，影响运算速度。主要因为softmax输出单元为10000个，$\hat y$计算公式中包含了大量的求和运算。  解决方案:分级（hierarchical）的softmax分类器和负采样（Negative Sampling）hierarchical softmax classifier树形分类器,与之前的softmax分类器不同，它在每个数节点上对目标单词进行区间判断，最终定位到目标单词,通常选择把比较常用的单词放在树的顶层，而把不常用的单词放在树的底层。这样更能提高搜索速度。关于context的采样，需要注意的是如果使用均匀采样，那么一些常用的介词、冠词，例如the, of, a, and, to等出现的概率更大一些。但是，这些单词的embedding vectors通常不是我们最关心的，我们更关心例如orange, apple， juice等这些名词等。所以，实际应用中，一般不选择随机均匀采样的方式来选择context，而是使用其它算法来处理这类问题。Negative Sampling判断选取的context word和target word是否构成一组正确的context-target对，一般包含一个正样本和k个负样本。例如，“orange”为context word，“juice”为target word，很明显“orange juice”是一组context-target对，为正样本，相应的target label为1。若“orange”为context word不变，target word随机选择“king”、“book”、“the”或者“of”等。这些都不是正确的context-target对，为负样本，相应的target label为0。一般地，固定某个context word对应的负样本个数k一般遵循：  若训练样本较小，k一般选择5～20；  若训练样本较大，k一般选择2～5即可。Negative sampling的数学模型为：其中，$\sigma$表示sigmoid激活函数。很明显，negative sampling某个固定的正样本对应k个负样本，即模型总共包含了k+1个binary classification。对比之前介绍的10000个输出单元的softmax分类，negative sampling转化为k+1个二分类问题，计算量要小很多，大大提高了模型运算速度。最后提一点，关于如何选择负样本对应的target单词，可以使用随机选择的方法。但有资料提出一个更实用、效果更好的方法，就是根据该词出现的频率进行选择，相应的概率公式为：其中，$f(w_i)$表示单词$w_i$在单词表中出现的概率。CBOW连续词袋模型（Continuous Bag-Of-Words Model）它获得中间词两边的的上下文，然后用周围的词去预测中间的词，这个模型也很有效，也有一些优点和缺点。  CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 通常情况下，Skip-Gram模型用到更多点GloVe word vectorsGloVe算法引入了一个新的参数：其中，i表示context，j表示target。一般地，如果不限定context一定在target的前面，则有对称关系$X_{ij}=X_{ji}$；如果有限定先后，则$X_{ij}\neq X_{ji}$。接下来的讨论中，我们默认存在对称关系$X_{ij}=X_{ji}$。GloVe模型的loss function为：从上式可以看出，若两个词的embedding vector越相近，同时出现的次数越多，则对应的loss越小。为了防止出现“log 0”，即两个单词不会同时出现，无相关性的情况，对loss function引入一个权重因子$f(X_{ij})$：当$X_{ij}=0$时，权重因子$f(X_{ij})=0$。这种做法直接忽略了无任何相关性的context和target，只考虑$X_{ij}&gt;0$的情况。出现频率较大的单词相应的权重因子$f(X_{ij})$较大，出现频率较小的单词相应的权重因子$f(X_{ij})$较小一些。具体的权重因子f(X_{ij})$选取方法可查阅相关论文资料。一般地，引入偏移量，则loss function表达式为：值得注意的是，参数θiθi\theta_i和ejeje_j是对称的。使用优化算法得到所有参数之后，最终的$e_w$可表示为：最后提一点的是，无论使用Skip-Gram模型还是GloVe模型等等，计算得到的embedding matrix EEE的每一个特征值不一定对应有实际物理意义的特征值，如gender，age等。Sentiment Classification情感分类问题的一个主要挑战是缺少足够多的训练样本。而Word embedding恰恰可以帮助解决训练样本不足的问题。不同单词出现的次序直接决定了句意。该RNN模型是典型的many-to-one模型，考虑单词出现的次序，能够有效识别句子表达的真实情感。值得一提的是使用word embedding，能够有效提高模型的泛化能力，即使训练样本不多，也能保证模型有不错的性能。词模型词袋模型：（词,出现次数）统计词频 CountVectorize 词频统计TF-IDF模型评估某一字词对于一个文件集或一个语料库的重要程度如果一个词条在一个类的文档中频繁出现，则说明该词条能够很好代表这个类的文本的特征$W_{tf}$ text frequency (TF) 某文档词频$W_{df}$ 在所有的文档总词频TfidfVectorizer.fit = TfidfTransformer.fit(CountVectorizer.fit_transform)词汇表模型词袋模型:文本由哪些单词组成 无法表达出单词之间的前后关系生成的词汇表对原有句子按照单词逐个进行编码Word2Vec生成词向量 显示词间关系 采用的模型有CBOW(Continuous Bag-Of-Words，即连续的词袋模型)Skip-Gram 两种  CBOW模型能够根据输入周围n-1个词来预测出这个词本身，而Skip-gram模型能够根据词本身来预测周围有哪些词。也就是说，CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量，而Skip-gram模型的输入是词A本身，输出是词A周围的n个单词的词向量。        Doc2Vec分为Distributed Memory (DM) 和Distributed Bag of Words (DBOW)。fasttext有效且快速的方式生成词向量以及进行文档分类 高效LDA一种文档主题模型，包含词、主题和文档三层结构LDA认为一篇文档由一些主题按照一定概率组成，一个主题又由一些词语按照一定概率组成。Why 主题：主题的个数通常为几百，这就把文档使用了维数为几百的向量进行了表示，大大加快了训练速度，并且相对不容易造成过拟合。从某种程度上来说，主题是对若干词语的抽象表示。TextRank TF-IDF使用TextRank提取关键字计算文档相似度simhash  n个(关键词，权重)对  计算关键词的hash，生成(hash,weight），并将hash和weight相乘，这一过程是对hash值加权  将hash和weight相乘的值相加，比如图中的[13, 108, -22, -5, -32, 55]，并最终转换成simhash值110001，转换的规则为正数为1负数为0评论预处理文本去重  原因  做法尽量保留有用的压缩去词短句删除]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> rnn </tag>
        
          <tag> nlp </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[人脸识别与神经风格迁移]]></title>
      <url>/blog/2018/10/03/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB</url>
      <content type="text"><![CDATA[Face recognition  人脸验证（face verification） 输入一张人脸图片，验证输出与模板是否为同一人，即一对一问题  人脸识别（face recognition） 输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题face recognition 更难一些。假设verification错误率是1%，recognition中输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些。One Shot Learning每个人的训练样本只包含一张照片，数据库有K个人，则CNN模型输出softmax层就是K维的。One-shot learning的性能并不好，其包含了两个缺点：  每个人只有一张图片，训练样本少，构建的CNN网络不够健壮  数据库增加另一个人，输出层softmax的维度就要发生变化，相当于要重新构建CNN网络，使模型计算量大大增加，不够灵活相似函数（similarity function）相似函数（similarity function）表示两张图片的相似程度，若较小，则表示两张图片相似。对于人脸识别问题，则只需计算测试图片与数据库中K个目标的相似函数，取其中d(img1,img2)最小的目标为匹配对象。若所有的d(img1,img2)都很大，则表示数据库没有这个人。Siamese NetworkCNN网络（包括CONV层、POOL层、FC层），最终得到全连接层FC，该FC层可以看成是原始图片的编码encoding，表征了原始图片的关键特征。这个网络结构我们称之为Siamese network。每张图片经过Siamese network后，由FC层每个神经元来表征。Triplet LossCNN模型，需要定义合适的损失函数。Triplet Loss需要每个样本包含三张图片：靶目标（Anchor）、正例（Positive）、反例（Negative）。靶目标和正例是同一人，靶目标和反例不是同一人。顺便提一下，这里的$\alpha$也被称为边界margin，类似与支持向量机中的margin。举个例子，若$d(A,P)=0.5，\alpha=0.2，则d(A,N)\geq0.7$。接下来，我们根据A，P，N三张图片，就可以定义Loss function为：相应地，对于m组训练样本，cost function为：关于训练样本，必须保证同一人包含多张照片，否则无法使用这种方法。最好的做法是人为选择A与P相差较大（例如换发型，留胡须等），A与N相差较小（例如发型一致，肤色一致等）。这种人为地增加难度和混淆度会让模型本身去寻找学习不同人脸之间关键的差异，尽力让d(A,P)更小，让d(A,N)更大，即让模型性能更好Face Verification and Binary Classification将两个siamese网络组合在一起，将各自的编码层输出经过一个逻辑输出单元，该神经元使用sigmoid函数，输出1则表示识别为同一人，输出0则表示识别为不同人。每组训练样本包含两张图片，每个siamese网络结构和参数完全相同。这样就把人脸识别问题转化成了一个二分类问题。引入逻辑输出层参数w和b，输出$\hat y$表达式为：其中参数$w_k$和b都是通过梯度下降算法迭代训练得到。在训练好网络之后，进行人脸识别的常规方法是测试图片与模板分别进行网络计算，编码层输出比较，计算逻辑输出单元。为了减少计算量，可以使用预计算的方式在训练时就将数据库每个模板的编码层输出f(x)保存下来。因为编码层输出f(x)比原始图片数据量少很多，所以无须保存模板图片，只要保存每个模板的f(x)即可，节约存储空间。而且，测试过程中，无须计算模板的siamese网络，只要计算测试图片的siamese网络，得到的$f(x^{(i)})$直接与存储的模板$f(x^{(j)})$进行下一步的逻辑输出单元计算即可，计算时间减小了接近一半。这种方法也可以应用在上一节的triplet loss网络中。neural style transfer将一张图片的风格“迁移”到另外一张图片中，生成具有其特色的图片。Cost Function神经风格迁移生成图片G的cost function由两部分组成：C与G的相似程度和S与G的相似程度。其中，$\alpha,\beta是超参数，用来调整J_{content}(C,G)与J_{style}(S,G)的相对比重$。  令G为随机像素点 使用梯度下降算法，不断修正G的所有像素点  J(G)不断减小,G逐渐有C的内容和G的风格Content Cost FunctionJ(G) 的第一部分$J_{content}(C,G)$，它表示内容图片C与生成图片G之间的相似度。使用的CNN网络是之前训练好的模型，例如Alex-Net。需要选择合适的层数l来计算$J_{content}(C,G)$。根据上一小节的内容，CNN的每个隐藏层分别提取原始图片的不同深度特征，由简单到复杂。如果l太小，没有迁移效果；如果l太深，则G上某个区域将直接会出现C中的物体。因此，l既不能太浅也不能太深，一般选择网络中间层。然后比较C和G在l层的激活函数输出与。相应的的表达式为：与越相似，则越小。方法就是使用梯度下降算法，不断迭代修正G的像素值，使不断减小。Style Cost Function利用CNN网络模型，图片的风格可以定义成第l层隐藏层不同通道间激活函数的乘积（相关性）。计算不同通道的相关性，反映了原始图片特征间的相互关系，从某种程度上刻画了图片的“风格”。进阶repo采用不同content loss 和 style loss 或者不同的颜色控制等方法  style-tf blog  hwalsuklee:tensorflow-style-transfer a simple example  cysmith:neural-style-tf 进阶 保留content颜色，多style混合，视频等  luanfujun:deep-photo-styletransfer 非tensorflow 更像照片，而不是画  TensorFlow CNN for fast style transfer]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> cnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RNN基础]]></title>
      <url>/blog/2018/10/03/RNN%E5%9F%BA%E7%A1%80</url>
      <content type="text"><![CDATA[Recurrent Neural Networks 循环神经网络应用领域语音识别，情感分类，机器翻译Notation符号Recurrent Neural Network Model  不同样本的输入序列长度或输出序列长度不同，即$T_x^{(i)}\neq T_x^{(j)}$，$T_y^{(i)}\neq T_y^{(j)}$，造成模型难以统一。解决办法之一是设定一个最大序列长度，对每个输入和输出序列补零并统一到最大长度。但是这种做法实际效果并不理想。  这种标准神经网络结构无法共享序列不同之间的特征。例如，如果某个即“Harry”是人名成分，那么句子其它位置出现了“Harry”，也很可能也是人名。这是共享特征的结果，如同CNN网络特点一样。但是，上图所示的网络不具备共享特征的能力。值得一提的是，共享特征还有助于减少神经网络中的参数数量，一定程度上减小了模型的计算复杂度。序列模型从左到右，依次传递，此例中，$T_x=T_y$。x&lt;t&gt;&#x5230;&#xA0;&#xA0;y^&lt;t&gt;&#x4E4B;&#x95F4;&#x662F;&#x9690;&#x85CF;&#x795E;&#x7ECF;&#x5143;&#x3002;a&lt;t&gt;会传入到第t+1个元素中，作为输入。其中，a&lt;0&gt;一般为零向量。RNN的正向传播（Forward Propagation）过程为：其中，$g(\cdot)$表示激活函数，不同的问题需要使用不同的激活函数。简化为：Backpropagation针对上面识别人名的例子，经过RNN正向传播，单个元素的Loss function为：该样本所有元素的Loss function为：然后，反向传播（Backpropagation）过程就是从右到左分别计算$L(\hat y,y)$对参数$W_{a}，W_{y}，b_a，b_y$Different types of RNNsRNN模型包含以下几个类型：  Many to many: $T_x=T_y$       seq to seq  Many to many: $T_x\neq T_y$  Many to one: $T_x&gt;1,T_y=1$  One to many: $T_x=1,T_y&gt;1$  One to one: $T_x=1,T_y=1$import numpy as npimport tensorflow as tfn_inputs = 3n_neurons = 5X0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))Y0 = tf.tanh(tf.matmul(X0, Wx) + b)Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)init = tf.global_variables_initializer()# Mini-batch: instance 0,instance 1,instance 2,instance 3X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])  # t = 0X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])  # t = 1with tf.Session() as sess: init.run() Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})print(Y0_val,'\n')print(Y1_val)相同模型static_rnn()函数为每个输入调用单元工厂的__call __()函数，创建单元的两个副本（每个单元包含 5 个循环神经元的循环层），并具有共享的权重和偏置项，像前面一样。static_rnn()函数返回两个对象。 第一个是包含每个时间步的输出张量的 Python 列表。第二个是包含网络最终状态的张量。 当你使用基本的单元时，最后的状态就等于最后的输出。X0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1],                                                dtype=tf.float32)Y0, Y1 = output_seqs缺点 这种方法仍然会建立一个每个时间步包含一个单元的图。如果使用具有200个时间步长的输入进行调用，则创建一个具有200个RNN步长的静态图形。首先，图形创建很慢。其次，您无法传递比您最初指定的更长的序列（&gt; 200）。tf.nn.dynamic_rnn解决了这个问题。它使用tf.While循环在执行时动态构造图形。这意味着图表创建速度更快，您可以提供可变大小的批量。import numpy as npimport tensorflow as tfimport pandas as pdif __name__ == '__main__':    n_steps = 2    n_inputs = 3    n_neurons = 5    """    input shape [batch_size, n_steps, n_inputs]    """    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)    init = tf.global_variables_initializer()    X_batch = np.array([        [[0, 1, 2], [9, 8, 7]],  # instance 1        [[3, 4, 5], [0, 0, 0]],  # instance 2        [[6, 7, 8], [6, 5, 4]],  # instance 3        [[9, 0, 1], [3, 2, 1]],  # instance 4    ])    with tf.Session() as sess:        init.run()        outputs_val = outputs.eval(feed_dict={X: X_batch})    print(outputs_val)变长输入序列你应该在调用dynamic_rnn()（或static_rnn()）函数时设置sequence_length参数n_steps = 2n_inputs = 3n_neurons = 5X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)seq_length = tf.placeholder(tf.int32, [None])outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,                                    sequence_length=seq_length)"""例如，假设第二个输入序列只包含一个输入而不是两个输入。 为了适应输入张量X，必须填充零向量（因为输入张量的第二维是最长序列的大小，即 2）"""X_batch = np.array([        # step 0     step 1        [[0, 1, 2], [9, 8, 7]], # instance 1        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)        [[6, 7, 8], [6, 5, 4]], # instance 3        [[9, 0, 1], [3, 2, 1]], # instance 4    ])seq_length_batch = np.array([2, 1, 2, 2])with tf.Session() as sess:    init.run()    outputs_val, states_val = sess.run(        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})actionn_steps = 28n_inputs = 28n_neurons = 150n_outputs = 10learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)logits = tf.layers.dense(states, n_outputs)xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,                                                          logits=logits)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer()"""现在让我们加载 MNIST 数据，并按照网络的预期方式将测试数据重塑为[batch_size, n_steps, n_inputs]。 我们之后会关注训练数据的重塑。"""from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("/tmp/data/")X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))y_test = mnist.test.labelsbatch_size = 150with tf.Session() as sess:    init.run()    for epoch in range(n_epochs):        for iteration in range(mnist.train.num_examples // batch_size):            X_batch, y_batch = mnist.train.next_batch(batch_size)            X_batch = X_batch.reshape((-1, n_steps, n_inputs))            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)NLP如何使用RNN构建语言模型大量的单词语句语料库（corpus）构成足够大的训练集，对corpus的每句话进行切分词（tokenize），建立vocabulary，对每个单词进行one-hot编码。对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。循环神经网络的梯度消失（Vanishing gradients with RNNs）某个word可能与它距离较远的某个word具有强依赖关系。一般的RNN模型每个元素受其周围附近的影响较大，难以建立跨度较大的依赖性。上面两句话的这种依赖关系，由于跨度很大，普通的RNN网络容易出现梯度消失，捕捉不到它们之间的依赖，造成语法错误。RNN也可能出现梯度爆炸的问题，即gradient过大。常用的解决办法是设定一个阈值，一旦梯度最大值达到这个阈值，就对整个梯度向量进行尺度缩小。这种做法被称为gradient clipping。GRU单元（Gated Recurrent Unit（GRU））更好地捕捉深层连接，并改善了梯度消失问题为了解决梯度消失问题，对上述单元进行修改，添加了记忆单元，构建GRU(门控循环单元)GRU单元将会有个新的变量称为c，代表细胞（cell）,记忆细胞的作用是提供了记忆的能力。        这就是GRU单元或者说是一个简化过的GRU单元，它的优点就是通过门决定，当你从左到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是不更新，不更新直到你到你真的需要使用记忆细胞的时候，这可能在句子之前就决定了。因为sigmoid的值，现在因为门很容易取到0值，只要这个值是一个很大的负数，再由于数值上的四舍五入，上面这些门大体上就是0，或者说非常非常非常接近0。所以在这样的情况下，这非常有利于维持细胞的值。因为$\Gamma_{u}$很接近0，可能是0.000001或者更小，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^t$几乎就等于$c^{t-1}$，而且$c^t$的值也很好地被维持了，即使经过很多很多的时间步。这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如说cat和was单词即使被中间的很多单词分割开。上面介绍的是简化的GRU模型，完整的GRU添加了另外一个gate，即$\Gamma_r$，表达式如下：LSTM长短期记忆 long short term memory        LSTM包含三个gates：Γu，Γf，Γo，分别对应update gate，forget gate和output gate。lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)Bidirectional RNN  在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息。  深层的RNN双向循环神经网络，并且这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，对于大量有自然语言处理问题的文本，有LSTM单元的双向RNN模型是用的最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，一个有LSTM单元的双向RNN模型，有前向和反向过程是一个不错的首选。缺点：  需要完整的数据的序列，才能预测任意位置。Deep RNN深层循环神经网络RNN的多个层堆叠在一起构建更深的模型与DNN一样，用上标[l]表示层数。Deep RNNs中的表达式为：我们知道DNN层数可达100多，而Deep RNNs一般没有那么多层，3层RNNs已经较复杂了。n_neurons = 100n_layers = 3basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)multi_layer_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * n_layers)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)states 变量包含了每层的一个张量，这个张量就代表了该层神经单元的最终状态（维度为[batch_size, n_neurons]）]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> rnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[目标检测]]></title>
      <url>/blog/2018/10/02/CNN%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B</url>
      <content type="text"><![CDATA[Object Localization目标定位和目标检测（包含多目标检测）Landmark Detection除了矩形区域检测目标类别和位置外，我们还可以仅对目标的关键特征点坐标进行定位，这些关键点被称为landmarks。Object Detection目标检测的一种简单方法是滑动窗算法。这种算法首先在训练样本集上搜集相应的各种目标图片和非目标图片。注意训练集图片尺寸较小，尽量仅包含相应目标。然后，使用这些训练集构建CNN模型，使得模型有较高的识别率。最后，在测试图片上，选择大小适宜的窗口、合适的步进长度，进行从左到右、从上倒下的滑动。每个窗口区域都送入之前构建好的CNN模型进行识别判断。若判断有目标，则此窗口即为目标区域；若判断没有目标，则此窗口为非目标区域。  滑动窗算法的优点是原理简单，且不需要人为选定目标区域（检测出目标的滑动窗即为目标区域）。  缺点：滑动窗的大小和步进长度都需要人为直观设定。滑动窗过小或过大，步进长度过大均会降低目标检测正确率。如果滑动窗和步进长度较小，整个目标检测的算法运行时间会很长。所以，滑动窗算法虽然简单，但是性能不佳，不够快，不够灵活。Turn Fc layer into conv layer全连接层转变成为卷积层，只需要使用与上层尺寸一致的滤波算子进行卷积运算即可。窗口步进长度与选择的MAX POOL大小有关。如果需要步进长度为4，只需设置MAX POOL为4 x 4即可利用卷积操作代替滑动窗算法，则不管原始图片有多大，只需要进行一次CNN正向计算，因为其中共享了很多重复计算部分，这大大节约了运算成本。之前的滑动窗算法需要反复进行CNN正向计算。Bounding Box Predictions滑动窗口算法有时会出现滑动窗不能完全涵盖目标的问题,YOLO（You Only Look Once）算法将原始图片分割成n x n网格，每个网格代表一块区域        然后，利用上一节卷积形式实现滑动窗口算法的思想，对该原始图片构建CNN网络，得到的的输出层维度为3 x 3 x 8。其中，3 x 3对应9个网格，每个网格的输出包含8个元素：如果目标中心坐标$(b_x,b_y)$不在当前网格内，则当前网格Pc=0；相反，则当前网格Pc=1（即只看中心坐标是否在当前网格内）。判断有目标的网格中，$b_x,b_y,b_h,b_w$限定了目标区域。值得注意的是，当前网格左上角坐标设定为(0, 0)，右下角坐标设定为(1, 1)，$(b_x,b_y)$范围限定在[0,1]之间，但是$b_h,b_w$可以大于1。因为目标可能超出该网格，横跨多个区域，如上图所示。目标占几个网格没有关系，目标中心坐标必然在一个网格之内。划分的网格可以更密一些。网格越小，则多个目标的中心坐标被划分到一个网格内的概率就越小，这恰恰是我们希望看到的。Intersection Over Union交集与并集之比，可以用来评价目标检测区域的准确性。真实目标区域,检测目标区域 之间的交集I, 并集U :IoU可以表示任意两块区域的接近程度。IoU值介于0～1之间，且越接近1表示两块区域越接近。Non-max SuppressionYOLO算法中,可能几个相邻网格都判断出同一目标的中心坐标在其内。使用非最大值抑制（Non-max Suppression）算法。  图示每个网格的Pc值可以求出，Pc值反映了该网格包含目标中心坐标的可信度。  首先选取Pc最大值对应的网格和区域，  然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值（例如0.5）的网格（剔除与该网格交叠较大的网格）。这样就能保证同一目标只有一个网格与之对应，且该网格Pc最大，最可信。  接着，再从剩下的网格中选取Pc最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。Anchor Boxes之前都是一个网格至多只能检测一个目标。那对于多个目标重叠的情况，使用不同形状的Anchor Boxes            同一网格出现了两个目标：人和车。为了同时检测两个目标，我们可以设置两个Anchor Boxes，Anchor box 1检测人，Anchor box 2检测车。每个Anchor box都有一个Pc值，若两个Pc值均大于某阈值，则检测到了两个目标。每个网格多加了一层输出。原来的输出维度是 3 x 3 x 8，现在是3 x 3 x 2 x 8（也可以写成3 x 3 x 16的形式）。这里的2表示有两个Anchor Boxes  使用YOLO算法时，只需对每个Anchor box使用上一节的非最大值抑制即可。Anchor Boxes之间并行实现。Anchor Boxes形状的选择可以通过人为选取，也可以使用其他机器学习算法，例如k聚类算法对待检测的所有目标进行形状分类，选择主要形状作为Anchor BoxesRegion Proposals滑动窗算法会对原始图片的每个区域都进行扫描,为了避免对无用区域的扫描，降低算法运行效率，耗费时间，可以使用Region Proposals的方法对原始图片进行分割算法处理，然后支队分割后的图片中的块进行目标检测。Region Proposals共有三种方法：  R-CNN: 滑动窗的形式，一次只对单个区域块进行目标检测，运算速度慢。  Fast R-CNN: 利用卷积实现滑动窗算法。  Faster R-CNN: 利用卷积对图片进行分割，进一步提高运行速度。比较而言，Faster R-CNN的运行速度还是比YOLO慢一些。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> cnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CNN基础]]></title>
      <url>/blog/2018/10/02/CNN%E5%9F%BA%E7%A1%80</url>
      <content type="text"><![CDATA[卷积神经网络，随着层数的增加，捕捉的区域更大，特征更加复杂，从边缘到纹理再到具体物体。机器视觉（Computer Vision）是深度学习应用的主要方向之一。一般的CV问题包括以下三类:  图片识别  目标检测  图片风格迁移如果图片尺寸较大,普通神经网络输入层的维度,网络权重W非常庞大  神经网络结构复杂，数据量相对不够，容易出现过拟合；  所需内存、计算量较大卷积层计算神经网络由浅层到深层，分别可以检测出图片的边缘特征 、局部特征（例如眼睛、鼻子等）、整体面部轮廓。图片边缘有两类：  垂直边缘（vertical edges）  水平边缘（horizontal edges）图片的边缘检测可以通过与相应滤波器进行卷积来实现卷积层计算过程input中与filter相同size的矩阵，对应格相乘得到的结果再相加得到output，通过filter不断移动step=1,获得完整output卷积用conv_forward()表示；tensorflow中，卷积用tf.nn.conv2d()表示；keras中，卷积用Conv2D()表示。边缘检测example        filter的数值一般需要通过模型训练得到，类似于标准神经网络中的权重W一样由梯度下降算法反复迭代求得。CNN的主要目的就是计算出这些filter的数值。确定得到了这些filter后，CNN浅层网络也就实现了对图片所有边缘特征的检测。padding原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为(n-f+1) x (n-f+1)，注意f一般为奇数。  卷积运算后，输出图片尺寸缩小  原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息为了解决图片缩小的问题，可以使用padding方法，即把原始图片尺寸进行扩展，扩展区域补零，用p来表示每个方向扩展的宽度。经过padding之后，原始图片尺寸为(n+2p) x (n+2p)，filter尺寸为f x f，则卷积后的图片尺寸为(n+2p-f+1) x (n+2p-f+1)。若要保证卷积前后图片尺寸不变，则p应满足：  没有padding操作，p=0，我们称之为Valid convolutions  有padding操作，$p=\frac{f-1}{2}$，我们称之为Same convolutions，保证卷积前后尺寸不变。卷积步长（Strided convolutions）stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前我们默认stride=1。三维卷积（Convolutions over volumes）检测灰度图像的特征，是一个通道，也想检测RGB彩色图像的特征,对应红、绿、蓝三个通道，即通道数为3。三维分别表示图片的高度（height）、宽度（weight）和通道（channel），图像的通道数必须和过滤器的通道数匹配。过程是将每个单通道与对应的filter进行卷积运算，然后再将3通道的和相加，得到输出图片的一个像素值。不同通道的滤波算子可以不相同。单层卷积网络（One layer of a convolutional network）选定滤波器组后，参数数目与输入图片尺寸无关。所以，就不存在由于图片尺寸过大，造成参数过多的情况。参数=filter.Hfilter.Wfilter.C*滤波器组数例如一张1000x1000x3的图片，标准神经网络输入层的维度将达到3百万，而在CNN中，参数数目只由滤波器组决定，数目相对来说要少得多，这是CNN的优势之一。最后，我们总结一下CNN单层结构的所有标记符号，设层数为l。  $f^{[l]}$ = filter size  $p^{[l]}$ = padding  $s^{[l]}$ = stride  $n_c^{[l]}$ = number of filters输入维度为：$n_W^{[l-1]} * n_H^{[l-1]} * n_c^{[l-1]}$每个滤波器组维度为：$f^{[l]} * f^{[l]} * n_c^{[l-1]}$权重维度为：$f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]}$偏置维度为：$1 * 1 * 1 * n_c^{[l]}$输出维度为：$n_H^{[l]} * n_W^{[l]} * n_c^{[l]}$其中，如果有m个样本，进行向量化运算，相应的输出维度为：$m * n_H^{[l]} * n_W^{[l]} * n_c^{[l]}$Simple Convolutional Network输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出ŷ 。随着CNN层数增加，$n_H^{[l]}和n_W^{[l]}一般逐渐减小，而n_c^{[l]}一般逐渐增大$。CNN有三种类型的layer：  Convolution层（CONV）卷积层  Pooling层（POOL）池化层  Fully connected层（FC）全连接层        池化层（Pooling layers）使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性Pooling layers的做法比convolution layers简单许多，没有卷积运算，仅仅是在滤波器算子滑动区域内取最大值，即max pooling，这是最常用的做法。如果是多个通道，那么就每个通道单独进行max pooling操作。除了max pooling之外，还有一种做法：average pooling。max pooling比average pooling更为常用。Max pooling的好处是只保留区域内的最大值（特征），忽略其它值，降低noise影响，提高模型健壮性。而且，max pooling需要的超参数仅为滤波器尺寸f和滤波器步进长度s，没有其他参数需要模型训练得到，计算量很小。        1x1 Convolutionsfilter的维度为1x1，可以用来缩减channel数目,获得filter个数的channel数，池化层压缩它的高度和宽度Inception network motivation代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层Inception Network使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块。提升性能的同时，计算量大。        由(28×28×32)x(5×5×192) =&gt; (28x28x16)x192+(28x28x32)x(5x5x16) 计算量减少了近90%Why CNN相比标准神经网络，CNN的优势之一就是参数数目要少得多。参数数目少的原因有两个：  参数共享：一个特征检测器（例如垂直边缘检测）对图片某块区域有用，同时也可能作用在图片其它区域。  连接的稀疏性：因为滤波器算子尺寸限制，每一层的每个输出只与输入部分区域内有关。除此之外，由于CNN参数数目较小，所需的训练样本就相对较少，从而一定程度上不容易发生过拟合现象。而且，CNN比较擅长捕捉区域位置偏移。也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。1D and 3D Generalizations            ]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> cnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NN优化策略]]></title>
      <url>/blog/2018/10/01/optimization-strategy</url>
      <content type="text"><![CDATA[正交化方法（Orthogonalization）每次只调试一个参数，保持其它参数不变，而得到的模型某一性能改变是一种最常用的调参策略。Orthogonalization的核心在于每次调试一个参数只会影响模型的某一个性能。对应到机器学习监督式学习模型中，可以大致分成四个独立的“功能”：  Fit training set well on cost function  Fit dev set well on cost function  Fit test set well on cost function  Performs well in real worldearly stopping在模型功能调试中并不推荐使用。因为early stopping在提升验证集性能的同时降低了训练集的性能。也就是说early stopping同时影响两个“功能”，不具有独立性、正交性。单一数字评估指标（Single number evaluation metric）构建、优化机器学习模型时，单值评价指标非常必要。有了量化的单值评价指标后，我们就能根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。例如F1 socre or 平均错误率满足和优化指标（Satisficing and optimizing metrics）有时候，要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的。解决办法是，我们可以把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行了。训练/开发/测试集划分（Train/dev/test distributions）Train/dev/test sets如何设置对机器学习的模型训练非常重要，合理设置能够大大提高模型训练效率和模型质量。应该尽量保证dev sets和test sets来源于同一分布且都反映了实际样本的情况。当样本数量不多（小于一万）的时候，通常将Train/dev/test sets的比例设为60%/20%/20%，在没有dev sets的情况下，Train/test sets的比例设为70%/30%。当样本数量很大（百万级别）的时候，通常将相应的比例设为98%/1%/1%或者99%/1%。对于dev sets数量的设置，应该遵循的准则是通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型。对于test sets数量的设置，应该遵循的准则是通过test sets能够反映出模型在实际中的表现。实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但是，条件允许的话，最好是有test sets，实现无偏估计。什么时候该改变开发/测试集和指标？（When to change dev/test sets and metrics）算法模型的评价标准有时候需要根据实际情况进行动态调整，不同的目的，不同的指标，不同的model。动态改变评价标准的情况是dev/test sets与实际使用的样本分布不一致。比如猫类识别样本图像分辨率差异对比 human-level performance &amp; ML model performance通过比较找出优化方向，实现优化。通常，我们把training error与human-level error之间的差值称为bias，也称作avoidable bias；把dev error与training error之间的差值称为variance。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。例如猫类识别的例子中，如果human-level error为1%，training error为8%，dev error为10%。由于training error与human-level error相差7%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小training error，即减小偏差bias。如果图片很模糊，肉眼也看不太清，human-level error提高到7.5%。这时，由于training error与human-level error只相差0.5%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小dev error，即方差variance。这是相对而言的。human-level performance定义较难，不同人可能选择的human-level performance基准是不同的，选择什么样的human-level error，有时候会影响bias和variance值的相对变化。一般来说，我们将表现最好的那一组。解决avoidable bias的常用方法包括：  Train bigger model  Train longer/better optimization algorithms: momentum, RMSprop, Adam  NN architecture/hyperparameters search解决variance的常用方法包括：  More data  Regularization: L2, dropout, data augmentation  NN architecture/hyperparameters search误差分析（Carrying out error analysis）error analysis可以同时评估多个影响模型性能的因素，通过各自在错误样本中所占的比例来判断其重要性。比例越大，影响越大，越应该花费时间和精力着重解决这一问题。这种error analysis让我们改进模型更加有针对性，从而提高效率。例如假阳性（false positives）和假阴性（false negatives）监督式学习中，训练样本有时候会出现输出y标注错误的情况，即incorrectly labeled examples。利用上节内容介绍的error analysis，统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例。根据该比例的大小，决定是否需要修正所有incorrectly labeled data，还是可以忽略。  如果你打算修正开发集上的部分数据，那么最好也对测试集做同样的修正以确保它们继续来自相同的分布。  同时检验算法判断正确和判断错误的样本，要检查算法出错的样本很容易，只需要看看那些样本是否需要修正，但还有可能有些样本算法判断正确，那些也需要修正。因为算法有可能因为运气好把某个东西判断对了，通常判断错的次数比判断正确的次数要少得多，检查正确的上要花的时间长得多，通常不这么做，但也是要考虑到的。  修正训练集中的标签其实相对没那么重要，你可能决定只修正开发集和测试集中的标签，因为它们通常比训练集小得多，你可能不想把所有额外的精力投入到修正大得多的训练集中的标签，所以这样其实是可以的。开发集和测试集来自同一分布非常重要，但如果你的训练集来自稍微不同的分布，通常这是一件很合理的事情。使用来自不同分布的数据，进行训练和测试（Training and testing on different distributions）面对train set与dev/test set分布不同的情况，有两种解决方法  将train set和dev/test set完全混合，然后在随机选择一部分作为train set，另一部分作为dev/test set。优点是实现train set和dev/test set分布一致。缺点dev/test set大部分来自非目标的分布（train set）,达不到验证效果,不建议使用。  将原来的train set和一部分dev/test set组合当成train set，剩下的dev/test set分别作为dev set和test set。dev/test set全部来自dev/test set，保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。数据分布不匹配时，偏差与方差的分析（Bias and Variance with mismatched data distributions）当你的训练集来自和开发集、测试集不同分布时，分析偏差和方差的方式可能不一样。差值的出现可能来自算法本身，也可能来自于样本分布不同。因此不能简单认为出现了Variance。从原来的train set中分割出一部分作为train-dev set，train-dev set不作为训练模型使用，而是与dev set一样用于验证。就有training error、training-dev error和dev error三种error，其中，training error与training-dev error的差值反映了variance；training-dev error与dev error的差值反映了data mismatch problem，即样本分布不一致。总结一下human-level error、training error、training-dev error、dev error以及test error之间的差值关系和反映的问题：处理数据不匹配问题（Addressing data mismatch）您的训练集来自和开发测试集不同的分布，如果错误分析显示你有一个数据不匹配的问题该怎么办。  Carry out manual error analysis to try to understand difference between training dev/test sets  Make training data more similar; or collect more data similar to dev/test sets尝试弄清楚开发集和训练集到底有什么不同，当你了解开发集误差的性质时，你就知道，开发集有可能跟训练集不同或者更难识别，那么你可以尝试把训练数据变得更像开发集一点。或者看看是否有办法收集更多看起来像开发集的数据作训练。为了让train set与dev/test set类似，我们可以使用人工数据合成的方法（artificial data synthesis），但你的学习算法可能会对合成的这一个小子集过拟合。面对全新ML应用尽快建立你的第一个简单模型，然后快速迭代。它可以是一个快速和粗糙的实现（quick and dirty implementation）初始系统的全部意义在于，有一个学习过的系统，有一个训练过的系统，让你确定偏差方差的范围，就可以知道下一步应该优先做什么，让你能够进行错误分析，可以观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向。迁移学习（Transfer learning）深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。迁移学习，重新训练权重系数，如果需要构建新模型的样本数量较少，那么可以像刚才所说的，只训练输出层的权重系数$W^{[L]},\ b^{[L]}$，保持其它层所有的权重系数$W^{[l]},\ b^{[l]}$不变。这种做法相对来说比较简单。如果样本数量足够多，那么也可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为毕竟样本对模型的影响最大。选择哪种方法通常由数据量决定。顺便提一下，如果重新训练所有权重系数，初始$W^{[l]},\ b^{[l]}$由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化$W^{[l]},\ b^{[l]}$的过程称为fine-tuning迁移学习之所以能这么做的原因是，神经网络浅层部分能够检测出许多图片固有特征，第一个训练好的神经网络已经帮我们实现如何提取图片有用特征了。 因此，即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型。迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。应用场合  Task A and B have the same input x.  You have a lot more data for Task A than Task B.  Low level features from A could be helpful for learning B.多任务学习（Multi-task learning）多任务学习是使用单个神经网络模型来实现多个任务。多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。值得一提的是，Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。应用场合  Training on a set of tasks that could benefit from having shared lower-level features.  Usually: Amount of data you have for each task is quite similar.  Can train a big enough neural network to do well on all the tasks.端到端的深度学习（end-to-end deep learning）以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。端到端深度学习做的是，你训练一个巨大的神经网络，直接学到了和之间的函数映射，直接绕过了其中很多步骤。如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。优点：  Let the data speak  Less hand-designing of components needed缺点：  May need large amount of data  Excludes potentially useful hand-designed]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NN优化 Hyperparameters 超参数]]></title>
      <url>/blog/2018/09/30/hyperparameter</url>
      <content type="text"><![CDATA[Hyperparameters Tuning常见超参数  $\alpha$：学习因子  $\beta$：动量梯度下降因子  $\beta_1,\beta_2,\varepsilon$：Adam算法参数  layers：神经网络层数  hidden units：各隐藏层神经元个数  active function：激活函数  learning rate decay：学习因子下降参数  mini-batch size：批量训练样本包含的样本个数参数采样  随机采样  放大表现较好的区域，再对此区域做更密集的随机采样，由粗到细的采样（coarse to fine sampling scheme）为超参数选择合适的范围对于某些超参数，可能需要非均匀随机采样（即非均匀刻度尺）。例如超参数αα\alpha，待调范围是[0.0001, 1]。如果使用均匀随机采样，那么有90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。这在实际应用中是不太好的，因为最佳的$\alpha$值可能主要分布在[0.0001, 0.1]之间，而[0.1, 1]范围内$\alpha$值效果并不好。因此我们更关注的是区间[0.0001, 0.1]，应该在这个区间内细分更多刻度。将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。一般解法是，如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，然后得到的采样值r，最后反推到线性区间，即$10^r$。$10^r$就是最终采样的超参数。m = np.log10(a)n = np.log10(b)r = np.random.rand()r = m + (n-m)*rr = np.power(10,r)经过调试选择完最佳的超参数并不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型。一般来说，对于非常复杂或者数据量很大的模型,一个模型进行训练，调试不同的超参数。也可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NN优化 gradient descent 算法]]></title>
      <url>/blog/2018/09/30/gradient-descent</url>
      <content type="text"><![CDATA[Mini-batch gradient descentBatch Gradient Descent所有m个样本,m很大，每次迭代都要对所有样本进行进行求和运算和矩阵运算，训练速度往往会很慢。Mini-batch Gradient Descent把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，每次在单一子集上进行神经网络训练，速度就会大大提高。先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。for  t=1,⋯,T  {    Forward Propagation     Compute CostFunction    Backward Propagation    W:=W−α⋅dW    b:=b−α⋅db}经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。振荡的原因是不同的mini-batch之间是有差异的  mini-batch size=m，即为Batch gradient descent      mini-batch size=1，即为Stachastic gradient descent    Batch gradient descent会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。  Stachastic gradient descent每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。  Mini-batch gradient descent每次前进速度较快，且振荡较小，基本能接近全局最小值。既能使用向量化优化算法，又能叫快速地找到最小值。m不太大时，例如$m\leq2000$，建议直接使用Batch gradient descent。如果总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。这些都是2的幂。之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。EVA指数加权平均（Exponentially weighted averages）,通过移动平均（moving average）的方法来对每天气温进行平滑处理。根据之前的推导公式，其一般形式为：$\beta$值决定了指数加权平均的天数，近似表示为：例如，当$\beta=0.9$，则$\frac{1}{1-\beta}=10$，表示将前10天进行指数加权平均。$\beta$值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移。Gradient descent with momentum动量梯度下降算法，其速度要比传统的梯度下降算法快很多。每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重W和常数项bRMSprop加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降。因此，表达式中Sb较大，而SW较小。AdamAdam（Adaptive Moment Estimation）算法结合了动量梯度下降算法和RMSprop算法。Learning rate decay随着迭代次数增加，学习因子α逐渐减小,有效提高神经网络训练速度Learning rate decay中对$\alpha$可由下列公式得到：其中，deacy_rate是参数（可调），epoch是训练完所有样本的次数。随着epoch增加，$\alpha$会不断变小。局部最优解（local optima）在使用梯度下降算法不断减小cost function时，可能会得到局部最优解（local optima）而不是全局最优解（global optima）。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima的概念发生了变化。准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为saddle point。类似马鞍状的plateaus会降低神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。                  只要选择合理的强大的神经网络，一般不太可能陷入local optima  Plateaus可能会使梯度下降变慢，降低学习速度  动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，大大提高神经网络的学习速度。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NN优化base model]]></title>
      <url>/blog/2018/09/29/base-model</url>
      <content type="text"><![CDATA[Train/Dev/Test sets一般地，我们将所有的样本数据分成三个部分：  Train sets用来训练你的算法模型；  Dev sets用来验证不同算法的表现情况，从中选择最好的算法模型；  Test sets用来测试最好算法的实际表现，作为该算法的无偏估计。设置合适的Train/Dev/Test sets数量，能有效提高训练效率。  数据集规模相对较小，适用传统的划分比例，如60%训练，20%验证和20%测试集  数据集规模较大的，验证集和测试集要小于数据总量的20%或10%。因为Dev sets的目标是用来比较验证不同算法的优劣，从而选择更好的算法模型就行了。因此，通常不需要所有样本的20%这么多的数据来进行验证。对于100万的样本，往往只需要1万个样本来做验证就够了。Test sets也是一样，目标是测试已选算法的实际表现，无偏估计。对于100万的样本，往往也只需要1万个样本就够了。因此，对于大数据样本，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。训练样本和测试样本来自于不同的分布，解决这一问题的比较科学的办法是尽量保证Dev sets和Test sets来自于同一分布。值得一提的是，训练样本非常重要，通常我们可以将现有的训练样本做一些处理，例如图片的翻转、假如随机噪声等，来扩大训练样本的数量，从而让该模型更加强大。即使Train sets和Dev/Test sets不来自同一分布，使用这些技巧也能提高模型性能。最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然，如果你不需要无偏估计，那就再好不过了。偏差，方差（Bias /Variance）Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型，两者可以区分对待，而不用权衡。通过两个数值Train set error和Dev set error来理解bias和variance  首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络          减少bias                  增加隐藏层数          增加神经元个数          更多时间来训练网络          选择其它更复杂的NN模型                      一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方差，我们要查看验证集性能，我们能从一个性能理想的训练集推断出验证集的性能是否也理想。          减少variance                  增加训练样本数据          正则化来减少过拟合                      只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。正则化（Regularization）L2L2正则比较常用，L1的在微分求导方面比较复杂。λ就是正则化参数，我们通常使用验证集或交叉验证集来配置这个参数，尝试各种各样的数据，寻找最好的参数，我们要考虑训练集之间的权衡，把参数设置为较小值，这样可以避免过拟合，所以λ是另外一个需要调整的超级参数其中，$(1-\alpha\frac{\lambda}{m})&lt;1$。当$\lambda$足够大，权重矩阵接近于0的值，相当于减轻了这些隐藏单元对网络的影响，简化网络。设激活函数为tanh: 当W减少=&gt;z减少=&gt;激活函数趋于线性=&gt;网络趋于线性l1_regularizer l1_l2_regularizer l2_regularizerscale = 0.001my_dense_layer = partial(    tf.layers.dense, activation=tf.nn.relu,    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))with tf.name_scope("dnn"):    hidden1 = my_dense_layer(X, n_hidden1, name="hidden1")    hidden2 = my_dense_layer(hidden1, n_hidden2, name="hidden2")    logits = my_dense_layer(hidden2, n_outputs, activation=None,                            name="outputs")必须将正则化损失加到基本损失with tf.name_scope("loss"):                                     # not shown in the book    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown        labels=y, logits=logits)                                # not shown    base_loss = tf.reduce_mean(xentropy, name="avg_xentropy")   # not shown    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)    loss = tf.add_n([base_loss] + reg_losses, name="loss")dropout每层的神经元，按照一定的概率将其暂时从网络中丢弃 =&gt; 每一层都有部分神经元不工作,简化网络。主要用于计算机视觉。计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据。因为我们通常没有足够的数据，所以一直存在过拟合。概率为0.5        import numpy as npkeep_prob = 0.8a = np.random.rand(2, 2)# 保留任意一个隐藏单元的概率为keep_probb = np.random.rand(a.shape[0], a.shape[1]) &lt; keep_prob# c为保留下的矩阵c = a * b# inverted dropout(反向随机失活)c /= keep_probtraining = tf.placeholder_with_default(False, shape=(), name='training')dropout_rate = 0.5  # == 1 - keep_probX_drop = tf.layers.dropout(X, dropout_rate, training=training)with tf.name_scope("dnn"):    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,                              name="hidden1")    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,                              name="hidden2")    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)    logits = tf.layers.dense(hidden2_drop, n_outputs, name="outputs")反向随机失活units失去(1-keep_prob)，Wa也会减少(1-keep_prob)，为了不影响z的期望值，所以Wa/keep_prob修正或弥补我们所需的那(1-keep_prob)训练过程  对于m个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。  每层的keep_prob可以不同，因为每层units数量不一定相同。或者一些层用dropout，有些不同。  值得注意的是，使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout和随机删减神经元，所有的神经元都在工作。缺点代价函数不再被明确。通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。其他正则化手段数据扩增对已有的训练样本进行一些处理来“制造”出更多的样本，称为data augmentation。例如图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。数字识别中，也可以将原有的数字图片进行任意旋转或者扭曲，或者增加一些noise。early stopping随着迭代训练次数增加，train set error一般是单调减小的。而dev set error 先减小，之后又增大。发生了过拟合。选择合适的迭代次数，即early stoppingsummaryget min cost function 和 防止overfit 是对立的。early stopping 通过减少训练防止过拟合，这样J不会足够小。在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。early stopping做到同时优化，但可能没有“分而治之”的效果好。L2 regularization可以实现“分而治之”的效果：迭代训练足够多，减小J，而且也能有效防止过拟合。而L2 regularization的缺点之一是最优的正则化参数$\lambda$的选择比较复杂。对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。Normalizing input标准化输入可以提高训练神经网络的速度。如果特征之间取值范围差异过大，只能选择很小的学习因子α，多次迭代，来避免J发生振荡。使得数据均值为0，方差为1,原始数据减去其均值μ后，再除以其方差$\sigma^2$。注意保证了训练集合测试集的标准化操作一致，用同一个μ和$\sigma^2$，由训练集数据计算得来标准化后代价函数优化起来更简单快速。可用较大步长，较小次数迭代。梯度消失/梯度爆炸（Vanishing / Exploding gradients）基于反向传播随机梯度下降来训练深度网络，不同的层学习的速度差异很大，因为学习速率= 激活值*残差,而残差是从上层的残差加权得到的,也与激活函数有关。极深的网络存在的问题  各层权重W的元素都稍大于1，1.5，L越大，Ŷ呈指数型增长。我们称之为数值爆炸。  各层权重W的元素都稍小于1，0.5,L越大，Ŷ呈指数型减小。我们称之为数值消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。L非常大时，例如L=150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。完善w初始化单个unit        这里忽略了常数项b。为了让z不会过大或者过小，思路是让w与n有关，且n越大，w应该越小才好。这样能够保证z不会过大。一种方法是在设$Var(w_i)\;=\;\frac1n$，n表示神经元的输入特征数量。w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(1/n[l-1])tanh，一般选择上面的初始化方法ReLU，权重w的初始化一般令其方差为$\frac2n$Xavier and He InitializationW1 = tf.get_variable('W1',[25,12288],initializer=tf.contrib.layers.xavier_initializer(seed=1))b1 = tf.get_variable('b1',[25,1],initializer=tf.zeros_initializer())W2 = tf.get_variable('W2',[12,25],initializer=tf.contrib.layers.xavier_initializer(seed=1))b2 = tf.get_variable('b2',[12,1],initializer=tf.zeros_initializer())W3 = tf.get_variable('W3',[6,12],initializer=tf.contrib.layers.xavier_initializer(seed=1))b3 = tf.get_variable('b3',[6,1],initializer=tf.zeros_initializer())默认是 glorot_uniform_initializer某程度上和xavier_initializer差不多he_init = tf.contrib.layers.variance_scaling_initializer()hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,                          kernel_initializer=he_init, name="hidden1")激活函数详细Gradient checkingBack Propagation神经网络有一项重要的测试是梯度检查（gradient checking）。其目的是检查验证反向传播过程中梯度下降算法是否正确。近似求出梯度值        梯度检查首先要做的是分别将$W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]}$这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量$\theta$。这样cost function $J(W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]})$就可以表示成$J(\theta)$。然后将反向传播过程通过梯度下降算法得到的$dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[L]}$按照一样的顺序构造成一个一维向量$d\theta$。$d\theta$的维度与$\theta$一致。接着利用$J(\theta)$对每个$\theta_i$计算近似梯度，其值与反向传播算法得到的$d\theta_i$相比较，检查是否一致。例如，对于第i个元素，近似梯度为：计算完所有$\theta_i$的近似梯度后，可以计算$d\theta_{approx}$与$d\theta$的欧氏（Euclidean）距离来比较二者的相似度。公式如下：一般来说，如果欧氏距离越小，例如$10^{-7}$，甚至更小，则表明$d\theta_{approx}$与$d\theta$越接近，即反向梯度计算是正确的，没有bugs。如果欧氏距离较大，例如$10^{-5}$，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。如果欧氏距离很大，例如$10^{-3}$，甚至更大，则表明$d\theta_{approx}$与$d\theta$差别很大，梯度下降计算过程有bugs，需要仔细检查。实施梯度检验的实用技巧和注意  不要在训练中使用梯度检验，只用于调试。  如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。  注意不要忽略正则化项，计算近似梯度的时候要包括进去。  梯度检查时关闭dropout，检查完毕后再打开dropout。  随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）Batch Normalization尽管使用 He初始化和 ELU（或任何 ReLU 变体）可以显著减少训练开始阶段的梯度消失/爆炸问题，但不保证在训练期间问题不会回来。Batch Normalization不仅可以让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。Normalizing input只是对输入进行了处理，Batch Normalization各隐藏层的输入进行标准化处理。第l层隐藏层的输入就是第l-1层隐藏层的输出$A^{[l-1]}$。对$A^{[l-1]}$进行标准化处理，从原理上来说可以提高$W^{[l]}$和$b^{[l]}$的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。值得注意的是，实际应用中，一般是对$Z^{[l-1]}$进行标准化处理而不是$A^{[l-1]}$，其实差别不是很大。  Normalizing inputs和Batch Normalization有区别的,Normalizing inputs使所有输入的均值为0，方差为1。而Batch Normalization可使各隐藏层输入的均值和方差为任意值。实际上，从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。这也解释了为什么需要用γ和β来对zl作进一步处理。如果实际应用的样本与训练样本分布不同，即发生了covariate shift，则一般是要对模型重新进行训练的。深度神经网络中，covariate shift会导致模型预测效果变差。而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强。Batch Norm减少了各层$W^{[l]}、B^{[l]}$之间的耦合性，让各层更加独立，实现自我训练学习的效果。也就是说，如果输入发生covariate shift，那么因为Batch Norm的作用，对个隐藏层输出$Z^{[l]}$进行均值和方差的归一化处理，$W^{[l]}和B^{[l]}$更加稳定，使得原来的模型也有不错的表现。从另一个方面来说，Batch Norm也起到轻微的正则化（regularization）效果。具体表现在：  每个mini-batch都进行均值为0，方差为1的归一化操作  每个mini-batch中，对各个隐藏层的$Z^{[l]}$添加了随机噪声，效果类似于Dropout  mini-batch越小，正则化效果越明显但是，Batch Norm的正则化效果比较微弱，正则化也不是Batch Norm的主要功能。Softmax 多分类目前我们介绍的都是二分类问题，神经网络输出层只有一个神经元，表示预测输出$\hat y$是正类的概率$P(y=1|x)，$\hat y&gt;0.5$则判断为正类，$\hat y&lt;0.5$则判断为负类。对于多分类问题，用C表示种类个数，神经网络中输出层就有C个神经元，即$Cn^{[L]}=C$。其中，每个神经元的输出依次对应属于该类的概率，即$P(y=c|x)$。为了处理多分类问题，我们一般使用Softmax回归模型。Softmax回归模型输出层的激活函数Softmax回归模型输出层的激活函数如下所示：输出层每个神经元的输出$a^{[L]}_i$对应属于该类的概率，满足：所有的$a^{[L]}_i，即\hat y$，维度为(C, 1)。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Deep Neural Network 深层神经网络]]></title>
      <url>/blog/2018/09/29/Deep-Neural-Network</url>
      <content type="text"><![CDATA[结构L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层传播正向传播过程$A^{\lbrack0\rbrack}$ 就是X。 只使用一次for 循环 从后往前层数l 1到L反向传播矩阵维度做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率        why deep不需要很大的神经网络，但是得有有深度。网络描述  大小 隐藏单元的数量  深度 隐藏层数量原因  随着深度增加，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。  减少神经元个数，从而减少计算量。使用浅层网络达到深层网络效果需要指数增长隐藏单元的数量。对实际问题进行建模时，尽量先选择层数少的神经网络模型，对于比较复杂的问题，再使用较深的神经网络模型Parameters vs Hyperparameters神经网络中的参数就是我们熟悉的$W^{[l]}和b^{[l]}$。而超参数则是例如学习速率$\alpha$，训练迭代次数N，神经网络层数L，各层神经元个数$n^{[l]}$，激活函数g(z)等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}和b^{[l]}$的值。how 最优参数经验 or 不停的尝试不同的值测试选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法即使用了很久的模型，可能你在做网络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变化，因为电脑的基础设施，CPU或是GPU可能会变化很大。所以有一条经验规律可能每几个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。优化model优化]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> dnn </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[pytrick or gist]]></title>
      <url>/blog/2018/09/28/pytrick</url>
      <content type="text"><![CDATA[区分一维数组和 行变量 列变量一维数组 行为 不总与行变量or列变量一直，造成不必要的bug。总是使用nx1维矩阵（基本上是列向量），或者1xn维矩阵（基本上是行向量），这样你可以减少很多assert语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维数时，不要羞于reshape操作。np.random.randint(5)a = np.random.randn(5)&gt;&gt;&gt; array([-1.05118915, -0.45812568,  0.28528316,  0.75364257,  1.23234449])&gt;&gt;&gt; a.shape(5,)b = a.reshape(1,-1)&gt;&gt;&gt; array([[-1.05118915, -0.45812568,  0.28528316,  0.75364257,  1.23234449]])&gt;&gt;&gt; b.shape(1, 5)sklearntrain_test_splitimport numpy as npdef split_train_test(data, test_ratio):    shuffled_indices = np.random.permutation(len(data))    test_set_size = int(len(data) * test_ratio)    test_indices = shuffled_indices[:test_set_size]    train_indices = shuffled_indices[test_set_size:]    return data.iloc[train_indices], data.iloc[test_indices]sklearnfrom sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)clean处理文本和类别属性 文本标签转换为数字# 这种做法的问题是，ML 算法会认为两个临近的值比两个疏远的值要更相似from sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()housing_cat = housing["ocean_proximity"]housing_cat_encoded = encoder.fit_transform(housing_cat)&gt;&gt;&gt; print(encoder.classes_)['&lt;1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']# 常见的方法是给每个分类创建一个二元属性 独热编码（One-Hot Encoding），因为只有一个属性会等于 1（热），其余会是 0（冷）"""注意输出结果是一个 SciPy 稀疏矩阵，而不是 NumPy 数组。当类别属性有数千个分类时，这样非常有用。经过独热编码，我们得到了一个有数千列的矩阵，这个矩阵每行只有一个 1，其余都是 0。使用大量内存来存储这些 0 非常浪费，所以稀疏矩阵只存储非零元素的位置。"""from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))使用类LabelBinarizer，我们可以用一步执行这两个转换（从文本分类到整数分类LabelEncoder，再从整数分类到独热向量OneHotEncoder）&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer&gt;&gt;&gt; encoder = LabelBinarizer()&gt;&gt;&gt; housing_cat_1hot = encoder.fit_transform(housing_cat)&gt;&gt;&gt; housing_cat_1hotarray([[0, 1, 0, 0, 0],       [0, 1, 0, 0, 0],       [0, 0, 0, 0, 1],       ...,       [0, 1, 0, 0, 0],       [1, 0, 0, 0, 0],       [0, 0, 0, 1, 0]])pandas&gt;&gt;&gt; labels, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])&gt;&gt;&gt; labelsarray([0, 0, 1, 2, 0])&gt;&gt;&gt; uniquesarray(['b', 'a', 'c'], dtype=object)PipelineSequentially apply a list of transforms and a final estimator.Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods.The final estimator only needs to implement fit.当你调用流水线的fit()方法，就会对所有转换器顺序调用fit_transform()方法，将每次调用的输出作为参数传递给下一个调用，一直到最后一个估计器，它只执行fit()方法。from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([        ('imputer', Imputer(strategy="median")),        ('attribs_adder', CombinedAttributesAdder()),        ('std_scaler', StandardScaler()),        ])housing_num_tr = num_pipeline.fit_transform(housing_num)Pipeline 合并 并行执行，等待输出，然后将输出合并起来，并返回结果from sklearn.pipeline import FeatureUnionnum_attribs = list(housing_num)cat_attribs = ["ocean_proximity"]num_pipeline = Pipeline([        ('selector', DataFrameSelector(num_attribs)),        ('imputer', Imputer(strategy="median")),        ('attribs_adder', CombinedAttributesAdder()),        ('std_scaler', StandardScaler()),    ])cat_pipeline = Pipeline([        ('selector', DataFrameSelector(cat_attribs)),        ('label_binarizer', LabelBinarizer()),    ])full_pipeline = FeatureUnion(transformer_list=[        ("num_pipeline", num_pipeline),        ("cat_pipeline", cat_pipeline),    ])评估# mse rmsefrom sklearn.metrics import mean_squared_errormse = mean_squared_error(y_true, y_pred)rmse = np.sqrt(mse)# 混淆矩阵from sklearn.model_selection import cross_val_predicty_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)from sklearn.metrics import confusion_matrixconfusion_matrix(y_train_5, y_train_pred)array([[53272, 1307],        [ 1077, 4344]])tn,fpfn,tp# 准确率与召回率&gt;&gt;&gt; from sklearn.metrics import precision_score, recall_score&gt;&gt;&gt; precision_score(y_train_5, y_pred) # == 4344 / (4344 + 1307)0.76871350203503808&gt;&gt;&gt; recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077)0.79136690647482011# F1 值&gt;&gt;&gt; from sklearn.metrics import f1_score&gt;&gt;&gt; f1_score(y_train_5, y_pred)0.78468208092485547决策分数 阈值&gt;&gt;&gt; y_scores = sgd_clf.decision_function([some_digit])&gt;&gt;&gt; y_scoresarray([ 161855.74572176])&gt;&gt;&gt; threshold = 0&gt;&gt;&gt; y_some_digit_pred = (y_scores &gt; threshold)array([ True], dtype=bool)画图from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")    plt.xlabel("Threshold")    plt.legend(loc="upper left")    plt.ylim([0, 1])plot_precision_recall_vs_threshold(precisions, recalls, thresholds)plt.show()save/loadfrom sklearn.externals import joblibjoblib.dump(my_model, "my_model.pkl")my_model_loaded = joblib.load("my_model.pkl")交叉验证from sklearn.model_selection import cross_val_score# K-fold cross-validationscores = cross_val_score(reg, X, y,scoring="neg_mean_squared_error", cv=10)rmse_scores = np.sqrt(-scores)超参数微调GridSearchCVfrom sklearn.model_selection import GridSearchCVparam_grid = [    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},  ]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg, param_grid, cv=5,                           scoring='neg_mean_squared_error')grid_search.fit(housing_prepared, housing_labels)网格搜索会探索12 + 6 = 18种RandomForestRegressor的超参数组合，会训练每个模型五次（因为用的是五折交叉验证）。换句话说，训练总共有18 × 5 = 90轮！K 折将要花费大量时间，完成后，你就能获得参数的最佳组合.&gt;&gt;&gt; grid_search.best_params_{'max_features': 6, 'n_estimators': 30}&gt;&gt;&gt; grid_search.best_estimator_RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,           max_features=6, max_leaf_nodes=None, min_samples_leaf=1,           min_samples_split=2, min_weight_fraction_leaf=0.0,           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,           verbose=0, warm_start=False)&gt;&gt;&gt; cvres = grid_search.cv_results_... for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):...     print(np.sqrt(-mean_score), params)...52172.0292957 {'max_features': 6, 'n_estimators': 10}49958.9555932 {'max_features': 6, 'n_estimators': 30}59122.260006 {'max_features': 8, 'n_estimators': 3}52441.5896087 {'max_features': 8, 'n_estimators': 10}50041.4899416 {'max_features': 8, 'n_estimators': 30}62371.1221202 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}54572.2557534 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}59634.0533132 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}52456.0883904 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}随机搜索当超参数的搜索空间很大时，最好使用RandomizedSearchCV通过选择每个超参数的一个随机值的特定数量的随机组合。这个方法有两个优点：  如果你让随机搜索运行，比如 1000 次，它会探索每个超参数的 1000 个不同的值（而不是像网格搜索那样，只搜索每个超参数的几个值）。  你可以方便地通过设定搜索次数，控制超参数搜索的计算量。feature selectfeature_importances = grid_search.best_estimator_.feature_importances_&gt;&gt;&gt; sorted(zip(feature_importances,attributes), reverse=True)[(0.32649798665134971, 'median_income'), (0.15334491760305854, 'INLAND'), (0.11305529021187399, 'pop_per_hhold'), (0.07793247662544775, 'bedrooms_per_room'), (0.071415642259275158, 'longitude'), (0.067613918945568688, 'latitude'), (0.060436577499703222, 'rooms_per_hhold'), (0.04442608939578685, 'housing_median_age'), (0.018240254462909437, 'population'), (0.01663085833886218, 'total_rooms'), (0.016607686091288865, 'total_bedrooms'), (0.016345876147580776, 'households'), (0.011216644219017424, '&lt;1H OCEAN'), (0.0034668118081117387, 'NEAR OCEAN'), (0.0026848388432755429, 'NEAR BAY'), (8.4130896890070617e-05, 'ISLAND')]学习曲线from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitdef plot_learning_curves(model, X, y):    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)    train_errors, val_errors = [], []    for m in range(1, len(X_train)):        model.fit(X_train[:m], y_train[:m])        y_train_predict = model.predict(X_train[:m])        y_val_predict = model.predict(X_val)        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))        val_errors.append(mean_squared_error(y_val_predict, y_val))plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")pandassummarypd compare sqlpd index slice etcplothousing.plot(kind="scatter", x="median_income",y="median_house_value",alpha=0.1)关系corr_matrix = housing.corr()corr_matrix["median_house_value"].sort_values(ascending=False)pd.value_counts() .info() scatter_plot .head() .describe()数据清洗用DataFrame的dropna()，drop()，和fillna()方法，可以方便地实现housing.dropna(subset=["total_bedrooms"])    # 选项1housing.drop("total_bedrooms", axis=1)       # 选项2median = housing["total_bedrooms"].median()housing["total_bedrooms"].fillna(median)     # 选项3scipyscipy.misc.imread(img_name)scipy.misc.imresize(, (32, 32))应用df = pd.DataFrame(depth, columns=['price', 'amount'])if not is_bids:    # asks 保留grad位+1    df['price'] = df['price'].apply(lambda x: math.ceil(x / g) * g)df['price'] = df['price'].apply(cut, grad=grad)ts = df.groupby('price', sort=False).agg({    'amount': 'sum',})# if is_bids:#     ts.sort_index(ascending=False, inplace=True)# else:#     ts.sort_index(inplace=True)ts.reset_index(inplace=True)ts['amount'] = np.round(ts['amount'], 8)depth_list = ts.values.tolist()df = pd.DataFrame(data).set_index("date")df["price"] = df["price"].astype("float64")all_list = filter(lambda x: x &lt; self.support_min_period, const.MINUTE.ALL)for m in all_list:    self.bar_table_name = self.table_name_format % m    resample_data = df.resample(const.PDMINUTE.NAME_DICT[m]).apply({"price": "ohlc", "amount": "sum"})    # ohlc fillna    resample_data.columns = resample_data.columns.droplevel()    resample_data["date"] = resample_data.index    resample_data['timestamp'] = resample_data['date'].apply(util.dt2ts)    resample_data['status'] = const.KLINE_STATUS.TRADE    resample_data = resample_data.apply(        lambda x: x.fillna(resample_data['close'].fillna(method='ffill'))    )    # sum fill0    resample_data.rename(columns={"amount": "volume"}, inplace=True)    # resample_data["volume"] = resample_data['volume'].replace(to_replace=0, method="ffill")    #    resample_data = self._poloniex_5_1(m, resample_data)    rows = resample_data.to_dict('records')df['timestamp'] = df['date'].apply(cls.dt2ms)df['timestamp'] = df['timestamp'].astype('int64')df.drop('date', axis=1, inplace=True)df.sort_values('timestamp', inplace=True)rows = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].values.tolist()df = pd.DataFrame(data).sort_values(by="date").set_index("date")# ohlc fillnaresample_data["date"] = resample_data.indexresample_data['timestamp'] = resample_data['date'].apply(util.dt2ts)resample_data['status'] = const.KLINE_STATUS.MERGEclose = resample_data['close'].fillna(method='ffill')resample_data = resample_data.apply(lambda x: x.fillna(close))# sum fill0# resample_data["volume"] = resample_data['volume'].replace(to_replace=0, method="ffill")rows = resample_data.to_dict('records')]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Shallow Neural Network 浅层神经网络]]></title>
      <url>/blog/2018/09/28/Shallow-Neural-Network</url>
      <content type="text"><![CDATA[结构结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。输入层和输出层，顾名思义，对应着训练样本的输入和输出，很好理解。隐藏层是抽象的非线性的中间层，这也是其被命名为隐藏层的原因。之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因计算正向传播        激活函数 Activation functionswhy Activation functions  用来加入非线性因素的，因为线性模型的表达能力不够，解决线性模型所不能解决的问题。假设所有的激活函数都是线性的，使用神经网络与直接使用线性模型的效果并没有什么两样。神经网络就没有任何作用。如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。Activation functions 种类对比神经网络隐藏层和输出层都需要激活函数sigmoid不多用 梯度爆炸/消失 涉及除法和指数计算量大why tanh better than sigmoidtanh函数在所有场合都优于sigmoid函数二分类的问题上，隐藏层使用tanh激活函数，输出层使用sigmoid函数因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为[0,+1]，所以一般会选择sigmoid作为激活函数。sigmoid函数和tanh函数两者共同的缺点在x特别大或者特别小 即 |x|很大 的情况下，激活函数的斜率（梯度）很小，最后就会接近于0，导致降低梯度下降的速度  所以输入层使|x|尽可能限定在零值附近，从而提高梯度下降算法运算速度 使用Normalizing inputReLU激活函数sigmoid函数和tanh函数的这个缺陷ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。summary  实践中隐藏层使用ReLU作为激活函数，比其他学习速度快。  sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，  而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象。  (同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)      在ReLu的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。    sigmoid激活函数：除了输出层是一个二分类问题基本不会用它。  tanh激活函数：tanh是非常优秀的，几乎适合所有场合。  ReLu激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用ReLu图像对比            激活函数导数权重初始化why not all 0权重每次迭代更新都会得到完全相同的结果,都是相等，这样隐藏层设置多个神经元就没有任何意义了。参数b可以全部初始化为零，并不会影响神经网络训练效果随机尽量小把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）使用高斯分布初始化权重np.random.randn()*0.01乘以0.01的目的是尽量使得权重W初始化比较小的值,因为如果使用sigmoidortanh函数作为激活函数的话，W比较小，得到的|x|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果W较大，得到的|x|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn基础 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Mac 问题]]></title>
      <url>/blog/2018/09/27/fix-mac</url>
      <content type="text"><![CDATA[重置蓝牙  按住 Option 和 Shift 键，点击状态栏蓝牙图标。  选择 调试 、 还原蓝牙模块 选择确定进行初始化。PRAM  关机，拔掉所有外设，接上电源。  启动时同时按住 Command, Option, p, r ， 听到三次 dang 的开机声音后放开。  启动电脑SMC 重置  关机，拔掉所有外设，接上电源。  同时按住 Shift, Control, Option, 电源键 ，此时电脑没有任何反应，等待十秒放开。  继续等待十秒，启动电脑。]]></content>
      <categories>
        
          <category> 生活 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[github pages 自定义域名]]></title>
      <url>/blog/2018/09/26/%E5%9F%9F%E5%90%8D</url>
      <content type="text"><![CDATA[大写加粗 的免费🤩下面正文开始申请域名cloudns 什么！？备案？身份验证？不存在的兴冲冲配置CNAME记录大概像这样www      CNAME           username.github.iogithub 上做对应配置其实就是到仓库的settings设置github会自动添加CNAME文件到你的code异常方便然后就发现cloudns的ssl证书要钱🤑SSL域名解析cloudflare填上在cloudns申请的域名，cloudflare会自动同步DNS记录，按照提示修改cloudnsNS记录，然后激活site,等它变绿😏，它的ssl证书要等一段时间起效，我睡个午觉就好了done后记好些绑定域名的第三方服务也要重新搞过，记得清理cloudflare上的缓存，让一些配置起效。这里也很蛋疼。]]></content>
      <categories>
        
          <category> 生活 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ML细化]]></title>
      <url>/blog/2018/09/25/ML%E7%BB%86%E5%8C%96</url>
      <content type="text"><![CDATA[项目概览  明确目标如何划定问题，要选择什么算法，评估模型性能的指标是什么划定问题：监督或非监督，还是强化学习？这是个分类任务、回归任务，还是其它的？要使用批量学习还是线上学习？  选择性能指标  核实假设Get Data。。。。。。spiderData Explore了解data概况  data structure      pd.value_counts() .info() scatter_plot .head() .describe()        可视化数据的分布，趋势，离散,或者相关度，分类的话，填好color and label  明确目的，根据以上进一步假设  EDA Exploratory Data AnalysisVariable Identification识别预测变量（输入）和目标（输出）变量。接下来，确定变量的数据类型和类别。Univariate Analysis逐个探索变量。执行单变量分析的方法将取决于变量类型是分类还是连续。  连续变量 在连续变量的情况下，我们需要了解变量的集中趋势和扩散。  分类变量 使用频率表来了解每个类别的分布。我们还可以读作每个类别下的值的百分比。可以使用两个指标（每个类别的计数和计数％）来衡量它。条形图可用作可视化。Bi-variate Analysis双变量分析找出两个变量之间的关系。  连续和连续 两个连续变量之间进行双变量分析时，我们应该看散点图。散点图的模式表明变量之间的关系。关系可以是线性的或非线性的。为了找到关系的强度，我们使用corr()相关系数  分类和分类Data Preprocess  data nan or noise,异常值 =&gt; Techniques of Outlier Detection and Treatment  类别特征（categorical features） =&gt; numerical features (各种 encode one-hot pd.get_dummies)  如果取值太多 只用取出现频率top n，其他归一类再做encode, 或者这个feature的重要性 干脆不要  在预测建模中处理分类变量的简单方法Feature select  凭经验觉得有用的 🤣  非线性变换获得更多feature  Feature Engineering  PCA ,Random Forest ,XGBoost ,GDBT  归一化  标准化  文本特征：文本长度、Word Embeddings、TF-IDF、LDA、LSI等，深度学习提取featuremodel select  Gradient Boosting—— GBDT (Xgboost)  Random Forest  SVM  Linear  Logistic Regression  Neural Networks时间model  ma  ARIMA  Markov Model  LSTM调参防止过拟合对结果不要想当然，熟悉model参数，确定小范围再grid search做好训练记录，保存参数，方便重现Cross Validation改进 Feature 或 Model 通常 K-Fold CV K=5Ensemble选择好的feature 再ensembleget a strong modelkaggle-ensembling-guideCost Function &amp; Evaluation Fuction  MSE/RMSE  L1_Loss(误差接近0的时候不平滑)/L2_Loss  Hinge_Loss/Margin_Loss  Cross-entropy_Loss/Sigmoid_Cross-ntropy_Loss/Softmax_Cross-entropy_Loss  Log_Loss  ROC/AUC]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[向量]]></title>
      <url>/blog/2018/09/20/%E5%90%91%E9%87%8F</url>
      <content type="text"><![CDATA[点乘内积、数量积 ps:怎么多个别名有意思？还有叫点积 😡  a·b&gt;0    方向基本相同，夹角在0°到90°之间  a·b=0    正交，相互垂直  a·b&lt;0    方向基本相反，夹角在90°到180°之间法线 法向量法线是垂直于该平面的三维向量基坐标轴 正交基的数量少于向量本身的维数,达到降维效果选择最优基第一维度最大程度保留原有的信息,希望投影后的投影值尽可能分散,不会重叠一起，离散程度用方差  二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大，方差最大的方向下一个投影方向如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是几乎重合在一起，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们希望它们之间不存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。相关性用协方差表示]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 几何 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[高频挑战]]></title>
      <url>/blog/2018/09/13/%E9%AB%98%E9%A2%91%E6%8C%91%E6%88%98</url>
      <content type="text"><![CDATA[执行算法 高频策略进入市场确认 风险检查增加的步骤带来更多的延时HFT HFT High Frequency Trading做市商  对市场有精确实时估计以合适价格完成订单  快速撤单  低买，速度极快并重复以稍高的价格卖出套利    不是严格无风险，套利机会可能瞬间 只能完成部分交易风险控制容易计算的风险争论承担风险才有的收益 不叫不公平在其他拥有相同想法的人之前获得头寸 保持获利退出头寸  老鼠仓利用客户订单信息 在客户之前进行相同的操作  塞单高速率建单或者撤单  有益市场活跃交易者高频特性  超高速的复杂计算机系统下单  直连交易所的数据通道  平均每次持仓时间极短  大量发送和取消委托订单  收盘时基本保持平仓(不持仓过夜)低延迟比较合理的测试仍然是在主机端做记录，测试从收到市场数据(tick)的 TCP/UDP 包到发送交易指令(trade)包的时差。目前(2014 年)的情况是，这个延迟如果平均控制在个位数字微秒级就是顶级了业务逻辑部分其实相当简单。做这种高速交易肯定不会有什么凸优化、解微分方程之类复杂的运算，核心的部分一般就是加加 减减，比比大小什么的。业务逻辑本身的处理完全可以做到纳秒级高频策略  被动做市(Passive Market Making)  HFT 做市商  这种策略是在交易所挂限价单进行双边交易以提供流动性。所谓双边交易，是指做 市商手中持有一定存货，然后同时进行买和卖两方交易。这种策略的收入包括买卖价差(spread)和交易所提供的返佣 (rebate)两部分。  请注意，这种业务里做市商不是很需要预测市场走势的能力，只要能做到不赔钱就可以赚进交易所的酬劳。这个时候关键点来 了，就是如何做到不赔钱?一个是按照上面说过的，做好对冲;另一个就是发现形势不利的时候要能及时撤单——这个是最考 验低延迟的地方，速度慢就会发生来不及撤单而遭受损失的情况。  交易所对做市商提供的报酬可以包括两个部分。第一是每笔交易的返佣(rebate)。返佣的数值一般很小(远远小于 tick size)，但如果交易笔数巨大，积少成多，便可以成为不菲的收入。通过赚取返佣，做市商只需要保证每笔交易不赔即可，并 非一定要追求低买高卖，反而是要保证自己的委托单能尽可能多地被执行，以争取更大的流量。为了做到这一点，下单和改单 的速度是个关键，这也是为什么这一行如今被以速度见长的高频交易商把持的原因。第二是固定的佣金。这是比返佣更吸引人的部分。这种情况下，做市商只需保证每月或每天参与一定规模的交易，就可以再额 外从交易所处获取一笔不小的收入。这种模式的好处在于，做市商不仅不用追求买卖价差，甚至连流量也不需争抢，只要完成 限定的额度即可，难度大大降低。  套利(Arbitrage) 这个策略应该是大家比较熟悉的，就是看两种高相关性的产品之间的价差。比如说一个股指 ETF 的价 格，理论上应该等于组成该 ETF 的股票价格的加权平均(具体计算方式按照此 ETF 定义来看)。但因为种种原因，有时我们 会发现市场上这两种价格并不一致，此时即产生套利机会，可以买入价低一方，同时卖出价高一方，以赚取其间的差价。随着 市场流动性的增强，这种机会发生的次数和规模会越来越小，并且机会往往转瞬即逝，因此往往需要借助高频交易的技术来加 大搜寻的规模和把握交易时机。  结构化(Structural)这是最恶的，也是公众眼中最能引人对高频交易诟病的策略。简单说就是利用 技术手段，比如高速连接和下单，来探测其他较慢的市场参与者的交易意图并且抢在他们之前进行交易，将利润建立在他人的 损失上。这种策略到底有多少人在用，报告中没有指明。我个人的看法是这种短视的策略或许能帮你赚一点快钱，但从任何角 度看都是一种饮鸩止渴的行为。高频交易本身有远超于此的价值，实在不值得为此浪费精力。  趋势(Directional)这个策略本身在中低频也存在。简而言之，就是预测一定时间内的价格走势，顺势而为。这种策略在 高频的玩法不同之处在于，高频交易的主要数据源是比 tick 更低一级的 order book events，所以可以在委托单的粒度上进行分 析和预测，来抓大单的动向。此外，报告中还提到一种操作手法，和中低频类似的，所谓“拉高出货”，即自己先下点单快速推 高或拉低价格，人为制造一种趋势，来吸引其他人入场。关于这些，相信有中低频或日内经验的高手们更清楚该如何做，我就 不多说了。QP宗Q重模型而轻数据，P则重数据而轻模型。      Q 可以让你在缺少数据的情况得出一些结论，从而可以凭空制造一些东西出来，所以卖方(投 行)用来做衍生品定价，业务模式是开发新的衍生品出来卖出去。    卖方投行随机过程、微分方程和编程        P喜欢大数据量，这天然就是买方(Hedge Fund 类)所需要的技术    买方Quant 概率、统计和编程  ]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[评估量化策略]]></title>
      <url>/blog/2018/09/11/%E8%AF%84%E4%BC%B0%E9%87%8F%E5%8C%96%E7%AD%96%E7%95%A5</url>
      <content type="text"><![CDATA[关于策略数据处理阿尔法投资组合构建执行风险管理]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[风险]]></title>
      <url>/blog/2018/09/10/%E9%A3%8E%E9%99%A9</url>
      <content type="text"><![CDATA[模型的风险给不能用模型解决的问题建模已知模型错误运用模型错误设定执行错误（代码 bug等，软硬件问题）对过去数据过于依赖 不够灵活  监控  敞口  既定头寸盈利时间百分比  执行监控  系统性能监控]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[数据]]></title>
      <url>/blog/2018/09/09/%E6%95%B0%E6%8D%AE</url>
      <content type="text"><![CDATA[输入变量特征价格数据基本面数据数据清洗缺失值错误值 异常值过滤数据训练 检验通过样本数据寻找最佳函数良好模型输出量累计盈利平均利益率高于平均收益的时间/总时间收益波动率波峰波谷最大降幅夏普 信息比率]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[执行模型]]></title>
      <url>/blog/2018/09/06/%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B</url>
      <content type="text"><![CDATA[重要概念判断价格是否公道      中间市场价最佳买入 卖出的平均价        交易量加权平均价vwap 最标准的指标衡量交易日里多笔交易试行算法质量  订单类型价格优先 （时间优先 或者 订单量按比例成交)要及时撤单  选择市价 限价 信号强度 置信水平主动订单马上成交 被动就不行了  订单规模]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[投资组合]]></title>
      <url>/blog/2018/08/31/%E6%8A%95%E8%B5%84%E7%BB%84%E5%90%88</url>
      <content type="text"><![CDATA[平衡收益、风险、交易成本基于经验  等权重（头寸，风险）使用前提使用等权模型 因为 非等权重 加权 有缺陷 非等权重加权 可能 承担不必要风险等权重 可以减少避免不良数据带来的风险可能受到流动性约束 不能真的等权重      等风险缺点风险的度量基本基于历史，可能发生突变        阿尔法驱动缺点  基于算法 通过目标函数寻找最优的组合MPT 现代投资组合均值方差优化 风险调整收益  投资组合规模  期望风险水平  缩小可行域的约束条件模型输入量  期望收益  期望波动率历史数据计算实际波动率  相关系数矩阵产品关系随时间会很不稳定  长时间也是 相关系数波动较大 不稳定是正常的，因为有多种因素决定金融产品间的相关性优化技术  无约束条件全仓最高收益的产品      有约束条件约束条件 总头寸规模     交易成本布莱特李特曼优化结合多个预测重新采样效率要确保历史样本数据能很好代表整个分布输出模型选择]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[交易成本模型]]></title>
      <url>/blog/2018/08/30/%E4%BA%A4%E6%98%93%E6%88%90%E6%9C%AC%E6%A8%A1%E5%9E%8B</url>
      <content type="text"><![CDATA[why 构建交易成本  让投资者知道交易成本状况 并非 最小化交易成本定义交易成本  佣金 commission 费用 fee给交易所交易手续费 不可避免  滑点 slippage 价格波动性从决策到交易执行的价格变动 可能交易不成功   市场冲击成本 market impact最重要的成本  入场时价格 与 交易执行价格 之差 =&gt; 就是订单头寸对深度的冲击力量化难  交易结算才能衡量市场冲击 此时就没有意义  滑点和市场冲击 有时难以区分交易成本模型交易成本模型高度依赖历史数据 因为不同时间，不同产品的波动率等特性都不一样  常值型  线性型  分段  二次]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[py配置]]></title>
      <url>/blog/2018/08/30/py%E9%85%8D%E7%BD%AE</url>
      <content type="text"><![CDATA[py2 py3 共存pyenv versionspyenv install [version]pyenv shell 3.6.5virtualenvvirtualenv -p python3 .env]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[风险模型]]></title>
      <url>/blog/2018/08/29/%E9%A3%8E%E9%99%A9%E6%A8%A1%E5%9E%8B</url>
      <content type="text"><![CDATA[敞口  在给定风险水平下控制敞口最大化收益控制规模的方式约束 惩罚要设定惩罚函数和临界水平度量风险方式  怎样才算控制了规模通过风险度量改变限制头寸 杆干率清除敞口理论驱动型风险模型专注已知或系统性风险因素经验驱动型风险模型怎样选择风险模型经验主义自适应性对比理论主义更强经验主义缺点优点]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[阿尔法模型]]></title>
      <url>/blog/2018/08/29/%E9%98%BF%E5%B0%94%E6%B3%95%E6%A8%A1%E5%9E%8B</url>
      <content type="text"><![CDATA[阿尔法策略长期低回报理论驱动策略种类基于价格数据策略研究趋势的延续或反转      趋势跟踪价格波动会沿着已有趋势运动风险在于横盘判断趋势显著 =&gt; 滤波 调理          MA 移动平均            均值回归 趋势反转价格围绕价值中枢波动 =&gt; 判断中枢 &amp; 波动方向          统计套利            技术情绪追踪投资者情绪买卖量 限价买卖量 交易量 限价book形态        基本面数据        其他策略市场高估高风险投资的风险 低估低风险投资的风险在适当时候买入高风险 卖出低风险投资卖低收益 买高收益          买高收益 卖低收益买涨幅大 卖涨幅小or负的买高质量 卖低质量      衡量股票质量  杆干比率  收入来源 std  管理水平  欺诈风险数据驱动缺点：  门槛高 理解难 计算量大 依赖历史数据经常修改风险大 算法噪音信号大解决的问题  how 决定现在的环境  定义相似  计算概率  确定回溯时间段实施策略注意问题  明确模型预测的目标  模型投资期限      投注结构 相对预测 分组        适用范围地理(美股，A股) 资产种类（外汇 股指） 产品类别（不同市场不同产品）          选择流动性强 优质数据 容易进行模型预测        模型设定对于模型关键判断点的设定: 判定方法 参数          参数设定 =&gt; 机器学习        条件变量修正型  止盈止损信号辅助型  运行频率model寻找新交易机会的频率频率快频率慢理论驱动设计图混合模型  可以多种策略一起使用线性  分配权重多元回归分析例子非线性条件模型例子  或者利用条件变量改变阿尔法模型权重旋转模型]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[资产配置]]></title>
      <url>/blog/2018/08/27/%E8%B5%84%E4%BA%A7%E9%85%8D%E7%BD%AE</url>
      <content type="text"><![CDATA[  时间序列预测收益率  构造均值/方差最优组合  使用最优组合进行样本外测试持续赢家$\delta\;=\;1-\frac1{2\;\ast\;P}$p 获胜概率 $\delta$ 持续赢概率]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[业绩分析]]></title>
      <url>/blog/2018/08/27/%E4%B8%9A%E7%BB%A9%E5%88%86%E6%9E%90</url>
      <content type="text"><![CDATA[区分运气与能力基于收益率分析比较组合 与 基准的夏普率组合分析那些因子创造附加值]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[策略实施]]></title>
      <url>/blog/2018/08/24/%E4%BA%A4%E6%98%93%E6%88%90%E6%9C%AC</url>
      <content type="text"><![CDATA[市值模型交易成本估计分析  执行价格 和 当日成交量加权平均价格VWAP  存货风险模型换手率  最优附加值下界  最优换手率交易最优化  做市商存活风险 交易内幕]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[策略实施]]></title>
      <url>/blog/2018/08/23/%E7%AD%96%E7%95%A5%E5%AE%9E%E6%96%BD</url>
      <content type="text"><![CDATA[必须面对的问题阿尔法分析 精确预测$\alpha\;=\;\mathrm{波动率}\;\ast\;IC\;\ast{\;\mathrm{标准分值}}$量级调整消除离群值交易成本VA附加值 风险厌恶  筛选法根据阿尔法选top 等权重组合  分类抽取分互斥类型 筛选法  线性规划换手率 头寸上下限 =&gt; max(α - cost)   二次规划充分考虑阿尔法 风险 和 交易脚本离差离差 = max收益率-min收益率策略的β 和 因子暴露度]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[预测基础]]></title>
      <url>/blog/2018/08/22/%E4%BF%A1%E6%81%AF%E5%B0%BA%E5%BA%A6</url>
      <content type="text"><![CDATA[信息时间尺度=&gt;保质期宏观分析先行 和 追随推导一般情况微观分析]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[预测基础]]></title>
      <url>/blog/2018/08/20/%E9%A2%84%E6%B5%8B%E5%9F%BA%E7%A1%80</url>
      <content type="text"><![CDATA[超常收益率##一个资产一个预测回归模型多一个预测$g’$高级预测时间序列kalman滤波非线性随机神经网络信息分析  将信息转化为组合IR 信息率IC 信息系数数据挖掘  直觉  克制  合乎情理  样本外测试]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[典型错误纠错]]></title>
      <url>/blog/2018/08/19/%E5%85%B8%E5%9E%8B%E9%94%99%E8%AF%AF%E7%BA%A0%E9%94%99</url>
      <content type="text"><![CDATA[慢查询优化  是否检索大量超过需要的数据。访问太多的行和列  mysql服务器层是否分析大量超过需要的数据典型错误（超过实际需要的数据，请求了不需要的数据）  查询不需要的记录（使用limit，舍弃大部分数据）  多表关联返回所有列（获取所需要的列）  总是使用select*（提高代码复用性，使用缓存机制）  多次查询相同的数据（缓存解决）扫描额外的记录查询开销指标 在慢日志记录  响应时间：服务时间+排队时间(等待资源时间)  扫描行数：重要指标 可能扫描多行生产结果集的一行  返回行数访问类型explain语句type列说明访问类型，最好使用索引(ref)重构select方式  一个复杂查询还是多个查询(主要是Mysql响应较慢)  切分查询(删除语句可能锁住数据，占用资源等，可用delete limit 返回删除结果)  分解关联查询]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[事务]]></title>
      <url>/blog/2018/08/19/%E4%BA%8B%E5%8A%A1</url>
      <content type="text"><![CDATA[1、数据库事务的属性－ACID（四个英文单词的首写字母）：  原子性（Atomicity）  所谓原子性就是将一组操作作为一个操作单元，是原子操作，即要么全部执行，要么全部不执行。  一致性（Consistency）  事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。  隔离性（Isolation）  隔离性指并发的事务是相互隔离的。即一个事务内部的操作及正在操作的数据必须封锁起来，不被其它企图进行修改的事务看到。  持久性（Durability）  持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响。即一旦一个事务提交，DBMS（DatabaseManagement System）保证它对数据库中数据的改变应该是永久性的，持久性通过数据库备份和恢复来保证。2、在关系型数据库中，事务的隔离性分为四个隔离级别，在解读这四个级别前先介绍几个关于读数据的概念。1）脏读（Dirty Reads）：所谓脏读就是对脏数据（Drity Data）的读取，而脏数据所指的就是未提交的数据。也就是说，一个事务正在对一条记录做修改，在这个事务完成并提交之前，这条数据是处于待定状态的（可能提交也可能回滚），这时，第二个事务来读取这条没有提交的数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被称为脏读。2）不可重复读（Non-Repeatable Reads）：一个事务先后读取同一条记录，但两次读取的数据不同，我们称之为不可重复读。也就是说，这个事务在两次读取之间该数据被其它事务所修改。3）幻读（Phantom Reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为幻读。3、事务四个隔离级别对比：1）未提交读（Read Uncommitted）：SELECT语句以非锁定方式被执行，所以有可能读到脏数据，隔离级别最低。2）提交读（Read Committed）：只能读取到已经提交的数据。即解决了脏读，但未解决不可重复读。3）可重复读（Repeated Read）：在同一个事务内的查询都是事务开始时刻一致的，InnoDB的默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻读。4）串行读（Serializable）：完全的串行化读，所有SELECT语句都被隐式的转换成SELECT … LOCK IN SHARE MODE，即读取使用表级共享锁，读写相互都会阻塞。隔离级别最高。隔离级别对比表：]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mysql查询过程]]></title>
      <url>/blog/2018/08/19/mysql%E6%9F%A5%E8%AF%A2%E8%BF%87%E7%A8%8B</url>
      <content type="text"><![CDATA[mysql查询过程mysql使用半双工 通过大小写敏感Hash命中缓存查询语句 ![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuezaih3z5j31160sggn4.jpg)mysql查询优化器  静态优化 只需要做一次 编译时优化  动态优化 每次执行都要重新评估 运行时优化  可优化类型：          重新定义关联表顺序      外连接转化内连接      等价变换规则      优化count min max      转化为常数表达式      …..      查询优化器局限性  关联子查询 最糟的是where条件中的包含in的子查询 会执行外层在执行里面的子查询（使用join来搭救 但是不要迷信要explain一下）msyql中子查询IN，EXISTS，ANY，ALL，UNION介绍  union子句添加order by和limit限制  松散索引的扫描（增加索引，查询语句添加常数列）  max min 没有索引的字段会全表扫描select ds_id from xx USE INDEX(PRIMARY) where oo=xx limit1;使用limit躲避min  同一个表里面查询和更新]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[事务]]></title>
      <url>/blog/2018/08/19/PAC-%E8%A7%A3%E6%9E%90</url>
      <content type="text"><![CDATA[最终一致性  过程松，结果紧，最终结果必须保持一致性  强一致性  强一致性（即时一致性） 假如A先写入了一个值到存储系统，存储系统保证后续A,B,C的读取操作都将返回最新值  弱一致性  假如A先写入了一个值到存储系统，存储系统不能保证后续A,B,C的读取操作能读取到最新值。此种情况下有一个“不一致性窗口”的概念，它特指从A写入值，到后续操作A,B,C读取到最新值这一段时间。  最终一致性  最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A,B,C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到最A写入的最新值。此种情况下，如果没有失败发生的话，“不一致性窗口”的大小依赖于以下的几个因素：交互延迟，系统的负载，以及复制技术中replica的个数（这个可以理解为master/salve模式中，salve的个数），最终一致性方面最出名的系统可以说是DNS系统，当更新一个域名的IP以后，根据配置策略以及缓存控制策略的不同，最终所有的客户都会看到最新的值。最终一致性  过程松，结果紧，最终结果必须保持一致性  强一致性  强一致性（即时一致性） 假如A先写入了一个值到存储系统，存储系统保证后续A,B,C的读取操作都将返回最新值  弱一致性  假如A先写入了一个值到存储系统，存储系统不能保证后续A,B,C的读取操作能读取到最新值。此种情况下有一个“不一致性窗口”的概念，它特指从A写入值，到后续操作A,B,C读取到最新值这一段时间。  最终一致性  最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A,B,C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到最A写入的最新值。此种情况下，如果没有失败发生的话，“不一致性窗口”的大小依赖于以下的几个因素：交互延迟，系统的负载，以及复制技术中replica的个数（这个可以理解为master/salve模式中，salve的个数），最终一致性方面最出名的系统可以说是DNS系统，当更新一个域名的IP以后，根据配置策略以及缓存控制策略的不同，最终所有的客户都会看到最新的值。CAPConsistency（一致性）：一致性是说数据的原子性，这种原子性在经典的数据库中是通过事务来保证的，当事务完成时，无论其是成功还是回滚，数据都会处于一致的状态。在分布式环境中，一致性是说多点的数据是否Availability（可用性）：可用性是说服务能一直保证是可用的状态，当用户发出一个请求，服务能在有限时间内返回结果。而这种可用性是不关乎结果的正确与否，所以，如果服务一致返回错误的结果，其实也可以称为其是可用的。Partition?Tolerance（分区容忍性）：Partition这个词不是常说的操作系统或数据库中的用语，而是指网络的分区。网络中的两个服务结点出现分区的原因很多，比如网络断了、对方结点因为程序bug或死机等原因不能访问。一个分布式系统只能有限的实现两者要求  依据CAP理论，从应用的需求不同，我们对数据库（其实就是一种结构化数据存储，和Bolb恰好不同）时，可以从三方面考虑：  考虑CA，这就是传统上的关系型数据库(RMDB).  考虑CP，主要是一些Key-value数据库，典型代表为google的Big Table  考虑AP，主要是一些面向文档的适用于分布式系统的数据库，如SimpleDB。一致性  一致性可分为强一致性和弱一致性，弱一致性又称为最终一致性。  在单机环境中，强一致性可以由数据库的事务保证。但在多机环境中，强一致性就很难做到。尽管可以使用2PC来实现分布式事务，但它的低性能（很多情况下满足不了可用性需求）使得不适合于互联网应用。这种强一致性效果的取得，其实是让提交处理过程同步化。  在多机环境中，通过使提交处理半同步半异步、或者全异步，取得最终一致性效果。例如数据库中的主从复制，在提交时就是主库同步从库异步，这对从库复制进度落后不多的场景很简单有效，但在从库落后主库很多时，如果应用还从从库读数据，就会读出脏数据，可以通过监控从库复制进度来选择读哪个从库以避免这个问题。在NOSQL模式下，以Dynamo为例，可以通过确定NRW的不同取值，可以做到同步、半同步半异步、或者全异步的效果。  最终一致性使得数据的提交效果具有延时性，而在一定的延时性范围内（比如1秒以内），应用的可用性就是OK的，比如提交后在客户端通过JS等停一段时间刷新页面就是要取得这种效果。主从OR对等系统主从系统  有着广泛的应用，它很适合于提交压力不会使得从库复制明显落后的场景。  它的缺点是，当主从提交压力增大、或者存在耗时长的提交命令时，从库复制进度会明显落后于主库。在Cache+DB的应用场景下，Cache的填充时机和策略也会受到主从模式影响。如果在一个Session中提交DB后作废Cache，而由后续的（或并发的）另一个Session来再次设置Cache、并且数据是从从库获取，就很有可能缓存住脏数据，如果Cache时间很长，那问题可能就很严重了。可以变通的策略是，在一个Session中提交DB后不是作废Cache，而是更新Cache，并且数据是从主库获取或者直接从应用环境获取。  主从的另一个问题是，它有提交单点，但是，如果主从能满足应用需要，在具有完备的主备切换的保证下，这个单点问题并不见得有多大。对等系统  解决了单点问题，有着高分区容忍性，但它的效果仍得商榷。  NRW的不同取值会影响读写数据效果，  多点提交带来的冲突解决也是个问题，尽管通常采用高版本替换低版本的粗暴策略。  现在的NOSQL对等系统存储的是简单的kv，并且可以采用加机器来扩容，所以性能方面应该是可接受的（尽管常见的单机kv存储性能要比数据库高很多，但在对等系统中，因为其复杂性，换算下来单机的性能并不见得有多高）]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MyISAM与InnoDB的主要区别]]></title>
      <url>/blog/2018/08/19/MyISAM%E4%B8%8EInnoDB%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB</url>
      <content type="text"><![CDATA[MySQL存储引擎MyISAM与InnoDB的主要区别对比MYsql innodb]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[缓存误用]]></title>
      <url>/blog/2018/08/19/%E7%BC%93%E5%AD%98%E8%AF%AF%E7%94%A8</url>
      <content type="text"><![CDATA[作为服务之间传递数据的媒介数据管道，数据通知场景，MQ更加适合多个服务关联同一个缓存实例，会导致服务耦合没有防雪崩服务先读缓存，缓存命中则返回，缓存不命中，再读数据库提前做容量预估，如果缓存挂掉，数据库仍能工作  使用高可用缓存集群，一个缓存实例挂掉后，能够自动做故障转移  使用缓存水平切分，一个缓存实例挂掉后，不至于所有的流量都压到数据库上。雪崩缓存挂掉，所有的请求会压到数据库，如果未提前做容量预估，可能会把数据库压垮（在缓存恢复之前，数据库可能一直都起不来），导致系统整体不可服务。]]></content>
      <categories>
        
          <category> cache </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[缓存改 or 淘汰]]></title>
      <url>/blog/2018/08/19/%E7%BC%93%E5%AD%98%E6%94%B9-or-%E6%B7%98%E6%B1%B0</url>
      <content type="text"><![CDATA[diff  淘汰某个key，操作简单，直接将key置为无效，但下一次该key的访问会cache miss  修改某个key的内容，逻辑相对复杂，但下一次该key的访问仍会cache hit改  序列化后的对象：一般需要先get数据，反序列化成对象，修改其中的成员，再序列化为binary，再set数据  json或者html数据：一般也需要先get文本，parse成doom树对象，修改相关元素，序列化为文本，再set数据以上情况一般选择直接淘汰缓存]]></content>
      <categories>
        
          <category> cache </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[缓存与数据库不一致]]></title>
      <url>/blog/2018/08/19/%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8D%E4%B8%80%E8%87%B4</url>
      <content type="text"><![CDATA[数据库集群架构一主多从，主从同步，读从写主。数据库主从不一致写后立刻读写主读从，主动同步完成之前，会读取到旧数据解决问题  允许短时间不一致 忽略  强制读主库 使用缓存 读和写都落到主库  选择性读主写主哪个库，哪个表，哪个主键三个信息拼装一个key设置到cache，超时时间，设置为“主从同步时延”读请求: 到cache里去查询 有key 读主  无key 读从读操作 缓存不命中命中 读cache尝试从缓存get数据，结果没有命中从数据库获取数据，读从库，读写分离把数据set到缓存，未来能够命中缓存写操作  del 缓存 not set并发写时序问题 数据库与缓存之间的数据不一致  cache first or db first准则：数据最终以数据库为准，写缓存成功，其实并不算成功db first if cache failed =&gt; 数据不一致so cache first if db failed =&gt; 不影响业务缓存与数据库不一致主从不一致 导致因为读操作弄脏缓存从库同步完成之后，如果有旧数据入缓存，应该及时把这个旧数据淘汰掉从库执行完写操作，向缓存再次发起删除，淘汰这段时间内可能写入缓存的旧数据订阅从库的binlog，这里能够最准确的知道，从库数据同步完成的时间]]></content>
      <categories>
        
          <category> cache </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[cache 术语]]></title>
      <url>/blog/2018/08/19/cache-%E6%9C%AF%E8%AF%AD</url>
      <content type="text"><![CDATA[缓存穿透一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。如果key对应的value是一定不存在的，并且对该key并发请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。如何避免？有一个比较巧妙的作法是，可以将这个不存在的key预先设定一个值。比如，”key” , “&amp;&amp;”。在返回这个&amp;&amp;值的时候，我们的应用就可以认为这是不存在的key，那我们的应用就可以决定是否继续等待继续访问，还是放弃掉这次操作。如果继续等待访问，过一个时间轮询点后，再次请求这个key，如果取到的值不再是&amp;&amp;，则可以认为这时候key有值了，从而避免了透传到数据库，从而把大量的类似请求挡在了缓存之中。缓存雪崩当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。如何避免？  在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。  不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。(缓存大量失效)  做二级缓存，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期（此点为补充）缓存预热在服务启动前将数据写入缓存缓存算法  FIFO算法：First in First out，先进先出。  原则：一个数据最先进入缓存中，则应该最早淘汰掉。也就是说，当缓存满的时候，应当把最先进入缓存的数据给淘汰掉。  LFU算法：Least Frequently Used，最不经常使用算法。  LRU算法：Least Recently Used，近期最少使用算法。LRU和LFU的区别。LFU算法是根据在一段时间里数据项被使用的次数选择出最少使用的数据项，即根据使用次数的差异来决定。而LRU是根据使用时间的差异来决定的。]]></content>
      <categories>
        
          <category> cache </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率-分布]]></title>
      <url>/blog/2018/08/18/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83</url>
      <content type="text"><![CDATA[离散数据分布律  伯努利分布事件A只有两种结果 进行独立重复试验$X\sim b(n,p)$分布函数连续数据概率密度      均匀分布        指数分布        正态分布  多维向量  联合分布  边缘分布  条件分布]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[内存缓存]]></title>
      <url>/blog/2018/08/18/%E5%86%85%E5%AD%98%E7%BC%93%E5%AD%98</url>
      <content type="text"><![CDATA[如果数据缓存在站点和服务的多个节点内，数据存了多份，一致性比较难保障。单节点通知其他节点修改完自己内存数据与数据库中的数据之后，再通知其他节点修改内存的数据。同一功能的一个集群的多个节点，相互耦合在一起，特别是节点较多时，网状连接关系极其复杂。使用MQ可以通过MQ通知其他节点。写请求发生在server1，在修改完自己内存数据与数据库中的数据之后，给MQ发布数据变化通知，其他server节点订阅MQ消息，也修改内存数据。这种方案虽然解除了节点之间的耦合，但引入了MQ，使得系统更加复杂。  前两种方案，节点数量越多，数据冗余份数越多，数据同时更新的原子性越难保证，一致性也就越难保证。直接拉数据库为了避免耦合，降低复杂性，干脆放弃了“实时一致性”，每个节点启动一个timer，定时从后端拉取最新的数据，更新内存缓存。在有节点更新后端数据，而其他节点通过timer更新数据之间，会读到脏数据。When 使用  只读数据，可以考虑在进程启动时加载到内存。  极其高并发的，如果透传后端压力极大的场景，可以考虑使用进程内缓存。  一定程度上允许数据不一致业务。不能随便用数据和状态尽量存储到后端的数据存储服务，例如数据库服务或者缓存服务，才能任意的加节点水平扩展。]]></content>
      <categories>
        
          <category> cache </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[编译C代码]]></title>
      <url>/blog/2018/08/17/%E7%BC%96%E8%AF%91C%E4%BB%A3%E7%A0%81</url>
      <content type="text"><![CDATA[可能获得哪种类型的速度提升编译后趋于更快运行的 Python 代码有可 能是数学方面的，并且可能有许多循环在重复着多次相同的运算。在这些循环中， 有可能会生成许多临时对象。只有当被编译的代码主要是 Python(并且可能主要是循 环)时才会运行得更快JIT 和 AOT 编译器的对比      提前编译(AOT)，你会创建一个为你的机器定制的静态库。如果你下载了 numpy、scipy 或 scikit-learn，它就会在你的机器上用 Cython 编译部分的 库(或者如果你正在使用像 Continuum’s Anaconda 之类的分发包，你就会使用一个 事先构建的预编译库)。通过在使用之前编译的方式，你就会获得一个能够在工作 中立即拿来使用来解决你的问题的库。        即时编译冷启动：通过即时编译，你不必提前做很多(如果有的话)，你让编译器在使用时只逐步编 译恰到好处的那部分代码。如果你的大部分 代码能够被编译并且当前都还没有被编译过，当你开始运行代码而且正在被编译 时，就会跑得很慢。如果这事在你每次运行脚本时都发生，并且你运行这脚本很多 次，开销就会变得很显著。  CythonIn pyxdef fib(n):    cdef double a = 0.0, b = 1.0    for i in range(n):        a, b = a + b, a    return a导入pyximport pyximportpyximport.install()from xx import fib]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[性能优化]]></title>
      <url>/blog/2018/08/17/%E4%BC%98%E5%8C%96</url>
      <content type="text"><![CDATA[上下文切换当一个程序进入 I/O 等待时，暂停执行，这样内核就能执行 I/O 请求相关的低级操作，直到 I/O 操作完成时才继续上下文切换是相当重 量级的操作。它要求我们保存程序的状态(丢失了我们在 CPU 层面上任何类型的 缓存)，放弃使用 CPU。之后，当我们允许再次运行时，我们必须花时间在主板上 重新初始化程序并准备好继续运行事件循环回调 or futureFuture 由 gevent.spawn 来创建，使用了一个函数和传递给这个函数的参数，并 且启动了一个负责运行这个函数的 greenlet。greenlet 能够被看作一个 future， 因为你声明的函数一旦运行完成，它的值就会包含在 greenlet 的 value 域中multiprocessing  把你的工作拆分成独立的工作单元。  如果你的工作者所花的时间是可变的，那就考虑随机化工作序列(另一个例子 就是处理大小可变的文件)。  对你的工作队列进行排序，这样首先处理最慢的任务可能是一个平均来说有用 的策略。  使用默认的 chunksize，除非你已经验证了调节它的理由。  让任务数量与物理 CPU 数量保持一致(默认的 chunksize 再次为你考虑到了，尽管它默认会使用超线程，这样可能不会提供额外的性能收益)。Queue 主要看通信序列化(pickle)和同步，使用 Queue 具有相当的开销使用一个更少的 Queue 的单进程解决方案明显要比使用两个 或多个进程的要快两个进程完成这个例子要比一个进程稍快一点，而四个或八个进程则比一个进程要更慢进程通信multiprocessing.Valuemmap内存基础类型的对象开销高 整数对象已经存在缓存中，这样就可以被复用。Python 缓存了类似整数的基础对象为以后所用。在一个RAM有限的系统中，这会造成问题，所以你应该注意到这些基础类型可能会构建在缓存中。Array 创建了一个连续的RAM块来保存底层数据，高效地存储了类似于整数、浮点数和字符的基础类型，但没有复数或者类注意在 array 中的唯一数字不是 Python 对象，它们在 array 中是字节。如果我 们要解引用它们中任何一个，那么一个新的 Python int 对象将会被构建。如果你想要在它们之上来做计算，不会发生整体上的节省，但是如果你想要把数组传递给一 个外部进程或者只使用一些数据，你应该看到相比使用一个整数的 list 来说，大大节约了 RAM]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GC]]></title>
      <url>/blog/2018/08/17/GC</url>
      <content type="text"><![CDATA[引用计数法一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。它的缺点是需要额外的空间维护引用计数，这个问题是其次的，不过最主要的问题是它不能解决对象的“循环引用”循环引用因此如果是使用引用计数法来管理这两对象的话，他们并不会被回收，它会一直驻留在内存中，就会造成了内存泄漏（内存空间在使用完毕后未释放）。为了解决对象的循环引用问题，Python引入了标记-清除和分代回收两种GC机制。标记清除  GC会把所有的活动对象打上标记  没有标记的对象非活动对象进行回收标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些容器对象，比如list、dict、tuple，instance等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象。分代回收以空间换时间Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象GC应用]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[shell-字符串]]></title>
      <url>/blog/2018/08/17/shell%E5%AD%97%E7%AC%A6%E4%B8%B2</url>
      <content type="text"><![CDATA[判断读取字符串值${var}	变量var的值, 与$var相同${var-DEFAULT}	如果var没有被声明, 那么就以$DEFAULT作为其值 *${var:-DEFAULT}	如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 *${var=DEFAULT}	如果var没有被声明, 那么就以$DEFAULT作为其值 *${var:=DEFAULT}	如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 *${var+OTHER}	如果var声明了, 那么其值就是$OTHER, 否则就为null字符串${var:+OTHER}	如果var被设置了, 那么其值就是$OTHER, 否则就为null字符串${var?ERR_MSG}	如果var没被声明, 那么就打印$ERR_MSG *${var:?ERR_MSG}	如果var没被设置, 那么就打印$ERR_MSG *${!varprefix*}	匹配之前所有以varprefix开头进行声明的变量${!varprefix@}	匹配之前所有以varprefix开头进行声明的变量字符串操作（长度，读取，替换）${#string}	$string的长度${string:position}	在$string中, 从位置$position开始提取子串${string:position:length}	在$string中, 从位置$position开始提取长度为$length的子串${string#substring}	从变量$string的开头, 删除最短匹配$substring的子串${string##substring}	从变量$string的开头, 删除最长匹配$substring的子串${string%substring}	从变量$string的结尾, 删除最短匹配$substring的子串${string%%substring}	从变量$string的结尾, 删除最长匹配$substring的子串${string/substring/replacement}	使用$replacement, 来代替第一个匹配的$substring${string//substring/replacement}	使用$replacement, 代替所有匹配的$substring${string/#substring/replacement}	如果$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring${string/%substring/replacement}	如果$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[shell $ 相关 参数变量]]></title>
      <url>/blog/2018/08/17/shell%E5%8F%82%E6%95%B0</url>
      <content type="text"><![CDATA[shell 参数 $0, $#, $*, $@, $?, $$和命令行参数$* 和 $@ 的区别$* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(" ")包含时，都以"$1" "$2" … "$n" 的形式输出所有参数。但是当它们被双引号(" ")包含时，"$*" 会将所有的参数作为一个整体，以"$1 $2 … $n"的形式输出所有参数；"$@" 会将各个参数分开，以"$1" "$2" … "$n" 的形式输出所有参数。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。  /dev/null 是一个特殊的设备文件，这个文件接收到的任何数据都会被丢弃。因此，null这个设备通常也被成为位桶（bit bucket）或黑洞0 —— stdin（标准输入）1 —— stdout （标准输出）2 —— stderr （标准错误)2&gt; /dev/null  错误重定向到黑洞$()和 ` ` 不同var=$(cmd) or var=`cmd`cmd =&gt; 原生命令 find or func param$() 可读性$var and ${var}大部分相同 注意区分var=fooecho $varbar# Prints nothing because there is no variable 'varbar'echo ${var}bar# foobar双引号=&gt; 一个单词var="foo bar"for i in "$var"; do # Expands to 'for i in "foo bar"; do...'    echo $i         #   so only runs the loop oncedone# foo barvar="foo bar"for i in $var; do # Expands to 'for i in foo bar; do...'    echo $i       #   so runs the loop twice, once for each argumentdone# foo# bar数组$ 有空格时不一样foo=("the first" "the second")for i in "${foo[@]}"; do # Expands to 'for i in "the first" "the second"; do...'    echo $i              #   so the loop runs twicedone# the first# the secondfoo=("the first" "the second")for i in ${foo[@]}; do # Expands to 'for i in the first the second; do...'    echo $i            #   so the loop runs four times!done# the# first# the# second]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[shell-if]]></title>
      <url>/blog/2018/08/17/shell-if</url>
      <content type="text"><![CDATA[if list then    do something hereelif list then    do another thing hereelse   do something else herefi注意：在[] 表达式中，常见的&gt;, &lt;需要加转义字符，表示字符串大小比较，以acill码 位置作为比较。不直接支持&lt;, &gt;运算符，还有逻辑运算符|| , &amp;&amp; 它需要用-a[and] –o[or]表示6.3 [[]] 表达式[ 1 -eq 1 ] &amp;&amp; echo 'ok'ok[[ 2 &lt; 3 ]] &amp;&amp; echo 'ok'ok[[ 2 &lt; 3 &amp;&amp; 4 &gt; 5 ]] &amp;&amp; echo 'ok'ok[[ 2 &lt; 3 -a 3 &gt; 4 ]] &amp;&amp; echo "ok"-bash: syntax error in conditional expression-bash: syntax error near `-a'注意：[[]] 运算符只是[]运算符的扩充。能够支持&lt;,&gt;符号运算不需要转义符，它还是以字符串比较大小。里面支持逻辑运算符：|| &amp;&amp; ，不再使用-a -o字符串判断str1 == str2　　　　　　当两个串有相同内容、长度时为真str1 != str2　　　　　 当串str1和str2不等时为真-n str1　　　　　　　 当串的长度大于0时为真(串非空)-z str1　　　　　　　 当串的长度为0时为真(空串)str1　　　　　　　　   当串str1为非空时为真数字的判断int1 -eq int2　　　　两数相等为真int1 -ne int2　　　　两数不等为真int1 -gt int2　　　　int1大于int2为真int1 -ge int2　　　　int1大于等于int2为真int1 -lt int2　　　　int1小于int2为真int1 -le int2　　　　int1小于等于int2为真文件的判断-e              文件或目录存在为真-r file　　　　　用户可读为真-w file　　　　　用户可写为真-x file　　　　　用户可执行为真-f file　　　　　文件为正规文件为真-d file　　　　　文件为目录为真-c file　　　　　文件为字符特殊文件为真-b file　　　　　文件为块特殊文件为真-s file　　　　　文件大小非0时为真-t file　　　　　当文件描述符(默认为1)指定的设备为终端时为真附录[ -b FILE ]  如果 FILE 存在且是一个块特殊文件则为真。[ -c FILE ]  如果 FILE 存在且是一个字特殊文件则为真。[ -d FILE ]  如果 FILE 存在且是一个目录则为真。[ -e FILE ]  如果 FILE 存在则为真。[ -f FILE ]  如果 FILE 存在且是一个普通文件则为真。[ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真。 [ -h FILE ]  如果 FILE 存在且是一个符号连接则为真。[ -k FILE ]  如果 FILE 存在且已经设置了粘制位则为真。[ -p FILE ]  如果 FILE 存在且是一个名字管道(F如果O)则为真。[ -r FILE ]  如果 FILE 存在且是可读的则为真。[ -s FILE ]  如果 FILE 存在且大小不为0则为真。[ -t FD ]  如果文件描述符 FD 打开且指向一个终端则为真。[ -u FILE ]  如果 FILE 存在且设置了SUID (set user ID)则为真。[ -w FILE ]  如果 FILE 如果 FILE 存在且是可写的则为真。[ -x FILE ]  如果 FILE 存在且是可执行的则为真。[ -O FILE ]  如果 FILE 存在且属有效用户ID则为真。[ -G FILE ]  如果 FILE 存在且属有效用户组则为真。[ -L FILE ]  如果 FILE 存在且是一个符号连接则为真。[ -N FILE ]  如果 FILE 存在 and has been mod如果ied since it was last read则为真。[ -S FILE ]  如果 FILE 存在且是一个套接字则为真。[ FILE1 -nt FILE2 ]  如果 FILE1 has been changed more recently than FILE2, or 如果 FILE1 exists and FILE2 does not则为真。[ FILE1 -ot FILE2 ]  如果 FILE1 比 FILE2 要老, 或者 FILE2 存在且 FILE1 不存在则为真。[ FILE1 -ef FILE2 ]  如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。[ -o OPTIONNAME ]  如果 shell选项 “OPTIONNAME” 开启则为真。[ -z STRING ]  “STRING” 的长度为零则为真。[ -n STRING ] or [ STRING ]  “STRING” 的长度为非零 non-zero则为真。[ STRING1 == STRING2 ]  如果2个字符串相同。 “=” may be used instead of “==” for strict POSIX compliance则为真。[ STRING1 != STRING2 ]  如果字符串不相等则为真。[ STRING1 &lt; STRING2 ]  如果 “STRING1” sorts before “STRING2” lexicographically in the current locale则为真。[ STRING1 &gt; STRING2 ]  如果 “STRING1” sorts after “STRING2” lexicographically in the current locale则为真。[ ARG1 OP ARG2 ] “OP” is one of -eq, -ne, -lt, -le, -gt or -ge. These arithmetic binary operators return true if “ARG1” is equal to, not equal to, less than, less than or equal to, greater than, or greater than or equal to “ARG2”, respectively. “ARG1” and “ARG2” are integers.]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[估值]]></title>
      <url>/blog/2018/08/16/%E4%BC%B0%E5%80%BC</url>
      <content type="text"><![CDATA[确定现金流不确定现金流现代估值$\mathrm{市场价格}\;=\;\mathrm{拟合价格}\;+{\;\mathrm{偏差}}$$\mathrm{超常收益率}\alpha\;=\;-\frac{\mathrm{偏差}}{\mathrm{市场价格}}\;=\;\frac{\mathrm{拟合价格}\;-\;\mathrm{市场价格}}{\mathrm{市场价格}}$]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[APT 套利定价]]></title>
      <url>/blog/2018/08/14/APT-%E5%A5%97%E5%88%A9%E5%AE%9A%E4%BB%B7</url>
      <content type="text"><![CDATA[寻找最佳投资组合Q 使得最高 收益/风险比率 夏普率组合风险最小 合格模型建立结构化模型  给定权重 =&gt; 估计收益率  给定收益率 =&gt; 估计权重统计模型  PCA]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[随机过程]]></title>
      <url>/blog/2018/08/01/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B</url>
      <content type="text"><![CDATA[期权定价所用的Black-Scholes-Meron设置$S_T=S_0exp((r-\frac12\sigma^2)T+\sigma\sqrt Tz)$ST  ：T日的指数水平 r ：恒定无风险短期利率 σ ：S的恒定波动率（= 收益率的标准差） z：标准正态分布随机变量 模拟的几何布朗运动S0 = 100  # initial valuer = 0.05  # constant short ratesigma = 0.25  # constant volatilityT = 2.0  # in yearsI = 10000  # number of random drawsST1 = S0 * np.exp((r - 0.5 * sigma ** 2) * T + sigma * np.sqrt(T) * np.random.standard_normal(I))##随机过程随机过程是一个随机变量序列,随机数的选取一般不是独立的,依赖于前几次选取的结果.  明天的过程值只依赖于今天的过程状态， 而不依赖其他任何 “历史” 状态． 甚至不依赖整个路径历史。无记忆过程x0 = 0.05kappa = 3.0theta = 0.02sigma = 0.1T = 2.0I = 10000M = 50dt = T / Mdef srd_euler():    xh = np.zeros((M + 1, I))    x1 = np.zeros_like(xh)    xh[0] = x0    x1[0] = x0    for t in range(1, M + 1):        xh[t] = (xh[t - 1]                 + kappa * (theta - np.maximum(xh[t - 1], 0)) * dt                 + sigma * np.sqrt(np.maximum(xh[t - 1], 0)) * np.sqrt(dt)                 * np.random.standard_normal(I))    x1 = np.maximum(xh, 0)    return x1平方根扩散的精确离散化def srd_exact():    x2 = np.zeros((M + 1, I))    x2[0] = x0    for t in range(1, M + 1):        df = 4 * theta * kappa / sigma ** 2        c = (sigma ** 2 * (1 - np.exp(-kappa * dt))) / (4 * kappa)        nc = np.exp(-kappa * dt) / c * x2[t - 1]        x2[t] = c * random.noncentral_chisquare(df, nc, size=I)    return x2##随机波动率S0 = 100.r = 0.05v0 = 0.1kappa = 3.0theta = 0.25sigma = 0.1rho = 0.6T = 1.0# 为了说明两个随机过程之间的相关性，我们需要确定相关矩阵的柯列斯基分解：corr_mat = np.zeros((2, 2))corr_mat[0, :] = [1.0, rho]corr_mat[1, :] = [rho, 1.0]cho_mat = np.linalg.cholesky(corr_mat)cho_mat# array([[ 1. ,  0. ],#        [ 0.6,  0.8]])# 在开始模拟随机过程之前， 我们为两个过程生成整组随机数， 指数过程使用第0组，波动性过程使用第1组：M = 50I = 10000ran_num = random.standard_normal((2, M + 1, I))# 对于以平方根扩散过程类型建模的波动性过程， 我们使用欧拉格式， 考虑相关性参数：dt = T / Mv = np.zeros_like(ran_num[0])vh = np.zeros_like(v)v[0] = v0vh[0] = v0for t in range(1, M + 1):    ran = np.dot(cho_mat, ran_num[:, t, :])    vh[t] = (vh[t - 1] + kappa * (theta - np.maximum(vh[t - 1], 0)) * dt             + sigma * np.sqrt(np.maximum(vh[t - 1], 0)) * np.sqrt(dt)             * ran[1])    v = np.maximum(vh, 0)# 对于指数水平过程, 我们也考虑相关性，使用几何布朗运动的精确欧拉格式：S = np.zeros_like(ran_num[0])S[0] = S0for t in range(1, M + 1):    ran = np.dot(cho_mat, ran_num[:, t, :])    S[t] = S[t - 1] * np.exp((r - 0.5 * v[t]) * dt +                             np.sqrt(v[t]) * ran[0] * np.sqrt(dt))# 这说明了平方根扩散中使用欧拉格式的另一项优势：相关性很容易一致性地处理， 因为我们只提取标准正态随机数。 没有一种棍合方法（对指数使用欧拉格式， 对波动性过程使用基于非中心卡方分布的精确方法）能够实现相同的效果。跳跃扩散S0 = 100.r = 0.05sigma = 0.2lamb = 0.75mu = -0.6delta = 0.25T = 1.0# 为了模拟跳跃扩散,需要生成3组（独立）随机数：M=50I=10000rj=lamb*(np.exp(mu+0.5*delta**2)-1)S=np.zeros((M+1,I))S[0]=S0sn1=random.standard_normal((M+1,I))sn2=random.standard_normal((M+1,I))poi=random.poisson(lamb*dt,(M+1,I))for t in range(1,M+1,1):    S[t]=S[t-1]*(np.exp((r-rj-0.5*sigma**2)*dt                        +sigma*np.sqrt(dt)*sn1[t])                        +(np.exp(mu+delta*sn2[t])-1)                        *poi[t])    S[t]=np.maximum(S[t],0)##方差缩减(sn - sn.mean()) / sn.std()]]></content>
      <categories>
        
          <category> py-finance </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[解方程]]></title>
      <url>/blog/2018/08/01/%E8%A7%A3%E6%96%B9%E7%A8%8B</url>
      <content type="text"><![CDATA[numpyscipysympy]]></content>
      <categories>
        
          <category> py-finance </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[np & pd & decimal 精度]]></title>
      <url>/blog/2018/07/30/np-pd-decimal%E7%B2%BE%E5%BA%A6</url>
      <content type="text"><![CDATA[np 结构化数组结构化数组 每列类型不一样pd增appendjoin how=’outer’ 并集 inner (索寻|的交集)、 left (默认俏，使用调用方法 的对象中的索引值)以及 right (使用被连接对象的索引值)。删del df[‘xxx’]Python 整数可以为任意大]]></content>
      <categories>
        
          <category> py-finance </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[信息率]]></title>
      <url>/blog/2018/07/25/%E4%BF%A1%E6%81%AF%E7%8E%87</url>
      <content type="text"><![CDATA[收益 风险 基准 偏好 信息率定义阿尔法先验$\mathrm{信息率}(information\;ratio)\;=\;\frac{\mathrm{年化收益率}}{\mathrm{年化风险}}$信息比率(Information Ratio)跟踪误差(Tracking Error)]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[梯度下降]]></title>
      <url>/blog/2018/07/12/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D</url>
      <content type="text"><![CDATA[梯度下降error函数的图像，看上去像个碗一样，中间是凹的，两边翘起。这个碗的最低点，也就是 error(x) 的最小值，就是我们要寻找的点。我们发现:当x在最小值左边的时候，error函数的导数（斜率）是负的；当x在最小值右边的时候，导数是正的；当x在最小值附近的时候，导数接近0.因此，如果我们在：导数为负的时候增加x；导数为正的时候减小x；x = x - derivative * alpha解释：derivative 是 error 在 x 处的导数，这里是用导数的定义求的。x = x - derivative 这就是“导数下降”名称的由来。通过不断地减去导数，x最终会到达函数的最低点。alpha 参数控制的是点 x 逆着导数方向前进的距离，alpha 越大，x 就会前进得越多，误差下降得越快。alpha 太小会导致下降缓慢，alpha 太大会导致 x 冲得太远，令函数无法收敛。]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 高数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[导 基本概念]]></title>
      <url>/blog/2018/07/12/%E5%AF%BC</url>
      <content type="text"><![CDATA[求导极限微分几何意义凹凸拐点驻点函数在一点处的一阶导数为零，该点即函数的驻点（Stationary Point）或稳定点极值偏导数]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 高数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[性能分析工具]]></title>
      <url>/blog/2018/07/08/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7</url>
      <content type="text"><![CDATA[Python性能分析指南性能分析实践分析一个程序的性能可以归结为回答4个基本的问题：1.它运行的有多块？2.那里是速度的瓶颈？3.它使用了多少内存？4.哪里发生了内存泄漏？Unix time 命令/usr/bin/time -p python julia1_nopil.py/usr/bin/time --verbose python julia1_nopil.pytimeitpython -m timeit -n 5 -r 5对于要长期运行的代码来说，最好指定循环次数(-n 5)以及重复次数(-r 5)。timeit 会对语句循环执行 n 次并计算平均值作为一个结果，重复 r 次并选出最好的那个结果time.time()分析器逐行统计时间和执行频率 line_profiler 库@profile装饰器kernprof -l -v fib.py-l选项通知kernprof注入@profile装饰器到你的脚步的内建函数，-v选项通知kernprof在脚本执行完毕的时候显示计时信息。上述脚本的输出看起来像这样：寻找具有高Hits值或高Time值的行。这些就是可以通过优化带来最大改善的地方。内存分析 memory_profiler 库@profile装饰器python -m memory_profiler primes.py内存泄漏 objgraph 库显示占据python程序内存的头N个对象显示一段时间以后哪些对象被删除活增加了在我们的脚本中显示某个给定对象的所有引用]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[风险]]></title>
      <url>/blog/2018/07/06/%E9%A3%8E%E9%99%A9</url>
      <content type="text"><![CDATA[单投资风险 标准差 = $\sqrt{\mathrm{方差}}$$\sigma\;=\;std(\mathrm{组合收益率-基准收益率})$N 股等权重投资 风险都是$\sigma$风险 = $\style{font-size:36px}{\sqrt{N(\frac\sigma N)^2}}$= $\style{font-size:36px}{\frac\sigma{\sqrt N}}$  风险随预测期长度的平方根增长$\sigma_年\;=\;\sqrt{12}\;\sigma_月$残差风险基本风险model计算协方差矩阵N =&gt; N(N-1)/2多因子风险model结构化风险模型挑选因子  有区分度 直观理解公认 有意义可解析收益表现因子分类  对外部变化响应因子宏观因子 基于历史数据通过回归、统计得到（有误差，不能反映当时）  横截面比较类因子股票属性 基本面(分红。。) 市场类(收益,波动率。。。)  统计因子常用因子  行业因子衡量不同行业行为差异  风险指数行业维度外不同股票群体之间的差异]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[运行流程]]></title>
      <url>/blog/2018/07/06/%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B</url>
      <content type="text"><![CDATA[当解释器通过字节码时，它会定期放弃GIL，而不需要经过正在执行代码的线程允许，这样其他线程便能运行。  默认情况下，检测间隔是1000 字节码。所有线程都运行相同的代码，并以相同的方式定期从他们的锁中抽出。在 Python 3 GIL的实施更加复杂，检测间隔不是一个固定数目的字节码，而是15 毫秒。然而，对于你的代码，这些差异并不显著。Python很多原子操作线程不会被打断Python 的标准 dis 模块编译的字节码&gt;&gt;&gt; import dis&gt;&gt;&gt; dis.dis(foo)LOAD_GLOBAL              0 (n)LOAD_CONST               1 (1)INPLACE_ADDSTORE_GLOBAL             0 (n)协同式多任务处理当一项任务比如网络 I/O启动，而在长的或不确定的时间，没有运行任何 Python 代码的需要，一个线程便会让出GIL，从而其他线程可以获取 GIL 而运行 Python[线程主动出让GIL]。这种礼貌行为称为协同式多任务处理，它允许并发；多个线程同时等待不同事件。def do_connect():    s = socket.socket()    s.connect(('python.org', 80))  # drop the GILfor i in range(2):    t = threading.Thread(target=do_connect)    t.start() 两个线程在同一时刻只能有一个执行 Python ，但一旦线程开始连接，它就会放弃 GIL ，这样其他线程就可以运行。简而言之：当N个线程在网络 I/O 堵塞，或等待重新获取GIL，而一个线程运行Python。  [一人工作多人围观]抢占式多任务处理]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[资本资产定价模型 CAPM 一致预期收益]]></title>
      <url>/blog/2018/07/05/%E8%B5%84%E6%9C%AC%E8%B5%84%E4%BA%A7%E5%AE%9A%E4%BB%B7%E6%A8%A1%E5%9E%8B-CAPM-%E4%B8%80%E8%87%B4%E9%A2%84%E6%9C%9F%E6%94%B6%E7%9B%8A</url>
      <content type="text"><![CDATA[股票收益率 = 市场收益率 + 残差收益率 （市场相关 市场不相关）$r_{i}\left ( t \right )= \beta_{i}\ast r_{m}\left ( t \right ) + \alpha_{i}\left ( t \right )$残差收益率 期望为0个股收益和大盘呈线性关系残差风险自负被动式风险低最优]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 主动投资组合管理 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mitmproxy   Appium]]></title>
      <url>/blog/2018/06/25/spider</url>
      <content type="text"><![CDATA[mitmproxyq 退出e 编辑 esc 退出编辑 a 新增一行 tab 换列截获的数据保存到文件mitmdump -w outfile本来处理截获的数据mitmdump -s script.pyAPIAppiumadb devices -1* daemon not running. starting it now on port 5037 ** daemon started successfully *85528347    device usb:336592896X product:OnePlus5 model:ONEPLUS_A5000 device:OnePlus5Appium Python ClientAppium 配置 SessionplatformName 平台名称，需要区分 Android或 iOS，此处填写 AndroiddeviceName 设备名称，此处是手机的具体类型 modelappPackage: 它是App程序包名 adb shell pm list packageappActivity: 它是入口 Activity名，这里通常需要以开头。adb连接情况adb devices -l获取包名和入口类名adb shell dumpsys window windows | grep -E 'mCurrentFocus'mCurrentFocus=Window{964c752 u0 com.gxb.wallet.app/com.gxb.sdk.activity.WalletHomeActivity}adb shell dumpsys activity | grep mFocusedActivity解压 apk 查看AndroidManifest.xml]]></content>
      <categories>
        
          <category> tools </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-网络]]></title>
      <url>/blog/2018/06/17/%E7%BD%91%E7%BB%9C</url>
      <content type="text"><![CDATA[主机绑定端口-P 标记时，会随机映射一个本地主机的49000~49900的端口到内部容器开放的网络端口。-p 以指定要映射的端口,在一个指定端口上只可以绑定一个容器docker run -d -p 5000:5000  -p 3000:80 training/webapp py thon app.pyip:hostPort:containerPort | ip::containerPort | hostPort:containerPorthostPort:containerPort会绑定本地所有接口上的所有地址ip:hostPort:containerPort映射到指定地址的指定端口ip::containerPort映射到指定地址的任意端口使用 docker port 来查看当前映射的端口配置，也可以查看到绑定的地址docker port &lt;container_name or id&gt; [port]容器互联–linksudo docker run -d -P --name web --link db:db training/webapp python app.py–link name:alias ，其中 name 是要链接的容器的 名称， 是这个连接的别名。docker ps 来查看容器的连接 NAMES web/db web 容器将被允许访问 db 容器高级Docker 启动在主机上创建一个docker0虚拟网桥，随机分配一个本地未占用的私有网段(在 RFC1918 中定义)中的一个地址给docker0接口，创建了在主机和所有容器之间一个虚拟共享网络]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-数据卷]]></title>
      <url>/blog/2018/06/17/%E6%95%B0%E6%8D%AE%E5%8D%B7</url>
      <content type="text"><![CDATA[数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性：  数据卷可以在容器之间共享和重用  对数据卷的修改会立马生效  对 数据卷 的更新，不会影响镜像  数据卷 默认会一直存在，即使容器被删除在用 docker run 命令的时候，使用 -v 标记来创建一个数据卷并挂载到容器 里。在一次 run 中多次使用可以挂载多个数据卷。也可以在 Dockerfile 中使用 VOLUME 来添加一个或者多个新的卷到由该镜 像创建的任意容器如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使 用 docker rm -v 这个命令docker run -d -P --name web -v /src/webapp:/opt/webapp[:ro]上面的命令加载主机的/src/webapp目录到容器的/opt/webapp目录。这个功能在进行测试的时候十分方便,本地目录的路径必须是绝对路径，如果目录不存在 Docker 会自动为你创建它。通过 :ro 指定为只读docker inspect web 查看数据卷的具体信息数据卷容器维护]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-容器管理]]></title>
      <url>/blog/2018/06/17/%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86</url>
      <content type="text"><![CDATA[启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止 状态(stopped)的容器重新启动。docker run -it ubuntu:14.04 /bin/bash  -t 选项让Docker分配一个伪终端(pseudo-tty)并绑定到容器的标准输入上   -i 让容器的标准输入保持打开。  -d Docker在后台运行而不是直接把执行命令的结果输出在当前宿主机下  --name 容器自定义命名run的操作  检查本地是否存在指定的镜像，不存在就从公有仓库下载  利用镜像创建并启动一个容器  分配一个文件系统，并在只读的镜像层外面挂载一层可读写层  从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去  从地址池配置一个ip地址给容器  执行用户指定的应用程序  执行完毕后容器被终止获取container信息可以查看所有已经创建的包括终止状态的容器docker ps -adocker logs [container ID or NAMES]可以利用 docker start 命令，直接将一个已经终止的容器启动运行。可以使用docker stop来终止一个运行中的容器。此外， docker restart命令会将一个运行态的容器终止，然后再重新启动它。容器删除docker rm [container ID or NAMES]如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器docker rm $(docker ps -a -q) 清理所有处于终止状态的容器快照导出容器快照到本地文件docker export [container ID] &gt; ubuntu.tar从容器快照文件中再导入为镜像cat ubuntu.tar | sudo docker import - test/ubuntu:v1.0]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[凯利公式]]></title>
      <url>/blog/2018/06/16/%E5%87%AF%E5%88%A9%E5%85%AC%E5%BC%8F</url>
      <content type="text"><![CDATA[f =（bp-q）/ bbp-q代表“赢面”f = 应投注的资本比值p = 获胜的概率（也就是抛硬币正面的概率）q = 失败的概率，即1 - p（也就是硬币反面的概率）b = 赔率，等于期望盈利 ÷可能亏损（也就是盈亏比）  期望值（bp-q）为0时，赌局为公平游戏，这时不应下任何赌注。  期望值（bp-q）为负时，赌徒不具备任何优势，也不应下任何赌注。  期望值（bp-q）为正时，这时按照凯利公式投注赚钱最快，风险最小。]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率概念]]></title>
      <url>/blog/2018/06/15/%E6%A6%82%E7%8E%87%E6%A6%82%E5%BF%B5</url>
      <content type="text"><![CDATA[有序 排列$A_m^n=\frac{m!}{(m-n)!}$无序 组合$C_m^n=\frac{m!}{n!(m-n)!}$等可能概型条件概率$P(B\vert A)$ 事件A发生下事件B发生的概率$P(B\vert A)\;=\frac{P(AB)}{P(A)}$$P(B)$ 先验概率 由以往数据分析得到$P(B\vert A)$ 后验概率 得到信息后重新修正$P(A)\;=\;\sum_i^nP(A\vert B_i)P(B_i)$结合全概率和条件概率 用于A B 反求相互独立]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-镜像管理]]></title>
      <url>/blog/2018/06/15/%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86</url>
      <content type="text"><![CDATA[获取镜像  docker search &lt;仓库名&gt;  docker pull [Docker Registry地址]&lt;仓库名&gt;:&lt;标签&gt;  docker pull ubuntu:14.04  docker pull --help列  #只会显示顶层镜像docker images#根据仓库名列出镜像docker images [选项] [REPOSITORY][:TAG]docker images ubuntu过滤在 mongo:3.2 之后建立的镜像docker images -f since=mongo:3.2docker images -f before=mongo:3.2特定格式显示docker images --format ": "docker images --format "table \t\t"返回 images iddocker images -q [REPOSITORY][:tag]虚悬镜像镜像既没有仓库名，也没有标签，均为 &lt;none&gt;  这类无标签镜像也被称为dangling image  虚悬镜像已经失去了存在的价值，是可以随意删除的  列  docker images -f dangling=true    虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除    docker rmi $(docker images -q -f dangling=true)    中间层镜像为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段 时间后，可能会看到一些依赖的中间层镜像只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除  docker images -a删除本地image并非所有的 docker rmi 都会产生删除镜像的行为，有可 能仅仅是取消了某个标签而已  docker images  docker rmi [选项] &lt;镜像ID1&gt; [&lt;镜像ID2&gt; ...]  docker rmi &lt;仓库名&gt;:&lt;标签&gt;]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-镜像构建]]></title>
      <url>/blog/2018/06/15/%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA</url>
      <content type="text"><![CDATA[镜像是多层存储，每一层是在前一层的基础上进行的修改；容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]慎用 docker commit 臃肿 黑箱docker diff &lt;container_name&gt;命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。使用 Dockerfile 定制镜像Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建      FROM 就是指定基础镜像，因此一个Dockerfile中FROM是必备的指令，并且必须是第一条指令        RUN 指令是用来执行命令行命令的。shell 格式：RUN &lt;命令&gt;exec 格式：RUN ["可执行文件", "参数1", "参数2"]    压缩 run 行数    多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写Shell脚本，而是在定义每一层该如何构建。    清理工作    删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件等 因此镜像构建时，一定要确保每一层只添加真正需要 添加的东西，任何无关的东西都应该清理掉。    Dockerfile 支持 Shell 类的行尾添加 \ 的 命令换行方式，以及行首 # 进行注释的格式。    COPYCOPY &lt;源路径&gt;... &lt;目标路径&gt;     可以是容器内的绝对路径，也可以是相对于工作目录的相对路径(工 作目录可以用 `WORKDIR` 指令来指定)。目标路径不需要事先创建，如果目录不存 在会在复制文件前先行**创建缺失目录**        ADD 比COPY高级所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD        CMD 启动时运行的命令。shell 格式：CMD &lt;命令&gt;exec 格式：CMD ["可执行文件", "参数1", "参数2"]    容器中的应用都应该以前台执行，而不是像虚拟机、物理机 里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。    对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。    CMD service nginx start =&gt; CMD ["nginx", "-g", "daemon off;"]  docker run ubuntu cat /etc/os-release  ENTRYPOINT&lt;ENTRYPOINT&gt; "&lt;CMD&gt;"FROM ubuntu:16.04RUN apt-get update \    &amp;&amp; apt-get install -y curl \    &amp;&amp; rm -rf /var/lib/apt/lists/*CMD [ "curl", "-s", "http://ip.cn" ]ERROR : docker run myip -idocker run myip curl -s http://ip.cn -i当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。FROM ubuntu:16.04RUN apt-get update \    &amp;&amp; apt-get install -y curl \    &amp;&amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ "curl", "-s", "http://ip.cn" ]ENTRYPOINT ["docker-entrypoint.sh"]CMD [ "redis-server" ]redis-server 为 .sh 的参数  ENV    ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...        例如    ENV VERSION=1.0 DEBUG=on \NAME="Happy Feet"  用$VERSION 引用        VOLUME     VOLUME ["&lt;路径1&gt;", "&lt;路径2&gt;"...]VOLUME &lt;路径&gt;        我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据        EXPOSE 运行时容器提供服务端口只是一个声明，在运行时并不 会因为这个声明应用就会开启这个端口的服务。    要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。 -p ，是 映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访 问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行 端口映射。    WORKDIR 指定工作目录以后各层的当前目录就被改为指定的目录，该目录需要已经存在      HEALTHCHECK    ONBUILD 继承父image构建image在 Dockerfile 文件所在目录执行docker build -t &lt;仓库名&gt;:&lt;new_标签&gt; &lt;上下文路径/URL/-&gt;镜像构建上下文（Context) 当执行COPY or ADD 时 本地文件路径，构建时要复制需要的文件到上下文目录如果目录下有些东西 确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一 个 .dockerignore]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[docker-基本概念]]></title>
      <url>/blog/2018/06/15/docker</url>
      <content type="text"><![CDATA[Docker 和传统虚拟化方式的不同容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。容器化  灵活：即使是最复杂的应用也可以集装箱化。  轻量级：容器利用并共享主机内核。  可互换：您可以即时部署更新和升级。  便携式：您可以在本地构建，部署到云，并在任何地方运行。  可扩展：您可以增加并自动分发容器副本。  可堆叠：您可以垂直和即时堆叠服务。Docker 三个基本概念  镜像（Image）  容器（Container）  仓库（Repository）镜像Linux 而言，内核启动后，会挂载root文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统镜像包含操作系统完整的root文件系统，其体积往往是庞大的。分层存储，由多层文件系统联合组成。镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。删除当前层不是真删除，该文件会一直跟随镜像每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。  镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。容器容器可以被创建、启动、停止、删除、暂停镜像(Image)和容器(Container)的关系，就像是面向对象程序设计中的 类 和 实例 一样容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间容器内的进程是运行在一个隔离的环境里容器运行时 以镜像为基础层 上面建立当前容器存储层(为容器运行时读写而准备)容器消亡时，容器存储层也随之消亡所有的文件写入操作，都应该使用数据卷(Volume)或者绑定宿主目录。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。仓库一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以**通过 : 的格式来指定具体是这个软件哪个版本的镜像**。命名空间是 Linux 内核一个强大的特性。每个容器都有自己单独的命名空间，运行 在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼互不影响。]]></content>
      <categories>
        
          <category> docker </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[git 操作]]></title>
      <url>/blog/2018/06/07/git-command-cmd</url>
      <content type="text"><![CDATA[删除远程分支git push origin --delete &lt;branchName&gt;git show &lt;tag-name&gt;git tag                            # see tag listsgit push origin &lt;tag-name&gt;         # push a single taggit push --tags                    # push all local tagsgit push origin --delete tag &lt;tagname&gt;如何在 Git 里撤销(几乎)任何操作新增git remote add origin &lt;address&gt;重设git remote set-url origin &lt;new-address&gt;重设committar sha 的上一个git rebase -i &lt;earlier SHA&gt;要丢弃一个 commit，只要在编辑器里删除那一行就行了。修改的 pick 替换为 r合并 pick 替换为 f 或者 s]]></content>
      <categories>
        
          <category> git </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GIL]]></title>
      <url>/blog/2018/06/06/GIL</url>
      <content type="text"><![CDATA[Python 运行流程  编译成字节码  解析器读取字节码 执行指令全局解释器锁保证同一时刻只有一个线程对共享资源进行存取线程切换协同式多任务处理一个线程无论何时开始睡眠或等待网络 I/O，其他线程总有机会获取 GIL 执行 Python 代码。这是协同式多任务处理。当一项任务比如网络 I/O启动，而在长的或不确定的时间，没有运行任何 Python 代码的需要，一个线程便会让出GIL，从而其他线程可以获取 GIL 而运行 Python[线程主动出让GIL]。这种礼貌行为称为协同式多任务处理，它允许并发；多个线程同时等待不同事件。抢占式多任务处理如果一个线程不间断地在 Python 2 中运行 1000 字节码指令，或者不间断地在 Python 3 运行15 毫秒，那么它便会放弃 GIL，而其他线程可以运行。当解释器通过字节码时，它会定期放弃GIL，而不需要经过正在执行代码的线程允许，这样其他线程便能运行diff 程序主动  系统主动  协作式多任务处理下一个进程被调度的前提是当前进程主动放弃时间片；  抢占式多任务处理操作系统完全决定进程调度方案，操作系统可以剥夺耗时长的进程的时间片，提供给其它进程。线程安全Python中的线程安全 许多 Python 操作是原子的尽管有 GIL，你仍然需要加锁来保护共享的可变状态， 始终围绕共享可变状态的读取和写入加锁。毕竟，在 Python 中获取一个 threading.Lock 是廉价的  使用线程进行并发 I/O 操作，在进程中进行并行计算]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Hello world]]></title>
      <url>/blog/2018/06/06/hello</url>
      <content type="text"><![CDATA[Hello world. I’m comingJekyll + NexT主题 + 自定义小修改配置教程测试命令bundle exec jekyll server]]></content>
      <categories>
        
          <category> 生活 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python 正则]]></title>
      <url>/blog/2018/06/03/%E6%AD%A3%E5%88%99</url>
      <content type="text"><![CDATA[import re#返回pattern对象re.compile(string[,flag])#以下为匹配所用函数re.match(pattern, string[, flags])re.search(pattern, string[, flags])re.split(pattern, string[, maxsplit])re.findall(pattern, string[, flags])re.finditer(pattern, string[, flags])re.sub(pattern, repl, string[, count])re.subn(pattern, repl, string[, count]) re.I(全拼：IGNORECASE): 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图） re.S(全拼：DOTALL): 点任意匹配模式，改变'.'的行为 re.L(全拼：LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定 re.U(全拼：UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。总结：尽量使用.*?匹配使用()获取匹配目标有换行用re.S尽量用re.search 扫描整个字符串返回第一个匹配 不用match因为pattern参数里面第一个不匹配就直接None]]></content>
      <categories>
        
          <category> 正则 </category>
        
      </categories>
      <tags>
        
          <tag> regex </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[计算机网络]]></title>
      <url>/blog/2018/06/02/computer-network</url>
      <content type="text"><![CDATA[体系结构物理层数据链路层网络层运输层三次握手四次挥手应用层]]></content>
      <categories>
        
          <category> network </category>
        
      </categories>
      <tags>
        
          <tag> network </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[奇技淫巧]]></title>
      <url>/blog/2018/06/02/%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7</url>
      <content type="text"><![CDATA[百度云盘播放加速 Chrome在 console 中2 倍加速videojs.getPlayers("video-player").html5player.tech_.setPlaybackRate(2)还原videojs.getPlayers("video-player").html5player.tech_.setPlaybackRate(1)VPN GFW TEST  Test Bandwagon URL for GFW   换IP教程md img size缩放&lt;img src="" style="zoom:40%"/&gt;blog 多图&lt;span class='gp-2'&gt;    &lt;img src='' /&gt;    &lt;img src='' /&gt;&lt;/span&gt;&lt;img src='' class="full-image"/&gt;code免费自定义js css_sass/_custom/custom.scss_includes/_third-party/custom.html内联连接validation]]></content>
      <categories>
        
          <category> 奇技淫巧 </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[数据预处理]]></title>
      <url>/blog/2018/05/26/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86</url>
      <content type="text"><![CDATA[清洗集成变换规约#缺失值处理#异常值处理#数据集成  统一不同数据源的矛盾    冗余消除  #数据变换  简单函数变换 对数 差分  归一化     #连续属性离散化#属性构造#数据规约#属性规约#数值规约]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[风险评价指标]]></title>
      <url>/blog/2018/05/25/%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87</url>
      <content type="text"><![CDATA[$\mathrm{年化收益率}=({\textstyle\frac{\mathrm{策略前总资产}}{\mathrm{策略后总资产}}})^{(\mathrm{一年交易天数}/\mathrm{回测日天数})}-1$Alpha反映策略投资能力 投资收益与市场收益无关example 投资收益12% 基准10% alpha 就是增值部分2%$Alpha\;=\;\mathrm{策略年化收益率}-(\mathrm{无风险收益率}+Beta(\mathrm{基准年化收益率}-\mathrm{无风险收益率}))$Beta 反映策略对大盘敏感程度 策略风险$Beta\;=\;\frac{Cov(\mathrm{策略收益率},\mathrm{基准收益率})}{var(\mathrm{基准收益率})}$波动率 volatility越大风险高$volatility\;=\;\sqrt{\frac{\mathrm{一年交易天数}}{\mathrm{回测日天数}-1}\sum_{i-1}^n(\mathrm{策略收益率}-\overline{\mathrm{策略收益率}})^2}$夏普率(sharpe ratio)策略额外承担一单位的风险，可以获得多少单位的收益作为补偿最大回撤]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[量化规则]]></title>
      <url>/blog/2018/05/25/%E9%87%8F%E5%8C%96%E8%A7%84%E5%88%99</url>
      <content type="text"><![CDATA[量化模块选股 择时 风控避免频繁短线 逆势 重仓让利润奔跑 尽快止损 坚持交易策略执行定量交易系统由四个主要部分组成：  策略识别 - 寻找策略，利用优势并决定交易频率  战略回测 - 获取数据，分析战略绩效并消除偏差  执行系统 - 链接到经纪商，实现交易自动化并最小化交易成本  风险管理 - 最佳资本配置，“赌注规模”/凯利准则和交易心理学量化 &amp; 人工 区别控制 买卖点，头寸大小阿尔法收益 = 收益 - 基准收益]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化黑箱 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[时序分析]]></title>
      <url>/blog/2018/05/25/%E6%97%B6%E5%BA%8F%E5%88%86%E6%9E%90</url>
      <content type="text"><![CDATA[收益从t-k 到 t 共k周期 简单收益率—弱平稳性弱平稳 序列{rt} 均值与cov(r1,r2)（相同周期内） var 不随时间改变非平稳通过d次差分（$r_t-r_{t-1}$）近似平稳自相关ACF检验自相关混成检验(Portmanteau Test)白噪声自回归模型AR(1)AR(p)$r_t\;=\;\Phi_0+\Phi_1r_{t-1}+\dots+\Phi_pr_{t-p}+a_t$AR性质$E(r_t)\;=\;\frac{\Phi_0}{1-\Phi_1-\dots-\Phi_p}$${\rho_l\;}=\;\Phi_1\rho_{l-1}+\Phi_2\rho_{l-2}+\dots+\Phi_p\rho_{l-p}$$\because$  B是向后推移算子 $B\rho_l\;=\;\rho_{l-1}$$\therefore 1-\Phi_1B-\Phi_2B^2-\dots\Phi_pB^p\;=\;0$$\therefore 1-\Phi_1x-\Phi_2x^2-\dots\Phi_px^p\;=\;0$$ if \left|x\right|&gt;1 序列{\;r_t\;}平稳 解的倒数为特征根$所以所有特征根的模 &lt; 1定阶p是未知偏相关函数（PACF）p步截尾信息准则  （AIC）适合的p使得aic最小贝叶斯信息准则 BIC##拟合优度$R^2\;=\;1-\frac{var(\mathrm{残差})}{var(r_t)}$—$0\leqslant R^2\;\leqslant1$$R^2越大模型拟合越好$预测h是预测原点 l是预测步长$r_{h+l}\;=\;\Phi_0+\Phi_1r_{h+l-1}+\dots+\Phi_pr_{h+l-p}+a_{h+l}$—移动平均MAMA总是弱平稳定阶acf q步截尾(q阶后自相关系数=0)ARMAp=0 =&gt; MA所以所有特征根的模 &lt; 1 才平稳EACF AICARIMA价格序列通常是非平稳的 对数收益率是平稳的单位根非平稳序列ARIMA =&gt; AR特征根=1单位根非平稳序列 差分若序列$y_t$ d重单位根非平稳序列 要d次差分（前-后) =&gt; d阶差分序列$y_t$是ARIMA(p,d,q)过程ADF 检验单位根原假设是有单位根 非平稳]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 时序 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[时序分析 statsmodels]]></title>
      <url>/blog/2018/05/24/%E6%97%B6%E5%BA%8F%E5%88%86%E6%9E%902</url>
      <content type="text"><![CDATA[是否随机游走 非平稳独立的难以预测（投资组合）$dx_t=\theta(\mu-x_t)dt\;+\;\sigma dW_t$θ 回归mean的速率μ meanσ 方差W 布朗运动价格波动 与 均值和现价的差成正比 + 随机噪声工具：判断平稳ADF Testtest unit root in autoregressive time series if γ = 0 非平稳ADF检验的原假设是存在单位根，只要这个统计值是小于1%水平下的数字就可以极显著的拒绝原假设（不含单位根），认为数据平稳。注意，ADF值一般是负的，也有正的，但是它只有小于1%水平下的才能认为是及其显著的拒绝原假设。P-VALUE显著性差异P,显著性水平αP &lt; α  拒绝原假设 具备显著性差异的可能性为（100%—α)一般以P &lt; 0.05 为显著， P &lt; 0.01 为非常显著hurst exponent投资组合APT stock 之间有相关性证明pair trade 线性回归模型合理性 检验是否高斯噪声 -&gt; stock x, y之间是平稳的残差 = y-βx 检验残差是非平稳 -&gt; x,y 平稳]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 时序 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率-抽样&分布]]></title>
      <url>/blog/2018/05/23/%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83</url>
      <content type="text"><![CDATA[统计量分布=&gt;抽样分布卡方分布t 分布F 分布]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率-假设]]></title>
      <url>/blog/2018/05/23/%E5%81%87%E8%AE%BE</url>
      <content type="text"><![CDATA[模型参数未知 推断样本特性接受假设 假设是对的显著性检验:当假设为真不接受的概率 &lt;= a 接受假设检验统计量 &gt;= 显著性水平 1-a 差异性显著 拒绝假设步骤如下置信区间&amp;假设P值检验理解]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率-参数估计]]></title>
      <url>/blog/2018/05/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1</url>
      <content type="text"><![CDATA[点估计分布函数已知 其中若干参数待定 通过样本估计参数值      矩估计法         最大拟然估计          离散型        连续型      估计评选标准  无偏  有效  相合区间估计估计值的在某区间的可信程度1-a 区间越短越好  置信区间      求置信区间的方法    单侧置信区间]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[概率-常用]]></title>
      <url>/blog/2018/05/21/%E6%A6%82%E7%8E%87%E5%B8%B8%E7%94%A8</url>
      <content type="text"><![CDATA[期望$E(x)\;=\;\sum_{k=1}^\infty x_kP_k$ p是k发生的概率方差协方差 相关系数协方差矩阵向量协方差矩阵]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 概率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[NN基础]]></title>
      <url>/blog/2018/05/18/NN</url>
      <content type="text"><![CDATA[求W损失函数  分类交叉熵  回归MSE反向传播 backward propagation方便求导求 cost的min梯度下降SGD 不断修正W$W\;=\;W\;-\;\frac{\partial\;cost}{\partial W}$Example :目标求$\frac{\partial\;P}{\partial W_2},\frac{\partial\;P}{\partial W_1}$链式偏导$\frac{\partial\;P}{\partial W_2} = \frac{\partial\;P}{\partial Z}*\frac{\partial\;Z}{\partial W_2}$ 恒等变换整理一下从后往前推导微分当网络复杂时 偏导路径 很多反向传播: 重复偏导的地方 重复使用不再算 省时间（链式偏导+动态规划）优化NNNN 结果对W初始值敏感 一般选随机复杂度 神经网络中神经元的个数,权值的数量D O(VD) dNN梯度消失激活函数sigmoid将负无穷到正无穷的数映射到0和1之间神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。此深层网络的学习就等价于只有后几层的浅层网络的学习梯度爆炸初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了  网络太深 -&gt; 因为梯度反向传播中的连乘效应 用ReLU取代sigmoidExample正向传播（Forward Propagation）J(a,b,c)=3(a+bc)反向传播 (Back Propagation)即计算输出对输入的偏导数首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数。则利用求导技巧，可以得到：根据这种思想，然后计算J对参数b的偏导数。从计算图上来看，从右到左，J是v的函数，v是u的函数，u是b的函数。可以推导：最后计算J对参数c的偏导数。仍从计算图上来看，从右到左，J是v的函数，v是u的函数，u是c的函数。可以推导：]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> nn基础 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[start]]></title>
      <url>/blog/2018/05/17/start</url>
      <content type="text"><![CDATA[  创建一个graph，  TensorFlow 会话(Session)负责处理在诸如 CPU 和 GPU 之类的设备上的操作并运行它们，并且它保留所有变量值。常量a = tf.constant(5)b = 5 + ac = a + bwith tf.Session() as sess:    result = sess.run(c)    print(result)10变量训练过程中会不断优化 当值需要在会话中更新时，我们使用可变张量x = tf.Variable(1, name="x")y = tf.Variable(2, name="y")f = (x + y) * a# 使用一个Variable初始化另一个Variableweights = tf.Variable(tf.random_normal([100,100],stddev=2))weight2=tf.Variable(weights.initialized_value(), name='w2')global_variables_initializer必须运行在所有Variable声明之后 不然报错 fuck session 和 变量初始化不能搞在一起吗?# 初始化变量init_var = tf.global_variables_initializer()with tf.Session() as sess:    sess.run(init_var)    """    想要高效地评估f和c，在一次图形运行中评估f和c,所有节点值都在图运行之间删除，除了变量值, b会重复求值    单进程TensorFlow中,每个会话都有自己的每个变量副本 多个会话不共享任何状态，即使它们重复使用同一个图。    变量在其初始化程序运行时启动其生命周期，并且在会话关闭时结束。    """    con_, var_ = sess.run([f, c])    print(con_, var_)15 10assign变量的再赋值  Difference between tf.assign and assignment operator (=)scopeget_variable and Variable  tf.Variable 必须有初始值  tf.Variable 始终创建一个新变量，已存在具有此类名称的变量，则可能会为变量名称添加后缀  tf.get_variable 从graph中获取具有这些参数的现有变量，如果它不存在，它将创建一个新变量,存在的话会报错get_variable初始化相关from pprint import pprinttf.Variable(0, name="a")  # name == "a:0"tf.get_variable(initializer=0, name="a")  # name == "a_1:0"tf.Variable(0, name="a")  # name == "a_2:0"b = tf.get_variable(initializer=0, name="b")  # name == "b:0"c = tf.get_variable(name="c", shape=1)  # name == "c:0"pprint(tf.trainable_variables())init = tf.global_variables_initializer()with tf.Session() as sess:    sess.run(init)    print(b.eval())    print(c.eval())name_scope and variable_scope  两个scope对使用tf.Variable创建的变量具有相同的效果，即范围将作为操作或变量名称的前缀添加。  tf.variable_scope就不一样了with tf.variable_scope('var'):    with tf.name_scope("name"):        v1 = tf.get_variable("var1", [1], dtype=tf.float32)        v2 = tf.Variable(1, name="var2", dtype=tf.float32)        a = tf.add(v1, v2)print(v1.name)  # var/var1:0print(v2.name)  # var/name/var2:0print(a.name)  # var/name/Add:0scope reuse 共享变量 tf.variable_scope 和 tf.get_variable 搭配with tf.variable_scope('bar'):    foo1 = tf.get_variable(name="a", shape=1)  # name == "bar/a:0"    foo2 = tf.get_variable(name="a", shape=1)  # ValueError: Variable bar/a already existswith tf.variable_scope('foo', reuse=tf.AUTO_REUSE):    foo1 = tf.get_variable(name="a", shape=1)  # name == "foo/a:0"with tf.name_scope("bar"):    with tf.variable_scope("foo", reuse=True):        v1 = tf.get_variable("a")        assert foo1 == v1pprint(tf.trainable_variables())占位符它们通常用于在训练期间将训练数据传递给TensorFlow 实际上不执行任何计算A = tf.placeholder(tf.float32, shape=(None, 3))B = A + 5with tf.Session() as sess:    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})print(B_val_1)print(B_val_2)[[6. 7. 8.]][[ 9. 10. 11.] [12. 13. 14.]]dtypedtype 转换b = tf.Variable(tf.random_uniform([5,10], 0, 2, dtype= tf.int32))b_new = tf.cast(b, dtype=tf.float32)# tensortf.convert_to_tensor()save load默认情况下，保存器将以自己的名称保存并还原所有Variable，但如果需要更多控制，则可以指定要保存或还原的变量以及要使用的名称saver = tf.train.Saver()"""指定保存的变量saver = tf.train.Saver({"weights": theta})"""with tf.Session() as sess:     save_path = saver.save(sess, "/tmp/my_model.ckpt")with tf.Session() as sess:     saver.restore(sess, "/tmp/my_model_final.ckpt")# 导入TensorFlow训练好的modelsaver = tf.train.import_meta_graph('mnist_dcgan-60000.meta')saver.restore(sess, tf.train.latest_checkpoint(''))graph = tf.get_default_graph()g = graph.get_tensor_by_name('generator/g/Tanh:0')noise = graph.get_tensor_by_name('noise:0')is_training = graph.get_tensor_by_name('is_training:0')n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})tensor board正在定期保存检查点 可视化训练进度 checkpointtf.summary.histogram('D_X/true', self.D_X(x))一般用来显示训练过程中变量的分布情况tf.summary.scalar('loss/cycle', cycle_loss)一般在画loss,accuary时会用到这个函数tf.summary.image('X/generated', utils.batch_convert2int(self.G(x)))# Merge all the summaries and write them out to the summaries_dir# 直接run(merged,feed_dict=**kwargs) 非常方便# merged = tf.summary.merge_all()# graph = sess.graph or tf.get_default_graph()file_writer = tf.summary.FileWriter(checkpoints_dir, graph)mse_summary = tf.summary.scalar('MSE', mse)with tf.Session() as sess:    for i in step:        s = sess.run(mse_summary)        file_writer.add_summary(s, i)        # file_writer.flush()file_writer.close()必须是完整路径 fuck 不能相对路径   Pycharm 有Copy Pathtensorboard --logdir path/to/log-directorydevice"/cpu:0" for the CPU devices and "/gpu:I" for the  GPU device  log_device_placement=True 验证TensorFlow确实使用指定的设备,打log  allow_soft_placement=True 如果您不确定该设备并希望TensorFlow选择现有和支持的设备GPU比CPU快得多，因为它们有许多小内核。然而，就计算速度而言，将GPU用于所有类型的计算并不总是有利的。与GPU相关的开销有时在计算上比GPU提供的并行计算的优势更昂贵。为了解决这个问题，TensorFlow提供了在特定设备上进行计算的条款。默认情况下，如果CPU和GPU都存在，TensorFlow优先考虑GPU。config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)# 手动选择设备with tf.device('/cpu:0'):    rand_t = tf.random_uniform([50,50], 0, 10, dtype=tf.float32, seed=0)    a = tf.Variable(rand_t)    b = tf.Variable(rand_t)    c = tf.matmul(a,b)    init = tf.global_variables_initializer()with tf.Session(config=config) as sess:    ....optimizerslearning_rate传入初始lr值，global_step用于逐步计算衰减指数，decay_steps用于决定衰减周期，decay_rate是每次衰减的倍率，staircase若为False则是标准的指数型衰减，True时则是阶梯式的衰减方法，目的是为了在一段时间内（往往是相同的epoch内）保持相同的learning rate其实可以直接使用一些改变learning_rate的变化的AdamOptimizer之类的decay every 100000 steps with a base of 0.96# 衰减learning_rateglobal_step = tf.Variable(0, trainable=False)initial_learning_rate = 0.2learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps=100000, decay_rate=0.95, staircase=True)optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)train_step = optimizer.minimize(loss)with tf.Session() as sess:    ...    sess.run(train_step, feed_dict = {X:X_data, Y:Y_data})Gradient Clipping直观作用就是让权重的更新限制在一个合适的范围trainable_variables = tf.trainable_variables() # 获取到模型中所有需要训练的变量grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM) # 求导，并且进行截断optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1.0)self.train_op = optimizer.apply_gradients(    zip(grads, trainable_variables))autoencodeCAEConvolutional Autoencoders (CAE)transposed convolution layers tf.nn.conv2d_transpose 产生人工假象(artefacts) 使用tf.image.resize_nearest_neighbor 代替### Encoderconv1 = tf.layers.conv2d(X_noisy, filters[1], (k,k), padding='same', activation=activation_fn)# Output size h_in x w_in x filters[1]maxpool1 = tf.layers.max_pooling2d(conv1, (p,p), (s,s), padding='same')# Output size h_l2 x w_l2 x filters[1]conv2 = tf.layers.conv2d(maxpool1, filters[2], (k,k), padding='same', activation=activation_fn)# Output size h_l2 x w_l2 x filters[2]maxpool2 = tf.layers.max_pooling2d(conv2,(p,p), (s,s), padding='same')# Output size h_l3 x w_l3 x filters[2]conv3 = tf.layers.conv2d(maxpool2,filters[3], (k,k), padding='same', activation=activation_fn)# Output size h_l3 x w_l3 x filters[3]encoded = tf.layers.max_pooling2d(conv3, (p,p), (s,s), padding='same')# Output size h_l3/s x w_l3/s x filters[3] Now 4x4x16### Decoderupsample1 = tf.image.resize_nearest_neighbor(encoded, (h_l3,w_l3))# Output size h_l3 x w_l3 x filters[3]conv4 = tf.layers.conv2d(upsample1, filters[3], (k,k), padding='same', activation=activation_fn)# Output size h_l3 x w_l3 x filters[3]upsample2 = tf.image.resize_nearest_neighbor(conv4, (h_l2,w_l2))# Output size h_l2 x w_l2 x filters[3]conv5 = tf.layers.conv2d(upsample2, filters[2], (k,k), padding='same', activation=activation_fn)# Output size h_l2 x w_l2 x filters[2]upsample3 = tf.image.resize_nearest_neighbor(conv5, (h_in,w_in))# Output size h_in x w_in x filters[2]conv6 = tf.layers.conv2d(upsample3, filters[1], (k,k), padding='same', activation=activation_fn)# Output size h_in x w_in x filters[1]logits = tf.layers.conv2d(conv6, 1, (k,k) , padding='same', activation=None)# Output size h_in x w_in x 1decoded = tf.nn.sigmoid(logits, name='decoded')loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)cost = tf.reduce_mean(loss)opt = tf.train.AdamOptimizer(0.001).minimize(cost)"""摘录来自: Antonio Gulli. “TensorFlow 1.x Deep Learning Cookbook。” Apple Books."""]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[keras start]]></title>
      <url>/blog/2018/05/17/keras-start</url>
      <content type="text"><![CDATA[顺序  搭建model  compile 定义训练方法  fit 喂入数据开始训练model = Sequential()model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,),activation='relu'))# model.add(Activation('relu'))model.add(Dropout(DROPOUT))model.add(Dense(N_HIDDEN))model.add(Activation('relu'))model.add(Dropout(DROPOUT))model.add(Dense(NB_CLASSES))model.add(Activation('softmax'))# 查看model结构 个人觉得效果一般model.summary()model.compile(loss='categorical_crossentropy',              optimizer=OPTIMIZER,              metrics=['accuracy'])history = model.fit(X_train, Y_train,                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)score = model.evaluate(X_test, Y_test, verbose=VERBOSE)print("\nTest score:", score[0])print('Test accuracy:', score[1])保存# save modelmodel_json = model.to_json()open('cifar10_architecture.json', 'w').write(model_json)model.save_weights('cifar10_weights.h5', overwrite=True)# load modelmodel_architecture = 'cifar10_architecture.json'model_weights = 'cifar10_weights.h5'model = model_from_json(open(model_architecture).read())model.load_weights(model_weights)model.save('xx.h5')load_model('xx.h5')callbacktensorboard = TensorBoard(log_dir="mnist/{}".format(time()))checkpoint = ModelCheckpoint(filepath=os.path.join('mnist', 'model-{epoch:02d}.hdf5'))history = model.fit(X_train, Y_train,                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT, callbacks=[tensorboard])分析history = model.fit(X_train, y_train,                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)score = model.evaluate(X_test, y_test, verbose=VERBOSE)print("\nTest score:", score[0])print('Test accuracy:', score[1])# list all data in historyprint(history.history.keys())# summarize history for accuracyplt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('model accuracy')plt.ylabel('accuracy')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show()# summarize history for lossplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show()]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> keras </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[树]]></title>
      <url>/blog/2018/05/15/%E6%A0%91</url>
      <content type="text"><![CDATA[决策树不同的表达式 path view / 递归view  分支个数（number of branches）  分支条件（branching criteria）  终止条件（termination criteria）  基本算法（base hypothesis）C&amp;RT可处理 回归 二元 多元分类不纯度计算分类from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifieriris = load_iris()X = iris.data[:, 2:] # petal length and widthy = iris.targettree_clf = DecisionTreeClassifier(max_depth=2)tree_clf.fit(X, y)&gt;&gt;&gt; tree_clf.predict_proba([[5, 1.5]])array([[ 0. , 0.90740741, 0.09259259]])&gt;&gt;&gt; tree_clf.predict([[5, 1.5]])array([1])回归from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor(max_depth=2)tree_reg.fit(X, y)停止条件不纯度impurity为0，表示该分支已经达到了最佳分类程度所有的xn相同，无法对其进行区分容易overfitprune 剪枝叶子数衡量正则化 $\lambda$ 用validation决定剪枝过程want N 片叶子  剪去one-leaf remove 剩下的树有N-1情况  argmin(Ein(G)) in N-1 直到得到不同叶子min Ein model  计算model误差最小surrogate 替代品缺失关键特征导致无法判断，寻找与该特征相似的替代feature随机森林bagging + C&amp;RTbagging 制造data-set 训练 C&amp;RT  不同决策树可以由不同主机并行训练生成，效率很高；  随机森林算法继承了C&amp;RT的优点；  将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题    &gt;&gt;&gt;from sklearn.ensemble import RandomForestClassifier&gt;&gt;&gt;rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)&gt;&gt;&gt;rnd_clf.fit(X_train, y_train)&gt;&gt;&gt;y_pred_rf = rnd_clf.predict(X_test)        增强决策树多样性    选择子特征 而不是 sub_data-set  选择子特征做线性组合成一个特征特征选择对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。通过比较某特征被随机值替代前后的表现，就能推断出该特征的权重和重要性permutation bootstrap 选出 打乱所有样本某个特征值 =&gt; 求出特征重要性为了避免重新训练，应该permutationOOB的值训练时使用原数据，进行OOB验证时，将所有的OOB样本的某特征重新洗牌，验证G的表&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; iris = load_iris()&gt;&gt;&gt; rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)&gt;&gt;&gt; rnd_clf.fit(iris["data"], iris["target"])&gt;&gt;&gt; for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):&gt;&gt;&gt;     print(name, score)sepal length (cm) 0.112492250999sepal width (cm) 0.0231192882825petal length (cm) 0.441030464364petal width (cm) 0.423357996355adaboost rf权重u实际上表示该样本在bootstrap中出现的次数，反映了它出现的概率。那么可以根据u值，对原样本集D进行一次重新的随机sampling，也就是带权重的随机抽样。sampling之后，会得到一个新的D’，D’中每个样本出现的几率与它权重u所占的比例应该是差不多接近的G的权重a容易无限大 是有prune 限制树高]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[分类 指标trick]]></title>
      <url>/blog/2018/05/10/%E5%88%86%E7%B1%BB-%E6%8C%87%E6%A0%87trick</url>
      <content type="text"><![CDATA[二元分类数据不均衡有效Confusion Matrix 混淆矩阵from sklearn.model_selection import cross_val_predicty_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)instead of returning the evaluation scores, it returns the predic‐ tions made on each test fold&gt;&gt;&gt; from sklearn.metrics import confusion_matrix&gt;&gt;&gt; confusion_matrix(y_train_5, y_train_pred)    array([[53272, 1307],          [ 1077,  4344]])          row actual classcloumn pridect class            PN  PT        AN  TN ,FP        AP  FN,TPbest TN TP onlyPrecisionthe accuracy of the positive predictionsRecalltruepositive ratethe ratio of positive instances that are correctly detected by the classifier&gt;&gt;&gt; from sklearn.metrics import precision_score, recall_score&gt;&gt;&gt; precision_score(y_train_5, y_pred) # == 4344 / (4344 + 1307) 0.76871350203503808&gt;&gt;&gt; recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.79136690647482011  不同情况关注点不一样防小孩看到黄暴视频，分类器拒绝很多好的视频(low recall)，只保留最安全的(high precision)F score&gt;&gt;&gt; from sklearn.metrics import f1_score &gt;&gt;&gt; f1_score(y_train_5, y_pred) 0.78468208092485547thresholdfind threshold for recall and precision  lowering the threshold increases recall and reduces precisiony_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,                                 method="decision_function")from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)ROC CurveThe receiver operating characteristic (ROC) curve is another common tool used with binary classifiersROC similar to the precision/recall curveAUC area under the ROC curvefrom sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores)&gt;&gt;&gt; from sklearn.metrics import roc_auc_score &gt;&gt;&gt; roc_auc_score(y_train_5, y_scores)0.97061072797174941ROC曲线的纵轴是真阳率（TPR），横轴是假阳率（FPR）。对所有样本的预测值（属于正类的概率值）降序排列，然后依次将预测的概率值作为阈值，每次得到该阈值下模型预测结果为正类、负类的样本数，然后生成一组 (FPR, TPR) 值，这样就可以得到ROC曲线上的一点，最后将所有的点连接起来就出现了ROC曲线。ROC曲线越靠近左上角，表示效果越好。  PR curve or ROC curvePR : positive class is rare or care more about the FP than the FN，其他情况选roc多元分类multiple classes ： Random Forest classifiers and naive Bayes classifiersstrictly binary classifiers :Support Vector Machine classifiers or Linear classifiersperform multiclass classification using multiple binary classifiers        以上的判别方式难以处理多元分类 太硬了 不灵活OVR二元分类 One-Versus-Rest(OVR)  n class fn(d)-&gt; if n for d in data select max(fn(d))        优点是简单高效缺点是如果数据类别很多时，正负类之间的数量差别就很大（数据unbalanced），这样会影响分类效果OVO  二元分类 每组二元分类 N × (N – 1) / 2 classifiers ，遍历所有二元分类配对 算出最高票投票        优点是更加高效，虽然分类次数增加，但是比较时数据量减少只使用两个类别的数据。而且一般不会出现数据unbalanced的情况。缺点是需要分类的次数多$C_k^2$个，时间复杂度&gt;空间复杂度一般用OVR小训练集上训练许多分类器比在大训练集上训练少量分类器更快 however, OVR is preferredScikit-Learn detects when you try to use a binary classification algorithm for a multi‐class classification task, and it automatically runs OVRfrom sklearn.multiclass import OneVsOneClassifier,OneVsRestClassifier多元混淆矩阵多分类问题，这就意味着每两两类别的组合都对应一个二元的混淆矩阵。  先在各个混淆矩阵中分别计算出结果，再计算平均值，这种方式称为宏平均。  二元混淆矩阵的对应的元素进行平均，得到 TP、TN、FP、FN 的平均值，然后再根据这些平均值来计算，这种方式称为微平均多标签分类一个样例输出多个类别，比如对于同一张图片，它识别出几个人from sklearn.neighbors import KNeighborsClassifiery_train_large = (y_train &gt;= 7)y_train_odd = (y_train % 2 == 1)y_multilabel = np.c_[y_train_large, y_train_odd]knn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_multilabel)类别不平衡 class-imbalance当面对不平衡的数据集时，机器学习算法倾向于产生不令人满意的分类器。在数据分析过程中面临的主要问题是-如何通过为这些异常获取大量样本来获得平衡数据集？，因为它们很少发生。当面对不平衡的数据集时，传统的模型评估方法不能准确地测量模型性能。往往只预测majority class数据。minority class被视为噪音，往往被忽视。因此，与majority class相比，minority class错误分类概率很高。imbalanced-classification-problem8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset重采样增加少数群体的频率或降低多数群体的频率SMOTESynthetic Minority Over-sampling Technique遵循该技术以避免在将少数例子的精确复制品添加到主数据集时发生的过度拟合。从少数类中获取数据子集作为示例，然后创建新的合成类似实例。然后将这些合成实例添加到原始数据集中。新数据集用作训练分类模型的样本。        MSMOTESMOTE 改进版通过计算少数类样本和训练数据样本之间的距离 将少数类分成三组 – Security/Safe samples, Border samples, and latent nose samples.安全样本是那些可以改善分类器性能的数据点。另一方面，噪声是可能降低分类器性能的数据点。难以归类为两者中任何一个的那些被归类为边界样本。集成算法修改现有的分类算法，使其适用于不平衡的数据集  在大多数情况下，SMOTE和MSMOTE等合成技术的性能将优于传统的过采样和欠采样方法。为了获得更好的效果，可以使用合成采样方法，如SMOTE和MSMOTE，以及先进的增强方法，如渐变增强和XG增强。更改性能指标准确性不是使用不平衡数据集时使用的度量标准。我们已经看到它具有误导性。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Bias-Variance trick]]></title>
      <url>/blog/2018/05/10/Bias_Variance_trick</url>
      <content type="text"><![CDATA[Bias-Variance Tradeoff      方差  varinace    代表我们使用不同训练集时模型表现的差异  如果模型具有较大的方差，训练集只要有少许变化，模型会有很大的改变。复杂的模型一般具有更大的方差        偏差 bias    代表实际模型与理想模型的差别          过拟合主要原因  noise  模型复杂度太高  train-set too small解决  数据清洗（错误数据的更正，或者删除）  简化模型  更多数据 （如果没有办法获得更多的训练集，对已知的样本进行简单的处理、变换，从而获得更多的样本）  regularization  减少特征  validation维度灾难当模型很复杂的时候 ，复杂度本身就会引入一种noise，所以即使高阶无noise，模型也不能很好泛化。欠拟合  增加模型的迭代次数  生成更好特征供训练使用  降低正则化水平Regularization正则化 给予model惩罚 减低model复杂度Validation普通set 分train and test =&gt; Modelargmin(E_test) =&gt; 得到model再用整体数据集训练 分量4:1Leave-One-Out每次从数据集中取一个样本作为test-set，直到N个样本都作过验证集，共计算N次，最后对验证误差求平均  计算量大 稳定性，例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动所以Leave-One-Out方法在实际中并不常用V-Fold Cross ValidationLeave-One-Out是将N个数据分成N分，那么改进措施是将N个数据分成V份（例如V=10），计算过程与Leave-One-Out相似。这样可以减少总的计算量，又能进行交叉验证，这种方法称为V-折交叉验证。Leave-One-Out就是V-折交叉验证的一个极端例子examplecross_val_score K-fold cross-validationfrom sklearn.model_selection import cross_val_scorecross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")归一化原始数据进行线性变换把数据映射到[0,1]最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。标准化数据均值为0，标准差为1 其中μ是样本的均值，σ是样本的标准差 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化非线性变换用不太复杂的model 解决非线性问题通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行线性分类  特征转换  训练线性模型例如模型太复杂容易带来过拟合非线性变换可能会带来的一些问题：时间复杂度和空间复杂度的增加，尽可能使用简单的模型，而不是模型越复杂越好Loss对数损失回归 mse rmse]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[凸函数]]></title>
      <url>/blog/2018/05/04/%E5%87%B8%E5%87%BD%E6%95%B0</url>
      <content type="text"><![CDATA[凸函数：函数任意两点的两点连线在函数上方因为凸函数具有局部最优解就是全局最优解的优良性质，我们可以在求解过程不用过多考虑局部最优解和全局最优解的问题，因此，现有优化问题研究更多放在将一般形式的目标函数转化为凸函数后求解。而对于凸优化问题，我们可以采用熟知的内插法、梯度下降法、牛顿拉斐逊算法以及BFGS算法等。]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 高数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[pca]]></title>
      <url>/blog/2018/05/02/pca</url>
      <content type="text"><![CDATA[  很多机器学习的问题都会涉及到有着几千甚至数百万维的特征的训练实例。这不仅让训练过程变得非常缓慢，同时还很难找到一个很好的解，我们接下来就会遇到这种情况。这种问题通常被称为维数灾难（curse of dimentionality）。高维数据集处于非常稀疏的风险，大多数训练实例可能彼此远离。当然，这也意味着一个新实例可能远离任何训练实例，这使得预测的可靠性远低于较低维度，因为它们将基于更大的外推。train-set 维数越多 overfit风险越大降低维数，肯定加快训练速度，但并不总是会导致更好或更简单的解决方案;这一切都取决于数据集。投影PCAPrincipal Component Analysis (PCA)首先识别与数据最接近的超平面，然后将数据投影到它上面主成分分析数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选 择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。我们会发现，大部分方差都包含 在最前面的几个新坐标轴中。我们可以忽略余下的坐标轴，即对数据进行了降维处理。How to:如何选择投影坐标轴方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。=&gt;方差正交基 协方差=0如何得到这些包含最大差异性的主成分方向 计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵特征值分解协方差矩阵奇异值分解协方差矩阵SVD奇异值分解PCA数学原理from sklearn.decomposition import PCApca=PCA(n_components=2)X2D=pca.fit_transform(X)"""可以使用components_访问每一个主成分每个主成分的方差解释率，可通过explained_variance_ratio_变量获得。它表示位于每个主成分轴上的数据集方差的比例。"""# 84.2% 的数据集方差位于第一轴，14.6% 的方差位于第二轴。第三轴的这一比例不到1.2％，&gt;&gt;&gt; print(pca.explained_variance_ratio_)array([0.84248607, 0.14631839])选择正确的维度pca=PCA()pac.fit(X)cumsum=np.cumsum(pca.explained_variance_ratio_)d=np.argmax(cumsum&gt;=0.95)+1# n_components设置为 0.0 到 1.0 之间的浮点数，表明您希望保留的方差比率pca=PCA(n_components=0.95)X_reduced=pca.fit_transform(X)解压pca=PCA(n_components=154)X_mnist_reduced=pca.fit_transform(X_mnist)X_mnist_recovered=pca.inverse_transform(X_mnist_reduced)批量PCAfrom sklearn.decomposition import IncrementalPCAn_batches=100inc_pca=IncrementalPCA(n_components=154)for X_batch in np.array_split(X_mnist,n_batches):    inc_pca.partial_fit(X_batch)X_mnist_reduced=inc_pca.transform(X_mnist)Randomized PCA快速找到前d个主成分的近似值。它的计算复杂度是O(m × d^2) + O(d^3)，而不是O(m × n^2) + O(n^3)，所以当d远小于n时，它比之前的算法快得多rnd_pca=PCA(n_components=154,svd_solver='randomized')X_reduced=rnd_pca.fit_transform(X_mnist)核 PCA（Kernel PCA）from sklearn.decomposition import KernelPCArbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.04)X_reduced=rbf_pca.fit_transform(X)LLE局部线性嵌入（Locally Linear Embedding）from sklearn.manifold import LocallyLinearEmbeddinglle=LocallyLinearEmbedding(n_components=2,n_neighbors=10)X_reduced=lle.fit_transform(X)]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[k-mean]]></title>
      <url>/blog/2018/05/02/kmean</url>
      <content type="text"><![CDATA[]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[线性模型]]></title>
      <url>/blog/2018/05/01/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B</url>
      <content type="text"><![CDATA[线性回归假设Regression Analysis with Assumptions, Plots &amp; SolutionsLinearity 线性应变量和每个自变量都是线性关系。非线性/不满足可加性 模型将无法很好的描述变量之间的关系，极有可能导致很大的泛化误差（generalization error）Indpendence 独立性误差项对于所有的观测值，它们的误差项相互之间是独立的。自相关性（Autocorrelation）经常发生于时间序列数据集上，后项会受到前项的影响。如果误差项是相关的，则估计的标准差倾向于 &lt; 真实的标准差。置信区间和预测区间变窄。较窄的置信区间意味着95％置信区间的概率小于0.95，它将包含系数的实际值。Durbin – Watson (DW)它必须介于0和4之间。如果DW = 2，则表示没有自相关，0 &lt;DW &lt;2表示正自相关，而2 &lt;DW &lt;4表示负自相关。自变量之间应相互独立当发现自变量是中度或高度相关时,多重共线性,会导致我们测得的标准差偏大，置信区间变宽。VIF因子 VIF&lt;=4  没有多重共线性  VIF&gt;=10  严重的多重共线性。最小二乘法的基础上，加入了一个与回归系数的模有关的惩罚项（岭回归，Lasso回归或弹性网（ElasticNet）回归），可以收缩模型的系数。一定程度上减少方差。Normality 正态性误差项服从正态分布,均值为0。如果误差项不呈正态分布，需要重点关注一些异常的点（误差较大但出现频率较高）Equal-variance 等方差所有的误差项具有同样方差。为常数。异方差性，出现在有异常值（Outlier）的数据集上，对模型影响很大。普通线性回归模型目标函数最小二乘法求权重&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; lin_reg = LinearRegression()&gt;&gt;&gt; lin_reg.fit(X,y)&gt;&gt;&gt; lin_reg.intercept_, lin_reg.coef_(array([4.21509616]),array([2.77011339]))&gt;&gt;&gt; lin_reg.predict(X_new)array([[4.21509616],[9.75532293]])logistic 分类二元分类logistic        预测值表示为1的概率，取值范围在[0,1]之间$ŷ=w^Tx+b$普通回归模型套一个 sigmoid function 控制输出 (0,1)Cost function&gt;&gt;&gt; from sklearn import datasets&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; list(iris.keys())['data', 'target_names', 'feature_names', 'target', 'DESCR']&gt;&gt;&gt; X = iris["data"][:, 3:] # petal width&gt;&gt;&gt; y = (iris["target"] == 2).astype(np.int)逻辑回归模型也可以 $\ell_1$ 或者 $\ell_2$ 惩罚使用进行正则化。Scikit-Learn 默认添加了 $\ell_2$ 惩罚。from sklearn.linear_model import LogisticRegressionlog_reg = LogisticRegression()log_reg.fit(X, y)# 如果你使用它进行预测（使用predict()方法而不是predict_proba()方法），它将返回一个最可能的结果X_new = np.linspace(0, 3, 1000).reshape(-1, 1)y_proba = log_reg.predict_proba(X_new)plt.plot(X_new, y_proba[:, 1], "g-", label="Iris-Virginica")plt.plot(X_new, y_proba[:, 0], "b--", label="Not Iris-Virginica")  在 Scikit-Learn 的LogisticRegression模型中控制正则化强度的超参数不是 $\alpha$（与其他线性模型一样），而是它的逆：C。 C 的值越大，模型正则化强度越低。多类别分类 softmaxLogistic 回归模型可以直接推广到支持多类别分类def softmax(X):    exps = np.exp(X)    return exps / np.sum(exps)softmax 损失函数 cross entrydef cross_entropy(X,y):    """    X is the output from fully connected layer (num_examples x num_classes)    y is labels (num_examples x 1)    	Note that y is not one-hot encoded vector.     	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.    """    m = y.shape[0]    p = softmax(X)    # We use multidimensional array indexing to extract     # softmax probability of the correct label for each sample.    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.    log_likelihood = -np.log(p[range(m),y])    loss = np.sum(log_likelihood) / m    return loss交叉熵，应该导致这个目标，因为当它估计目标类别的低概率时，它会对模型进行惩罚。交叉熵通常用于衡量一组估计的类别概率与目标类别的匹配程度目标函数X = iris["data"][:, (2, 3)] # petal length, petal widthy = iris["target"]softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10)softmax_reg.fit(X, y)&gt;&gt;&gt; softmax_reg.predict([[5, 2]])array([2])&gt;&gt;&gt; softmax_reg.predict_proba([[5, 2]])array([[ 6.33134078e-07, 5.75276067e-02, 9.42471760e-01]])Gradient 梯度梯度下降error函数的图像，看上去像个碗一样，中间是凹的，两边翘起。这个碗的最低点，也就是 error(x) 的最小值，就是我们要寻找的点。发现:当x在最小值左边的时候，error函数的导数（斜率）是负的；当x在最小值右边的时候，导数是正的；当x在最小值附近的时候，导数接近0.因此，如果我们在：导数为负的时候增加x；导数为正的时候减小x；解释：求权重的偏导  derivative 是 error 在 x 处的导数，这里是用导数的定义求的。x = x - derivative 这就是“导数下降”名称的由来。通过不断地减去导数，x最终会到达函数的最低点。  alpha 参数控制的是点 x 逆着导数方向前进的距离（下降速度），alpha 越大，x 就会前进得越多，误差下降得越快。alpha 太小会导致下降缓慢，alpha 太大会导致 x 冲得太远，令函数无法收敛。Batch Gradient Descent它使用整个训练集来计算每一步的梯度，这使得训练集很大时非常缓慢eta = 0.1 # 学习率n_iterations = 1000m = 100theta = np.random.randn(2,1) # 随机初始值for iteration in range(n_iterations):    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)    theta = theta - eta * gradiensStochastic Gradient Descent随机梯度下降只是在每个步骤中在训练集中选取一个随机实例，并仅基于该单个实例计算梯度由于其随机性，该算法比批处理梯度成本函数会上下反弹 只会平均减少 随着时间的推移，它将最终接近最小值，但一旦它到达那里，它将继续反弹，永远不会稳定下来。所以一旦算法停止，最终的参数值是好的，但不是最优的当成本函数非常不规则时，这实际上可以帮助算法跳出局部最小值，因此，随机性很好地摆脱局部最优。但不好，因为这意味着该算法永远无法最小化方法是逐渐降低学习率。这些步骤开始较大（这有助于快速进展并避免局部最小值），然后变得越来越小，从而使算法在全局最小值处达到最小。（simulated annealing模拟退火）n_epochs = 50t0, t1 = 5, 50  #learning_schedule的超参数def learning_schedule(t):    return t0 / (t + t1)theta = np.random.randn(2,1)for epoch in range(n_epochs):    for i in range(m):        random_index = np.random.randint(m)        xi = X_b[random_index:random_index+1]        yi = y[random_index:random_index+1]        gradients = 2 * xi.T.dot(xi,dot(theta)-yi)        eta = learning_schedule(epoch * m + i)        theta = theta - eta * gradiensMini-batch Gradient Descent小批量的小随机实例集上的梯度进展比SGD更不稳定，特别是在相当大的小批量时。因此，小批量GD最终会走得比SGD更接近最小值。但另一方面，它可能难以摆脱局部最小值m is the number of training instances and n is the number of features多项式m = 100X = 6 * np.random.rand(m, 1) - 3y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; poly_features = PolynomialFeatures(degree=2,include_bias=False)&gt;&gt;&gt; X_poly = poly_features.fit_transform(X)&gt;&gt;&gt; X[0]array([-0.75275929])&gt;&gt;&gt; X_poly[0]array([-0.75275929, 0.56664654])&gt;&gt;&gt; lin_reg = LinearRegression()&gt;&gt;&gt; lin_reg.fit(X_poly, y)&gt;&gt;&gt; lin_reg.intercept_, lin_reg.coef_(array([ 1.78134581]), array([[ 0.93366893, 0.56456263]]))模型预测函数 $\hat{y}=0.56x_1^2+0.93x_1+1.78$非线性变换用不太复杂的model 解决非线性问题通过非线性变换，将非线性模型映射到另一个空间，转换为线性模型，再来进行线性分类  特征转换  训练线性模型例如模型太复杂容易带来过拟合非线性变换可能会带来的一些问题：时间复杂度和空间复杂度的增加，尽可能使用简单的模型，而不是模型越复杂越好过拟合造成overfit 主要是  noise  模型复杂度太高  train-set too small    解决    数据清洗（错误数据的更正，或者删除）  简化模型  更多数据 （如果没有办法获得更多的训练集，对已知的样本进行简单的处理、变换，从而获得更多的样本）  regularization  validation维度灾难当模型很复杂的时候 ，复杂度本身就会引入一种noise，所以即使高阶无noise，模型也不能很好泛化。Regularization正则化 给予model惩罚 减低model复杂度。L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用Ridge 岭回归 L2普通回归+惩罚项（ $\alpha$ 越大惩罚越大）&gt;&gt;&gt; from sklearn.linear_model import Ridge&gt;&gt;&gt; ridge_reg = Ridge(alpha=1, solver="cholesky")&gt;&gt;&gt; ridge_reg.fit(X, y)&gt;&gt;&gt; ridge_reg.predict([[1.5]])array([[ 1.55071465]]&gt;&gt;&gt; sgd_reg = SGDRegressor(penalty="l2")&gt;&gt;&gt; sgd_reg.fit(X, y.ravel())&gt;&gt;&gt; sgd_reg.predict([[1.5]])array([[ 1.13500145]])lasso L1部分权重归0 完全消除最不重要的特征的权重SGDRegressor(penalty="l1")&gt;&gt;&gt; from sklearn.linear_model import Lasso&gt;&gt;&gt; lasso_reg = Lasso(alpha=0.1)&gt;&gt;&gt; lasso_reg.fit(X, y)&gt;&gt;&gt; lasso_reg.predict([[1.5]])array([ 1.53788174]弹性网络（ElasticNet）弹性网络介于 Ridge 回归和 Lasso 回归之间。它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合，同时你可以控制它们的混合率 r，当 r=0 时，弹性网络就是 Ridge 回归，当 r=1 时，其就是 Lasso 回归。具体表示如公式 4-12。公式 4-12：弹性网络损失函数&gt;&gt;&gt; from sklearn.linear_model import ElasticNet&gt;&gt;&gt; elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)&gt;&gt;&gt; elastic_net.fit(X, y)&gt;&gt;&gt; elastic_net.predict([[1.5]])array([ 1.54333232])  Ridge回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择 Lasso 和弹性网络。就像我们讨论的那样，它两能够将无用特征的权重降为零。一般来说，弹性网络的表现要比 Lasso 好，因为当特征数量比样本的数量大的时候，或者特征之间有很强的相关性时，Lasso 可能会表现的不规律。早停法(Early Stopping)随着迭代训练次数增加，train set error一般是单调减小的。而dev set error 先减小，之后又增大。发生了过拟合。选择合适的迭代次数，即early stopping只有在验证误差高于最小值一段时间后（你确信该模型不会变得更好了），才停止，之后将模型参数回滚到验证误差最小值。from sklearn.base import clone# 注意：当warm_start=True时，调用fit()方法后，训练会从停下来的地方继续，而不是从头重新开始。sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,learning_rate="constant", eta0=0.0005)minimum_val_error = float("inf")best_epoch = Nonebest_model = Nonefor epoch in range(1000):    sgd_reg.fit(X_train_poly_scaled, y_train)    y_val_predict = sgd_reg.predict(X_val_poly_scaled)    val_error = mean_squared_error(y_val_predict, y_val)    if val_error &lt; minimum_val_error:        minimum_val_error = val_error        best_epoch = epoch        best_model = clone(sgd_reg)更好的训练结果  选择简单的model 参数很少，或者用regularization，数据集的特征减少中参数个数减少，都能降低模型复杂度  训练数据和验证数据要服从同一个分布，最好都是独立分布的，这样训练得到的模型才能更好地具有代表性。  在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据。第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[集成学习 Ensemble]]></title>
      <url>/blog/2018/04/30/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0</url>
      <content type="text"><![CDATA[sklearn xgbootEnsemble好处feature transform和regularization单一模型通常只能倾向于feature transform和regularization之一，但是Ensemble却能将feature transform和regularization各自的优势结合起来常见的 Ensemble 方法有这么几种：Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。vote 分类avg 回归加权Linear blending训练过程用validation 训练不同的base model, 得到的值代入 Linear blending 获取对应的权重a。使用完整的data set 训练不同的base model得到更好的model + 之前的权重 =&gt; 目标函数获取base model  select different base model  different params投票分类器&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier&gt;&gt;&gt; from sklearn.ensemble import VotingClassifier&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression&gt;&gt;&gt; from sklearn.svm import SVC&gt;&gt;&gt; log_clf = LogisticRegression()&gt;&gt;&gt; rnd_clf = RandomForestClassifier()&gt;&gt;&gt; svm_clf = SVC()voting="soft"有更好的效果，使用交叉验证去预测类别概率，其降低了训练速度  If ‘hard’, uses predicted class labels for majority rule voting.Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities,which is recommended for an ensemble of well-calibrated classifiers.&gt;&gt;&gt; voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), &gt;&gt;&gt; ('svc', svm_clf)],voting='hard')&gt;&gt;&gt; voting_clf.fit(X_train, y_train)测试集准确率&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; for clf in (log_clf, rnd_clf, svm_clf, voting_clf):&gt;&gt;&gt;     clf.fit(X_train, y_train)&gt;&gt;&gt;     y_pred = clf.predict(X_test)&gt;&gt;&gt;     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))LogisticRegression 0.864RandomForestClassifier 0.872SVC 0.888VotingClassifier 0.896Stacking在blending的基础上 =&gt; 多层blendingBlending只有一层，而Stacking有多层 使用前一层的输出(预测结果)作为本层的训练集(有点像多层神经网络) ，不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，训练一个模型来执行这个聚合模型复杂度过高，容易造成过拟合例子：诀窍是将训练集分成三个子集：第一个子集用来训练第一层，第二个子集用来创建训练第二层的训练集（使用第一层分类器的预测值），第三个子集被用来创建训练第三层的训练集（使用第二层分类器的预测值）。以上步骤做完了，我们可以通过逐个遍历每个层来预测一个新的实例。你也可以使用开源的项目例如 brew （网址为 https://github.com/viisar/brew）Bagging使用训练数据的不同随机子集来，有放回随机抽取m次每次n个样本作为子集训练相同Base Model，由于不同data-set子集得到不同的model最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。Pasting 是无放回  Blending &amp; Bagging 都是可并行训练base model 再aggregation以上算法不能解决下图 阴影是三个model都出错部分BaggingClassifier类或者对于回归可以是BaggingRegressor。尝试Pasting，就设置bootstrap=False如果基分类器可以预测类别概率（例如它拥有predict_proba()方法），那么BaggingClassifier会自动的运行软投票，这是决策树分类器的情况。from sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifierbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,max_samples=100, bootstrap=True, n_jobs=-1)bag_clf.fit(X_train, y_train)y_pred = bag_clf.predict(X_test)OOBout-of-bag bagging 没有被选上的数据 用来验证G-  直接验证G 加快效率BaggingClassifier来自动评估时设置oob_score=True来自动评估&gt;&gt;&gt; bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)&gt;&gt;&gt; bag_clf.fit(X_train, y_train)&gt;&gt;&gt; bag_clf.oob_score_0.93066666666666664测试集&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; y_pred = bag_clf.predict(X_test)&gt;&gt;&gt; accuracy_score(y_test, y_pred)0.93600000000000005Boosting迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。前一个base model 预测错误的的样本会增权，正确的会减少权重，权值更新过后的样本训练下一个新的base model 直到目标错误率或最大迭代次数aggregation时 表现好的model 权重就大AdaBoost错误因子犯错放大，正确缩小通常学习比乱猜要好所以犯错率ϵt &lt;= 0.5u 表示重要性初始化aggregation时 表现好的 权重就大 $\alpha\;=\;\frac12\ln\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)$from sklearn.ensemble import AdaBoostClassifierada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm="SAMME.R", learning_rate=0.5)ada_clf.fit(X_train, y_train)Gradient Boosting也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果，不像Adaboost那样每一次迭代都更改实例的权重，这个方法是去使用新的分类器去拟合前面分类器预测的残差GBRT Gradient Boosted Regression Treesfrom sklearn.tree import DecisionTreeRegressortree_reg1 = DecisionTreeRegressor(max_depth=2)tree_reg1.fit(X, y)y2 = y - tree_reg1.predict(X)tree_reg2 = DecisionTreeRegressor(max_depth=2)tree_reg2.fit(X, y2)y3 = y2 - tree_reg1.predict(X)tree_reg3 = DecisionTreeRegressor(max_depth=2)tree_reg3.fit(X, y3)y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))from sklearn.ensemble import GradientBoostingRegressorgbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)gbrt.fit(X, y)找到树的最佳数量 staged_predictimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorX_train, X_val, y_train, y_val = train_test_split(X, y)gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)gbrt.fit(X_train, y_train)errors = [mean_squared_error(y_val, y_pred)  for y_pred in gbrt.staged_predict(X_val)]bst_n_estimators = np.argmin(errors)gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)gbrt_best.fit(X_train, y_train)早停法 设置warm_start=True来实现 ，这使得当fit()方法被调用时 sklearn 保留现有树，并允许增量训练&gt;&gt;&gt;gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)min_val_error = float("inf")error_going_up = 0for n_estimators in range(1, 120):    gbrt.n_estimators = n_estimators    gbrt.fit(X_train, y_train)    y_pred = gbrt.predict(X_val)    val_error = mean_squared_error(y_val, y_pred)    if val_error &lt; min_val_error:        min_val_error = val_error        error_going_up = 0    else:        error_going_up += 1        if error_going_up == 5:            break  # early stopping]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[SVM]]></title>
      <url>/blog/2018/04/29/SVM</url>
      <content type="text"><![CDATA[Support Vector Machine 介绍若要保证对未知的测量数据也能进行正确分类，最好让分类直线距离数据集的点都有一定的距离。距离越大，分类直线对测量数据误差的容忍度越高。容忍更多的noise,有更好的泛化能力 距离用margin表示Why large margin ?因为分隔的线粗 难以将数据集任意完全分隔，有点像regularization，降低model复杂度。支持向量(support vector)就是离分隔超平面最近的那些点。接下来要试着最大化支持向量到分隔面的距离 W 是法向量 分隔超平面的形式可以写成$w^Tx+b$要计算点A到分隔超平面的距离,就必须给出点到分隔面的法线 或垂线的长度,该值为$\left|w^TA+b\right|/\left|\left|w\right|\right|$  margin最大 =&gt; 即让离分类线最近的点到分类线距离最大SVM 对特征缩放比较敏感，但对特征缩放后（例如使用Scikit-Learn的StandardScaler），判定边界看起来要好得多推导过程简化计算再简化非线性SVMDual SVM对于非线性SVM，我们通常可以使用非线性变换将变量从x域转换到z域，设z域维度=d+1 ,d越高，svm通过二次规划求解就越复杂。dual不依赖d当数据量N很大时，也同样会增大计算难度。如果N不是很大，一般使用Dual SVM来解决问题,只是简化求解不能真正脱离对d的依赖Kernel基于dual SVM推导 Kernel Function 合并特征转换和计算内积   非线性不受维度限制简化计算速度 polynomial Kernel简化好看的形式  不同的Kernel=&gt;不同的marginfrom sklearn.svm import SVCpoly_kernel_svm_clf = Pipeline((        ("scaler", StandardScaler()),        ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))    ))poly_kernel_svm_clf.fit(X, y)linear KernelSVC(kernel="linear", C=1)Gaussian Kernel无限维转换  在低维空间解决非线性利用泰勒展开反推 =&gt; Φ(x) ，Φ(x)是无限多维的高斯核函数称为径向基函数(Radial Basis Function, RBF)rbf_kernel_svm_clf = Pipeline((        ("scaler", StandardScaler()),        ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))    ))rbf_kernel_svm_clf.fit(X, y)对比Linear Kernel优点：  计算简单、快速，  直观，便于理解缺点：如果数据不是线性可分的情况，不能使用了Polynomial Kernel比线性灵活缺点：  当Q很大时，K的数值范围波动很大  参数个数较多难以选择合适的值Gaussian Kernel最常用 要小心用  比其他kernel强大  容易计算 比poly  参数少 比poly缺点  不直观  slower than linear  easy overfitother kernelm 数据集数量 n 维度Soft-Margin允许有分类错误 在 Scikit-Learn 库的 SVM 类，你可以用C超参数（惩罚系数）来控制这种平衡C 越小 容忍错误度越大  overfit 也可以减少C上面的不能用二次规划计算，不能区分error的程度点距离边界其中，ξn表示每个点犯错误的程度，ξn=0，表示没有错误，ξn越大，表示错误越大，即点距离边界越小。参数C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡，因为边界宽了，往往犯错误的点会增加large C表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类small C表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型（使用LinearSVC类，超参数C=1，hinge 损失函数）来检测 Virginica 鸢尾花import numpy as npfrom sklearn import datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCiris = datasets.load_iris()X = iris["data"][:, (2, 3)] # petal length, petal widthy = (iris["target"] == 2).astype(np.float64) # Iris-Virginicasvm_clf = Pipeline((        ("scaler", StandardScaler()),        ("linear_svc", LinearSVC(C=1, loss="hinge")),    ))svm_clf.fit(X_scaled, y)&gt;&gt;&gt; svm_clf.predict([[5.5, 1.7]])array([ 1.])svm regressionλ 越大，L2Regularization的程度就越大。C越小，相应的margin就越大。Large-Margin等同于Regularization效果一致，防止过拟合的作用。L2-regularized linear model都可以使用kernel来解决。不仅仅支持线性和非线性的分类任务，还支持线性和非线性的回归任务。技巧在于逆转我们的目标：限制间隔违规的情况下，不是试图在两个类别之间找到尽可能大的“街道”（即间隔）。SVM 回归任务是限制间隔违规情况下，尽量放置更多的样本在“街道”上。“街道”的宽度由超参数$ϵ$控制。from sklearn.svm import LinearSVRsvm_reg = LinearSVR(epsilon=1.5)svm_reg.fit(X, y)from sklearn.svm import SVRsvm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)svm_poly_reg.fit(X, y)      SVM Classifier在margin外trying to fit the largest possible street between two classes while limiting margin violations        SVM Regression在margin内tries to fit as many instances as possible on the street while limiting margin violations  ]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Bayes 贝叶斯]]></title>
      <url>/blog/2018/04/22/Bayes</url>
      <content type="text"><![CDATA[推导例子极大似然估计贝叶斯估计]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[KNN]]></title>
      <url>/blog/2018/04/17/knn</url>
      <content type="text"><![CDATA[测量不同特征值之间的距离方法选择特征最相似的K个实例 选择K个实例的多数作为新数据的分类K-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外, 由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。三个要素距离度量、K 、分类决策规则距离度量Kk 减小 过拟合 通常使用较小的 交叉验证分类决策多数表决K邻近算法优化线性扫描kdtree  训练实例»空间维数 接近的 == 线性扫描]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[行列式计算]]></title>
      <url>/blog/2018/04/15/%E8%A1%8C%E5%88%97%E5%BC%8F%E8%AE%A1%E7%AE%97</url>
      <content type="text"><![CDATA[性质例子行列式展开 高阶-&gt;低阶 代数余子式克拉默法则 解n个未知数n个线性方程]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 线性代数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[线性相关]]></title>
      <url>/blog/2018/04/15/%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3</url>
      <content type="text"><![CDATA[]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 线性代数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[线性方程初等变化 矩阵初等变换]]></title>
      <url>/blog/2018/04/15/%E7%9F%A9%E9%98%B5%E5%8F%98%E6%8D%A2</url>
      <content type="text"><![CDATA[秩]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 线性代数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[矩阵]]></title>
      <url>/blog/2018/04/15/%E7%9F%A9%E9%98%B5</url>
      <content type="text"><![CDATA[]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 线性代数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[特征值 特征向量]]></title>
      <url>/blog/2018/04/15/%E7%89%B9%E5%BE%81%E5%80%BC-%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F</url>
      <content type="text"><![CDATA[内积长度特征值和特征向量]]></content>
      <categories>
        
          <category> 数学 </category>
        
      </categories>
      <tags>
        
          <tag> 线性代数 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[投资优化]]></title>
      <url>/blog/2018/04/07/%E6%8A%95%E8%B5%84%E4%BC%98%E5%8C%96</url>
      <content type="text"><![CDATA[投资组合求对数收益 协方差sum(W) =1预期收益最优化投资组合所有最优化投资组合一一即目标收益率水平俨波动率最小的所有投资者(或者纷走风 险水平下收益率最大的所有投资组合)对数收益率收益相关性  线性回归                              pd.corr          rolling_corr 不同时期相关性                    回归分析numpydef np_deg(x, y, deg=1):    """线性单项式x x^2 x^3"""    p = np.polyfit(x, y, deg=deg)    """    N = len(p)    p[0]*x**(N-1) + p[1]*x**(N-2) + ... + p[N-2]*x + p[N-1]    """    return np.polyval(p, x)def spi_k(x, y, k=1):    """    插值    在两个相邻数据点之 间进行回归，    不仅产生的分段插值函数完全匹配数据点，而且函数在数据点上连续可 微分。        比回归效果好    必须有排序(且"无噪声" )的数据，该方 法仅限于低维度问题.样条插值的计算要求也更高，在某些用例中可 能导致花费的时间比回归方法长得多.    """    p = spi.splrep(x, y, k=k)    return spi.splev(x, p)statsmodelsimport pandas as pdimport statsmodels.api as smdf = pd.DataFrame({"A": [10, 20, 30, 40, 50], "B": [20, 30, 10, 40, 50], "C": [32, 234, 23, 23, 42523]})result = sm.OLS(df['A'], sm.add_constant(df[['B', 'C']])).fit()# print(result.summary())print(df['B'] * result.params['B'] + df['C'] * result.params['C'] + result.params['const'])sklearnfrom sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit(df[['B', 'C']], df['A'])# print(reg.coef_)# print(reg.intercept_)print(df['B'] * reg.coef_[0] + df['C'] * reg.coef_[1] + reg.intercept_)持久化pickle .pkl csvsql sqlite快numpy save .npy loadpd HDFStore  空间小使用numpy的结构数组 保存到PyTable 更好PyTable内存外计算]]></content>
      <categories>
        
          <category> py-finance </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[投资优化 续]]></title>
      <url>/blog/2018/04/07/%E6%8A%95%E8%B5%84%E4%BC%98%E5%8C%96-%E7%BB%AD</url>
      <content type="text"><![CDATA[蒙特卡洛模拟S0 =100. r =0.05sigma =0.2 T =1.0M= 501 =250000生成 25 万条路 径，每条有 50个时间步def gen_monte():    dt = T / M    S = np.zeros((M + 1, I), np.float64)    S[0] = S0    for t in range(1, M + 1):        rand = np.random.standard_normal(I)        rand = (rand-rand.mean())/rand.std()        S[t] = S[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt + sigma * np.sqrt(dt) * rand)$\textstyle\sum_{}w_i\;=1$,  $\textstyle w_i\geq\;0$def rand_w(split=5):    w = np.random.random(split)    w /= np.sum(w)    return w预期收益 252年化np.sum(rets.mean() * weight * 252)预期投资方差var = np.dot(weights.T, np.dot(rets.cov() * 252, weights))std = np.sqrt(var)优化组合 夏普指数（即预期投资组合超额收益）投资收益率超过无风险利率 rf 的部分除以预期投资组合标准差。为了简单起见，我们假定 rf=0。 rf =0def statistics(weights):    """    Return portfolio statistics    :param weights: weights for different securities in portfolio    :return:    pret:float    expected portfolio return    pvol:float    expected portfolio volatility    pret/pvol:float    Sharpe ratio for rf=0    """    weights = np.array(weights)    pret = np.sum(rets.mean() * weights) * 252    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))    return np.array([pret, pvol, pret-rf / pvol])import scipy.optimize as scodef min_func_sharpe(weights):    return -statistics(weights)[2]# 约束是所有参数（权重）的总和为1。 这可以用minimize函数的约定表达如下cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})# 我们还将参数值（权重）限制在0和l之间。 这些值以多个元组组成的一个元组形式提供给最小化函数：bnds = tuple((0, 1) for x in range(noa))# 优化函数调用中忽略的唯一输入是起始参数列表（对权重的初始猜测）。我们简单地使用平均分布：noa * [1. / noa, ]# [0.2, 0.2, 0.2, 0.2, 0.2]opts = sco.minimize(min_func_sharpe, noa * [1. / noa, ], method='SLSQP', bounds=bnds, constraints=cons)statistics(opts['x'].round(3))# array([ 0.22201418,  0.28871174,  0.76898216])# 预期收益率约为22.2%. 预期被动率约为28.9%， 得到的最优夏普指数为0.77有效边界目标收益率水平下波动率最小  预期收益trettrets = np.linspace(0.0, 0.25, 50)tvols = []bnds = tuple((0, 1) for x in weights)for tret in trets:    cons = ({'type': 'eq', 'fun': lambda x: statistics(x)[0] - tret},            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})    res = sco.minimize(min_func_port, noa * [1. / noa, ], method='SLSQP', bounds=bnds, constraints=cons)    tvols.append(res['fun'])tvols = np.array(tvols)资本市场线import scipy.interpolate as sciind = np.argmin(tvols)evols = tvols[ind:]erets = trets[ind:]tck = sci.splrep(evols, erets)# 通过这条数值化路径，最终可以为有效边界定义一个连续可微函数# 和对应的一阶导数函数df(x):def f(x):    """    有效边界 插值逼近    """    return sci.splev(x, tck, der=0)def df(x):    """    有效边界函数的一阶导数。    """    return sci.splev(x, tck, der=1)#我们所寻求的是函数 t(x)=α+b.x，描述穿过风险-收益空间中无风险资产、与有效边界 相切的一条值线# 定义一个函数，返回给定参数集p=(a,b,x)def equations(p, rf=0.01):    eq1 = rf - p[0]    eq2 = rf + p[1] * p[2] - f(p[2])    eq3 = p[1]-df(p[2])    return eq1,eq2,eq3# 数值优化得到如下的值#无风险利率为a = 1%时的资本市场线和相切的投资组合opt=sco.fsolve(equations,[0.01,0.5,0.15])cons = ({'type': 'eq', 'fun': lambda x: statistics(x)[0] - f(opt[2])},        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})res = sco.minimize(min_func_port, noa * [1. / noa, ],                   method='SLSQP', bounds=bnds, constraints=cons)res['x'].round(3)# array([ 0.   ,  0.   ,  0.   ,  0.703,  0.297])]]></content>
      <categories>
        
          <category> py-finance </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[部署Gitlab]]></title>
      <url>/blog/2018/04/04/gitlab</url>
      <content type="text"><![CDATA[gitlab 配置sudo vim /etc/gitlab/gitlab.rb external_urlgitlab-ctl tail #查看所有日志 gitlab-ctl tail nginx/gitlab_access.log #查看nginx访问日志//启动 sudo gitlab-ctl star//停止 sudo gitlab-ctl stop//重启 sudo gitlab-ctl restart//使更改配置生效 sudo gitlab-ctl reconfiguresudo gitlab-ctl hup nginx## 卸载 sudo gitlab-ctl stop sudo gitlab-ctl uninstall sudo gitlab-ctl cleanse sudo rm -rf /opt/gitlab]]></content>
      <categories>
        
          <category> git </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[数据]]></title>
      <url>/blog/2018/04/03/data</url>
      <content type="text"><![CDATA[大量回测数据  用二进制文件分日期分股票存储  使用sql server，pg这样支持分区表的事务型数据库  使用hive这样的离线数据仓库  用Greenplum这样的开源或商业MPP数据仓库  kdb+和DolphinDB这样的专业时序数据库。 效率高Greenplum和DolphinDB支持的比较好，支持库内计算，不需要移动数据，速度很快。其它的存储方法可以考虑写一个跟通用计算引擎spark的适配器，然后用spark来实现分布式SQL和分布式机器学习，但性能上会比库内计算差不少。如果涉及到分布式计算，或者需要多次迭代，数据本身有可能是动态变化的，数据的一致性也要注意。一般数据仓库本身提供的库内计算，能提供快照级别的隔离，保证计算过程总用到的所有数据是一致的。Greenplum和DolphinDB都支持快照级别隔离。Spark不能工作在动态数据上。数据  日K单表mysql  分钟及Tick数据日期分表存储添加了普通索引交易  普通限价下单 主要用于普通的策略回测，  拆单限价下单（TWAP及VWAP） 更多用于策略扩大规模时的容量测试，  智能优化下单（只限定报单量，由算法自动根据当前市场状态优化报单价格）更多地是与策略本身解耦，单独进行测试，并在实盘中选择是否使用。下单方法TWAP适用于流动性较好的市场和订单规模较小的交易。TWAP(Time Weighted Average Price)，时间加权平均价格算法。该模型将交易时间进行均匀分割，并在每个分割节点上等量拆分订单进行提交。使交易对市场影响减小的同时提供一个较低的平均成交价格，从而达到减小交易成本的目的。在分时成交量无法准确估计的情况下，该模型可以较好地实现算法交易的基本目的。订单规模很大的情况下，均匀分配到每个节点上的下单量仍然较大，当市场流动性不足时仍可能对市场造成一定的冲击。真实市场的成交量总是在波动变化的，将所有的订单均匀分配到每个节点上显然是不够合理的。VWAP拆分大额委托单，在约定时间段内分批执行，以期使得最终买入或卖出成交均价尽量接近这段时间内整个市场成交均价的交易策略。VWAP(Volume Weighted Average Price)，成交量加权平均价格算法，VWAP模型的目的就是使得在指定时间段所执行的订单的VWAP值 &lt;= 市场上相应时间段的VWAP值。从VWAP的定义公式看，如果希望VWAP(实际)能足够接近VWAP(理论)，则需要将拆分订单按照市场真实的成交量分时按比例提交，这就需要对市场分时成交量(成交量比例)进行预测将总单拆分成多少单，分别以怎样的时间频率交易VWAP算法交易的目的是最小化冲击成本，并不寻求最小化所有成本PoV用实际成交量作为指标，因此在交易时段内总是按照市场成交量的一定比例交易剩余的头寸.]]></content>
      <categories>
        
          <category> quant </category>
        
      </categories>
      <tags>
        
          <tag> 量化系统 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[crontab]]></title>
      <url>/blog/2018/03/28/crontab</url>
      <content type="text"><![CDATA[crontab命令cron 测试效果运行不了 source /etc/profile      编辑$HOME目录下的. profile文件，在其中加入这样一行:    EDITOR=vi; export EDITOR source .profile        crontab -l使用-l参数列出crontab文件    可以使用这种方法在$HOME目录中对crontab文件做一备份: $ crontab -l &gt; $HOME/mycron 这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。      编辑crontab文件 crontab -e如果希望添加、删除或编辑crontab文件中的条目，而EDITOR环境变量又设置为vi，那么就可以用vi来编辑crontab文件        删除crontab文件 crontab -r        格式              分      时      日      月      星期      cmd                  1~59      1~23 (0表示子夜)      1~31      1~12      0~6(0表示周日)              - 在上午8点到11点的第3和第15分钟执行    3,15 8-11 * * * myCommand - 每隔两天的上午8点到11点的第3和第15分钟执行    3,15 8-11 */2  *  * myCommand - 每1分钟执行一次myCommand    * * * * * myCommand  注意清理系统用户的邮件日志  每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。例如，可以在crontab文件中设置如下形式，忽略日志输出:0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1  新创建的cron job，不会马上执行，至少要过2分钟才执行。如果  重启cron则马上执行。  新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。  当crontab失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail-f /var/log/cron。  千万别乱运行crontab-r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。  在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\%，如经常用的date‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\%Y\%m\%d’。  更新系统时间时区后需要重启cron,在ubuntu中服务名为cron:    $service cron restart    ubuntu下启动、停止与重启cron:    $sudo /etc/init.d/cron start    $sudo /etc/init.d/cron stop    $sudo /etc/init.d/cron restart]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python 精度]]></title>
      <url>/blog/2018/03/20/%E7%B2%BE%E5%BA%A6</url>
      <content type="text"><![CDATA[要求较小的精度round使用格式化 round（）是一样的要求超过17位的精度分析python默认的是17位精度,也就是小数点后16位高精度使用decimal模块，配合getcontext&gt;&gt;&gt; from decimal import *&gt;&gt;&gt; print(getcontext())Context(prec=28, rounding=ROUND_HALF_EVEN, Emin=-999999, Emax=999999, capitals=1, clamp=0, flags=[], traps=[InvalidOperation, DivisionByZero, Overflow])&gt;&gt;&gt; getcontext().prec = 50&gt;&gt;&gt; b = Decimal(1)/Decimal(3)&gt;&gt;&gt; bDecimal('0.33333333333333333333333333333333333333333333333333')&gt;&gt;&gt; c = Decimal(1)/Decimal(7)&gt;&gt;&gt; cDecimal('0.14285714285714285714285714285714285714285714285714')&gt;&gt;&gt; float(c)0.14285714285714285  math模块的ceil(x)取&gt;=x的最小整数。math模块的floor(x)取&lt;=x的最大整数]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[aio]]></title>
      <url>/blog/2018/03/20/aio</url>
      <content type="text"><![CDATA[阻塞  程序未得到所需计算资源时被挂起的状态。  程序在等待某个操作完成期间，自身无法继续干别的事情，则称该程序在该操作上是阻塞的。  常见的阻塞形式有：网络I/O阻塞、磁盘I/O阻塞、用户输入阻塞等。阻塞是无处不在的，包括CPU切换上下文时，所有的进程都无法真正干事情，它们也会被阻塞。（如果是多核CPU则正在执行上下文切换操作的核不可被利用。）非阻塞  程序在等待某操作过程中，自身不被阻塞，可以继续运行干别的事情，则称该程序在该操作上是非阻塞的。  非阻塞并不是在任何程序级别、任何情况下都可以存在的。  仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的。同步  不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，称这些程序单元是同步执行的。例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的。简言之，同步意味着有序。异步  为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式。  不相关的程序单元之间可以是异步的。例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定。简言之，异步意味着无序。  “通信方式”通常是指异步和并发编程提供的同步原语，如信号量、锁、同步队列等等。我们需知道，虽然这些通信方式是为了让多个程序在一定条件下同步执行，但正因为是异步的存在，才需要这些通信方式。如果所有程序都是按序执行，其本身就是同步的，又何需这些同步信号呢？异步开始  一次只允许处理一个事件。故而有关异步的讨论几乎都集中在了单线程内。如果某事件处理程序需要长时间执行，所有其他部分都会被阻塞。所以，一旦采取异步编程，每个异步调用必须“足够小”，不能耗时太久。如何拆分异步任务成了难题。程序下一步行为往往依赖上一步执行结果，如何知晓上次异步调用已完成并获取结果？回调（Callback）成了必然选择。那又需要面临“回调地狱”的折磨。同步代码改为异步代码，必然破坏代码结构。解决问题的逻辑也要转变，不再是一条路走到黑，需要精心安排异步任务。多进程进程切换开销不止像“CPU的时间观”所列的“上下文切换”那么低。CPU从一个进程切换到另一个进程，需要把旧进程运行时的寄存器状态、内存状态全部保存好，再将另一个进程之前保存的数据恢复。对CPU来讲，几个小时就干等着。当进程数量大于CPU核心数量时，进程切换是必然需要的。除了切换开销，多进程还有另外的缺点。一般的服务器在能够稳定运行的前提下，可以同时处理的进程数在数十个到数百个规模。如果进程数量规模更大，系统运行将不稳定，而且可用内存资源往往也会不足。除了切换开销大，以及可支持的任务规模小之外，多进程还有其他缺点，如状态共享等问题，后文会有提及，此处不再细究。多线程因为在做阻塞的系统调用时，例如sock.connect(),sock.recv()时，当前线程会释放GIL，让别的线程有执行机会。但是单个线程内，在阻塞调用上还是阻塞的。除了GIL之外，所有的多线程还有通病。它们是被OS调度，调度策略是抢占式的，以保证同等优先级的线程都有均等的执行机会，那带来的问题是：并不知道下一时刻是哪个线程被运行，也不知道它正要执行的代码是什么。所以就可能存在竞态条件。epoll判断非阻塞调用是否就绪如果 OS 能做，是不是应用程序就可以不用自己去等待和判断了，就可以利用这个空闲去做其他事情以提高效率。OS将I/O状态的变化都封装成了事件，如可读事件、可写事件。并且提供了专门的系统模块让应用程序可以接收事件通知select因其算法效率比较低，后来改进成了poll，再后来又有进一步改进，BSD内核改进成了kqueue模块，而Linux内核改进成了epoll模块。这四个模块的作用都相同，暴露给程序员使用的API也几乎一致，区别在于kqueue 和 epoll 在处理大量文件描述符时效率更高。事件循环+回调AsyncIO for the Working Python Developerevent loop ：管理和分配的不同任务的执行,It registers them and handles distributing the flow of control between them.Coroutines(协程) ：await会释放控制权到event loop ，协程一旦被Tasks包装成一个Future，就会安排到event loop上运行。Tasks挂起时，有其他的tasks pending 会进行上下文切换Future：就是对象 包含可能或可能未执行的Tasks结果,This result may be an exception.context switch(上下文切换): 就是the event loop yielding the flow of control from one coroutine to the next不同的task包装import asyncioasync def foo():    print('Running in foo')    await asyncio.sleep(0)    print('Explicit context switch to foo again')async def bar():    print('Explicit context to bar')    await asyncio.sleep(0)    print('Implicit context switch back to bar')async def main():    tasks = [foo(), bar()]    await asyncio.gather(*tasks)    # 与上面一样    # await asyncio.wait(tasks)asyncio.run(main())Future 迭代器gathering the coroutines into a list, each of them ready to be scheduled and executed. The as_completed function returns an iterator that will yield a completed future as they come inasync def main():    start = time.time()    futures = [fetch_async(i) for i in range(1, MAX_CLIENTS + 1)]    """    gathering the coroutines into a list, each of them ready to be scheduled and executed.     The as_completed function returns an iterator that will yield a completed future as they come in    """    for i, future in enumerate(asyncio.as_completed(futures)):        result = await future        print('{} {}'.format("&gt;&gt;" * (i + 1), result))    print("Process took: {:.2f} seconds".format(time.time() - start))同步异步Future状态Pending Running Done Cancelled  When a future is done its result method will return the result of the future, if it’s pending or running it raises InvalidStateError, if it’s cancelled it will raise CancelledError, and finally if the coroutine raised an exception it will be raised again, which is the same behaviour as calling exception.handle errorasync def fetch_ip(service):    start = time.time()    print('Fetching IP from {}'.format(service.name))    try:        json_response = await aiohttp_get_json(service.url)    except:        return '{} is unresponsive'.format(service.name)    ip = json_response[service.ip_attr]    return '{} finished with result: {}, took: {:.2f} seconds'.format(        service.name, ip, time.time() - start)async def main():    futures = [fetch_ip(service) for service in SERVICES]    done, _ = await asyncio.wait(futures)    for future in done:        print(future.result())asyncio.run(main())timeoutfutures = [fetch_ip(service) for service in SERVICES]done, pending = await asyncio.wait(    futures, timeout=timeout, return_when=FIRST_COMPLETED)for future in pending:    future.cancel()for future in done:    response["ip"] = future.result()print(response)回调把I/O事件的等待和监听任务交给了 OS，那 OS 在知道I/O状态发生改变后（例如socket连接已建立成功可发送数据），它又怎么知道接下来该干嘛呢？只能回调。缺点:  回调层次过多时代码可读性差  破坏代码结构 (原本从上而下的代码结构，要改成从内到外的)  共享状态管理困难  错误处理困难一连串的回调构成一个完整的调用链，调用链断掉，接力传递的状态也会丢失，这种现象称为调用栈撕裂。所以，为了防止栈撕裂，异常必须以数据的形式返回，而不是直接抛出异常，然后每个回调中需要检查上次调用的返回值，以防错误吞没。  换言之，程序得知道当前所处的状态，而且要将这个状态在不同的回调之间延续下去协程任务之间得相互通知，每个任务得有自己的状态。每个协程具有自己的栈帧，当然能知道自己处于什么状态，协程之间可以协作那自然可以通知别的协程生成器协程风格和回调风格对比总结在回调风格中：  存在链式回调（虽然示例中嵌套回调只有一层）请求和响应也不得不分为两个回调以至于破坏了同步代码那种结构程序员必须在回调之间维护必须的状态。而基于生成器协程的风格：  无链式调用selector的回调里只管给future设置值，不再关心业务逻辑loop 内回调callback()不再关注是谁触发了事件已趋近于同步代码的结构无需程序员在多个协程之间维护状态yield from解决的就是在生成器里玩生成器不方便让嵌套生成器不必通过循环迭代yield，而是直接yield fromdef b():    for i in a:        yield idef c():    yield from a]]></content>
      <categories>
        
          <category> py </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[机器学习概览]]></title>
      <url>/blog/2018/03/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88</url>
      <content type="text"><![CDATA[照抄 Sklearn 与 TensorFlow 机器学习实用指南 机器学习概览]]></content>
      <categories>
        
          <category> ml </category>
        
      </categories>
      <tags>
        
          <tag> ml优化 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[锁]]></title>
      <url>/blog/2018/01/25/%E9%94%81</url>
      <content type="text"><![CDATA[InnoDB 排他锁  for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。  其他线程对该记录的更新与删除操作都会阻塞  排他锁包含行锁、表锁 (没有用到索引引起表锁  还有like &lt;&gt; )  部分条件符合也会针对索引字段锁上(会做成多行锁) 只有所有条件完全不符合的时候才不会上锁一致性解决方案悲观锁加排他锁 适合写入频繁的场景begin;select * from goods where id = 1 for update;update goods set stock = stock - 1 where id = 1;commit;乐观锁乐观锁方案：每次获取商品时，不对该商品加锁。在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新，反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。乐观锁适合读取频繁的场景#不加锁获取 id=1 的商品对象select * from goods where id = 1begin;#更新 stock 值，这里需要注意 where 条件 “stock = cur_stock”，只有程序中获取到的库存量与数据库中的库存量相等才执行更新update goods set stock = stock - 1 where id = 1 and stock = cur_stock;commit;]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[replace and  INSERT ON DUPLICATE KEY UPDATE]]></title>
      <url>/blog/2018/01/25/replace-and-INSERT-ON-DUPLICATE-KEY-UPDATE</url>
      <content type="text"><![CDATA[      处理具有唯一键或主键的记录时，REPLACE将执行DELETE，然后执行INSERT，或者只执行INSERT。此函数将导致删除记录，并在末尾插入，这将导致索引分离，从而降低表的效率。        该记录具有与我们尝试更新的记录相同的UNIQUE或PRIMARY KEY。如果找到现有的，则为要更新的列指定一个子句。否则，它将执行正常的INSERT。  ]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[分区]]></title>
      <url>/blog/2018/01/24/%E5%88%86%E5%8C%BA</url>
      <content type="text"><![CDATA[一张独立逻辑表 底层有若干物理子表构成使用场景限制分区表达式partition by range (整数)保证大数据量扩展性分区会遇到的问题  NULL是分区过滤无效 或者是一些非法值（特殊分区）  分区列和索引列不匹配  选择分区成本高 限制分区个数大致100个  打开锁住所有底层表的成本可能高  维护分区成本高    查询优化    where条件带入分区列  some分区命令分区类型总结]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[shell-常用命令]]></title>
      <url>/blog/2018/01/23/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4</url>
      <content type="text"><![CDATA[# 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容find . -type f -name "*" | xargs grep "140.206.111.111"# 当前目录及子目录下查找所有以.txt和.pdf结尾的文件find . -name "*.txt" -o -name "*.pdf"# 所有目录 /find / -name httpd.confchmod -R 777 dirsudo chmod 644 filesudo chown fibo:fibo file# 端口sudo netstat -tlnup | grep ...# 端口 PIDlsof -i:3306通过list open file命令可以查看到当前打开文件，在linux中所有事物都是以文件形式存在，包括网络连接及硬件设备# 程序ps aux|grep ...nohup cmd &amp; 关掉终端也可以 会有一个输出文件setsid cmd 关掉终端也可以查看环境变量env查看内核版本信息cat /proc/version查看Linux系统版本的命令lsb_release -a查看系统时间date '+%Y-%m-%d %H:%M:%S'日志分析查看——grep,sed,sort,awk运用Linux命令查询释放内存查看内存占用free -h运行sync 命令以确保文件系统的完整性 将所有未写的系统缓冲区写到磁盘中 否则在释放缓存的过程中，可能会丢失未保存的文件利用 sh -c 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。sync &amp;&amp; sudo sh -c 'echo 3 &gt; /proc/sys/vm/drop_caches'/proc/sys/vm/drop_caches]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[unix进程间通信]]></title>
      <url>/blog/2018/01/23/unix%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1</url>
      <content type="text"><![CDATA[进程的状态与转换Python 中的进程、线程、协程、同步、异步、回调unix进程间通信方式  管道（Pipe）：管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信。  命名管道（namedpipe）：命名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信。命名管道在文件系统中有对应的文件名。命名管道通过命令mkfifo或系统调用mkfifo来创建。  信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）。  消息（Message）队列：消息队列是消息的链接表，包括Posix消息队列systemV消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺      共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。    内存映射（mappedmemory）：内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间来实现它。  信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。  套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和SystemV的变种都支持套接字。]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[索引]]></title>
      <url>/blog/2018/01/23/%E7%B4%A2%E5%BC%95%E4%BC%98%E7%BC%BA</url>
      <content type="text"><![CDATA[优缺点种类  BTree索引  Hash索引不是所有的引擎都支持Hash索引，对于一些长字符串比较慢模拟Hash 使用CRC32()做Hash，同时select目标和Hash值。SHA1()和MD5()作为哈希字符串比较长，但是表很大，CRC32()会出现大量Hash冲突，可以考虑自己实现自定义hash函数返回整型。缺点维护比较麻烦  前缀索引 索引选择性=不重复索引数（基数）/count(*)  越大索引性能越好。使用left(col,n) 取前n位前缀。通过group by统计索引统计方法2 找出最接近的后缀索引（邮箱，电话…） –将字符串翻转做前缀索引使用触发器维护  多列索引 多个单列索引并不能提高MYSQL性能5.0以后会有索引合并UNION 操作符用于合并两个或多个 SELECT 语句的结果集。请注意，UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。UNION ALL 是允许重复值  5. 多列索引 基数大的排第一 ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fufej9pl5ej319c13ggpm.jpg)  6. 聚簇索引 是一种数据存储方式 最好按主键顺序插入。避免不连续且分布范围非常大的聚簇索引，页分裂数据碎片 ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fufejwxzj4j314m0mswhb.jpg)优化索引索引的数据类型  越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快。  简单的数据类型更好：整型数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应该用内置的日期和时间数据类型，而不是用字符串来存储时间；以及用整型数据类型存储IP地址。  尽量避免NULL：应该指定列为NOT NULL，除非你想存储NULL。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值。建索引的几大原则  最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。  =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式  尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录  索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);  尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可索引失效  如果条件中有or，即使其中有条件带索引也不会使用。要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引。  like查询是以%开头  类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引  如果mysql估计使用全表扫描要比使用索引快,则不使用索引  上面给出一个多列索引(username,password,last_login)，当三列在where中出现的顺序如(username,password,last_login)、 (username,password)、(username)才能用到索引，如下面几个顺序(password,last_login)、(passwrod)、(last_login)—这三者不 从username开始，(username,last_login)—断层，少了password，都无法利用到索引。因为B+tree多列索引保存的顺序是按照索引创 建的顺序，检索索引时按照此顺序检索理解MySQL——索引与优化MYSQL索引结构原理、性能分析与优化  要取出所有等值谓词中的列，作为索引开头的最开始的列(任意顺序)  要将 ORDER BY 列加入索引中      要将查询语句剩余的列全部加入到索引中    不只是将等值谓词的列加入索引，它的作用是减少索引片的大小以减少需要扫描的数据行  用于避免排序，减少磁盘 IO 和内存的使用  用于避免每一个索引对应的数据行都需要进行一次随机 IO 从聚集索引中读取剩余的数据]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[事务]]></title>
      <url>/blog/2018/01/23/%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87</url>
      <content type="text"><![CDATA[数据库设计  查询优化 where order by 索引 &amp; 避免全表扫描  避免在where进行字段进行null值判断 导致引擎放弃索引进行全表扫描  索引列有大量重复数据 效率不高  索引提高select 降低insert &amp; update（重建索引）一个表最好6个  避免更新索引列  尽量使用数字型字段 字符串逐个比较  varchar 替代 char  使用表变量代替临时表SQL方面  避免where子句使用 != / &lt;&gt; 全表扫面  能用between and 不用 in  like ‘%abc%’  where 避免表达式 where num/2 = 100 =&gt; num = 100*2  List item]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[事务]]></title>
      <url>/blog/2018/01/23/%E5%B0%91%E5%81%9A-%E4%B8%8D%E5%81%9A-%E5%BF%AB%E9%80%9F%E5%81%9A</url>
      <content type="text"><![CDATA[count()  使用汇总表  外部缓存    关联优化    on 列上有索引  group by/order by只是涉及一个表的列分页优化 或者使用between and 替代已知范围/先缓存大量的数据union创建并填充临时表执行union查询  最好使用union all除非要消除重复行，临时表有distinct临时表数据唯一性检查，代价非常高。  union 区分冷热数据使用自定义变量计算相关]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mysql优化思考]]></title>
      <url>/blog/2018/01/23/mysql%E4%BC%98%E5%8C%96</url>
      <content type="text"><![CDATA[mysql考虑  数据的容量：1-3年内会大概多少条数据，每条数据大概多少字节；  数据项：是否有大字段，那些字段的值是否经常被更新；  数据查询SQL条件：哪些数据项的列名称经常出现在WHERE、GROUP BY、ORDER BY子句中等；  数据更新类SQL条件：有多少列经常出现UPDATE或DELETE 的WHERE子句中；  SQL量的统计比，如：SELECT：UPDATE+DELETE：INSERT=多少？  预计大表及相关联的SQL，每天总的执行量在何数量级？  表中的数据：更新为主的业务 还是 查询为主的业务  打算采用什么数据库物理服务器，以及数据库服务器架构？  并发如何？  存储引擎选择InnoDB还是MyISAM？优化方向  优化sql 索引  缓存(memcached,redis)  读写分离 主从复制  分区（分区条件列）  垂直拆分（按列分割，子表行数一样）（example：冷热列）  水平拆分 （按记录分割，列数相同）（example：表很大 表冷热不均 存放到多个介质） 数据水平切分的主键全局唯一方案慢查询优化基本步骤  先运行看看是否真的很慢，注意设置SQL_NO_CACHE  where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高  explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）  order by limit 形式的sql语句让排序的表优先查  了解业务方使用场景  加索引时参照建索引的几大原则  观察结果，不符合预期继续从0分析调整sql语句性能的几个要点MySQL数据库性能优化之缓存参数优化MySQL数据库性能优化之表结构MySQL数据库性能优化之索引优化MySQL数据库性能优化之SQL优化MySQL数据库性能优化之存储引擎选择MySQL数据库性能优化之硬件优化MySQL性能优化的最佳20+条经验]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[部署Gitolite]]></title>
      <url>/blog/2018/01/22/gitolite</url>
      <content type="text"><![CDATA[Gitolite    sudo useradd -m git -s /bin/bash创建git用户 安装gitServersudo su rootapt-get install gitcreate ssh key创建密钥对ssh-keygen -t rsa -f git# server 创建sudo mv .ssh/git.pub /home/git/# 复制自己的公钥scp 上去serverscp id_rsa.pub git@192.168.1.107:admin.pubsudo chown git:git /home/git/admin.pubServerinstall gitolitesu gitcd /home/gitgit clone git://github.com/sitaramc/gitolitemkdir bingitolite/install -to /home/git/bin/home/git/bin/gitolite setup -pk git.pubClientclone# 测试 clone 管理项目git clone git@192.168.1.106:gitolite-admin]]></content>
      <categories>
        
          <category> git </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Kafka 设计]]></title>
      <url>/blog/2018/01/12/Kafka-%E8%AE%BE%E8%AE%A1</url>
      <content type="text"><![CDATA[Why 文件系统来存储数据  文件系统存储速度快慢一定程度上也取决于我们对磁盘的用法。线性写的速度约是随机写 的 6000 多倍  Java 对 象内存消耗非常高， 且随着 Java 对象的增加JVM垃圾回收也越来越频繁和繁琐，这些都加大了内存的消耗 。数据的持久化队列可以建立在简单地对文件进行追加的实现方案上，因为是顺序追加， 所以 Kafka在设计上是采用时间复杂度。(1)的磁盘结构，它提供了常量时间的性能，即使是存 储海量的信息( TB 级)也如此，性能和数据的大小关系也不大，同时 Kafka将数据持久化到磁 盘上，这样只要磁盘空间足够大数据就可以一直追加，而不会像一般的消息系统在消息被消费 后就删除掉， Kafka 提供了相关配置让用户自己决定消息要保存多久，这样为消费者提供了更 灵活的处理方式，因此 Kafka 能够在没有性能损失的情况下提供一般消息系统不具备的特性 。]]></content>
      <categories>
        
          <category> kafka </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Kafka 结构]]></title>
      <url>/blog/2018/01/12/Kafka-%E7%BB%93%E6%9E%84</url>
      <content type="text"><![CDATA[Kafka 是一个高吞吐量、分布式的发布一订阅消息系统。当前的 Kafka己经定位为一个分布式流式处理平台。Kafka 是一款开源的、轻量级的 、分布式、可分区和具有复制备份的 (Replicated)、基于 ZooKeeper 协调 管理的分布式流平台的功能 强大的消息系统 。与传统 的消息系统相比， Kafka 能够很好地处理活跃的流数据，使得数据在各个子系统中高性能、低延迟地不停流转。  能够允许发布和订阅流数据  存储流数据时提供相应的容错机制  当流数据到达时能够被及时处理生产者负责生产消息，将消息写入 Kafka 集群:消费者从 Kafka 集群中拉取消息message消息是 Kafka通信的基本单位topictopic 对 Message 的一个分类。生产者将消息发送到特定主题，消费者订阅主题或主题的某些分区进行消费。partition and duplicate一组消息归纳为一个主题每个主题又被分成一个或多个分区每个分区由一系列有序、不可变的消息组成，一个有序队列。每个分区又有一至多个 副本( Replica)，分区的副本分布在集群的不同代理上，以提高可用性物理上：每个分区在物理上对应为一个文件夹分区的命名规则为主题名称后接“一”连接符，之 后再接分区编号，分区编号从 0 开始分区分区有点：分区使得 Kafka在井发处理上变得更加容易，理论上来说，分区数越多吞吐量越高，但这 要根据集群实际环境及业务场景而定。同时，分区也是 Kafka保证消息被顺序消费以及对消息 进行负载均衡的基础。Kafka 只能保证一个分区之内消息的有序性，并不能保证跨分区消息的有序性 。 每条消息 被追加到相应的分区中，是顺序写磁盘，因此效率非常高，这是 Kafka 高吞吐率的 一个重要保证。Kafka 并不会立即删除已被消费的消息，由于磁盘的限制 消息也不会一直被存储(事实上这也是没有必要的)，因此 Kafka提供两种删除老数据的策略， 一是基于消息己存储的时间长度， 二是基于分区的大小一个分区的多个副本之间数据的一致性， Kafka会选 择该分区的 一个副本作为 Leader 副本，而该分区其他副本即为 Follower 副本，只有 Leader 副 本才负责处理客户端读/写请求， Follower 副本从 Leader 副本同步数据。log分区的副本与日志对象是一一对应的。任何发布到分区的消息会被直接追加到日志文件的尾部代理生产环境中 Kafka集群一般包括一台或多台服务器 ，我们可以在一台服务器 上配置一个或多个代理。每一个代理都有唯一的标识 id，这个 id是一个非负整数。在一个 Kafka 集群中，每增加一个代理就需要为这个代理配置一个与该集群中其他代理不同的 id, id值可以 选择任意非负整数 即可，只 要保证它在整个 Kafka 集群中唯 一 ，这个 id 就是代理的名字，也就 是在启动 代理时配置的 broker.id 对应的值，因此在本书中有时我们也称为 brokerId。##生产者生产者(Producer)负责将消息发送给代理，也就是向 Kafka代理发送消息的客户端。##消费者我们可以为每个消费者指定一个消费组，以 groupId代表消费组名称如果不指定消费组，则该消费者属于默认消费组 test-consumer-groupKafka 会自动为该消费者生成一个全局唯一的 id，格式为${groupld}-${hostName}-${timestamp}-${UUID 前 8位字符}基本概念多个消费者分配到同一个消费者多个消费组订阅同一个主题同一主题的一条消息只能被同一个消费组里某一个消费者消费，不同消费组的消费者可同时消费该消息##ISR在 ZooKeeper 中动态维护了一个ISR,保存同步的副本列表，该列表中保存的是与 Leader 副本保持消息同步的所有副本对应的代理节点 idZooKeeperKafka 在启动或运行过程当中会在 ZooKeeper 上创建相应节点来保 存元数据信息， Kafka 通过监昕机制在这些节点注册相应监听器来监昕节点元数据的变化，从 而由 ZooKeeper 负责管理维护 Kafka集群，同时通过 ZooKeeper 我们能够很方便地对 Kafka 集 群进行水平扩展及数据迁移Kafka 基本结构]]></content>
      <categories>
        
          <category> kafka </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
